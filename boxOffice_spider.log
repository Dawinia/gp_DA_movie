2020-04-18 01:12:26,971 -  log.py[line:146] - INFO: Scrapy 1.8.0 started (bot: movie)
2020-04-18 01:12:26,974 -  log.py[line:149] - INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.3 (default, Mar 27 2019, 22:11:17) - [GCC 7.3.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.6.1, Platform Linux-5.3.0-46-generic-x86_64-with-debian-buster-sid
2020-04-18 01:12:26,977 -  crawler.py[line:49] - INFO: Overridden settings: {'BOT_NAME': 'movie', 'COMMANDS_MODULE': 'movie.commands', 'LOG_LEVEL': 'WARNING', 'NEWSPIDER_MODULE': 'movie.spiders', 'SPIDER_MODULES': ['movie.spiders']}
2020-04-18 01:12:26,984 -  telnet.py[line:60] - INFO: Telnet Password: a5bbe7099760e386
2020-04-18 01:12:26,992 -  middleware.py[line:48] - INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-04-18 01:12:27,097 -  middleware.py[line:48] - INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'movie.middlewares.UserAgentMiddleware',
 'movie.middlewares.MovieDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-04-18 01:12:27,099 -  middleware.py[line:48] - INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'movie.middlewares.MovieSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-04-18 01:12:27,105 -  pipelines.py[line:16] - ERROR: start to use judge duplicate
2020-04-18 01:12:27,105 -  pipelines.py[line:62] - ERROR: setting: custom boxOffice setting
2020-04-18 01:12:27,140 -  pipelines.py[line:58] - ERROR: pipeline init
2020-04-18 01:12:27,140 -  middleware.py[line:48] - INFO: Enabled item pipelines:
['movie.pipelines.BoxOfficePipeline', 'movie.pipelines.MySQLPipeline']
2020-04-18 01:12:27,140 -  engine.py[line:257] - INFO: Spider opened
2020-04-18 01:12:27,141 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-18 01:12:27,141 -  middlewares.py[line:112] - INFO: Spider opened: movie
2020-04-18 01:12:27,141 -  middlewares.py[line:65] - INFO: Spider opened: movie
2020-04-18 01:12:27,142 -  telnet.py[line:74] - INFO: Telnet console listening on 127.0.0.1:6026
2020-04-18 01:12:27,144 -  movie_spider.py[line:96] - ERROR: boxOffice start request for url = http://piaofang.maoyan.com/dashboard-ajax/movie?
2020-04-18 01:12:27,310 -  engine.py[line:239] - DEBUG: Crawled (200) <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419> (referer: None)
2020-04-18 01:12:27,314 -  engine.py[line:239] - DEBUG: Crawled (200) <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418> (referer: None)
2020-04-18 01:12:27,494 -  movie_spider.py[line:115] - ERROR: now crawl url : http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419
2020-04-18 01:12:27,499 -  movie_spider.py[line:143] - WARNING: get 1 movie info, named None.
2020-04-18 01:12:27,503 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['1801.52'],
 'boxRate': ['29.6%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1211727],
 'movieName': ['反贪风暴4'],
 'releaseInfo': ['上映16天'],
 'seatRate': ['5.5%'],
 'showInfo': [74705],
 'showRate': ['20.7%'],
 'showView': ['7'],
 'splitBoxInfo': ['1665.02'],
 'splitSumBoxInfo': ['63400.0'],
 'sumBoxInfo': ['68600.0'],
 'yearRate': ['2019-04-19#1']}
{'boxInfo': ['1801.52'],
 'boxRate': ['29.6%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1211727],
 'movieName': ['反贪风暴4'],
 'releaseInfo': ['上映16天'],
 'seatRate': ['5.5%'],
 'showInfo': [74705],
 'showRate': ['20.7%'],
 'showView': ['7'],
 'splitBoxInfo': ['1665.02'],
 'splitSumBoxInfo': ['63400.0'],
 'sumBoxInfo': ['68600.0'],
 'yearRate': ['2019-04-19#1']}
2020-04-18 01:12:27,513 -  movie_spider.py[line:143] - WARNING: get 2 movie info, named None.
2020-04-18 01:12:27,517 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['1079.52'],
 'boxRate': ['17.7%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1239544],
 'movieName': ['调音师'],
 'releaseInfo': ['上映17天'],
 'seatRate': ['7.6%'],
 'showInfo': [38571],
 'showRate': ['10.6%'],
 'showView': ['9'],
 'splitBoxInfo': ['981.18'],
 'splitSumBoxInfo': ['22400.0'],
 'sumBoxInfo': ['24900.0'],
 'yearRate': ['2019-04-19#2']}
{'boxInfo': ['1079.52'],
 'boxRate': ['17.7%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1239544],
 'movieName': ['调音师'],
 'releaseInfo': ['上映17天'],
 'seatRate': ['7.6%'],
 'showInfo': [38571],
 'showRate': ['10.6%'],
 'showView': ['9'],
 'splitBoxInfo': ['981.18'],
 'splitSumBoxInfo': ['22400.0'],
 'sumBoxInfo': ['24900.0'],
 'yearRate': ['2019-04-19#2']}
2020-04-18 01:12:27,598 -  movie_spider.py[line:115] - ERROR: now crawl url : http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418
2020-04-18 01:12:27,602 -  movie_spider.py[line:143] - WARNING: get 1 movie info, named None.
2020-04-18 01:12:27,606 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['1462.50'],
 'boxRate': ['41.8%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1211727],
 'movieName': ['反贪风暴4'],
 'releaseInfo': ['上映15天'],
 'seatRate': ['4.0%'],
 'showInfo': [97520],
 'showRate': ['31.0%'],
 'showView': ['5'],
 'splitBoxInfo': ['1344.97'],
 'splitSumBoxInfo': ['61700.0'],
 'sumBoxInfo': ['66800.0'],
 'yearRate': ['2019-04-18#1']}
{'boxInfo': ['1462.50'],
 'boxRate': ['41.8%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1211727],
 'movieName': ['反贪风暴4'],
 'releaseInfo': ['上映15天'],
 'seatRate': ['4.0%'],
 'showInfo': [97520],
 'showRate': ['31.0%'],
 'showView': ['5'],
 'splitBoxInfo': ['1344.97'],
 'splitSumBoxInfo': ['61700.0'],
 'sumBoxInfo': ['66800.0'],
 'yearRate': ['2019-04-18#1']}
2020-04-18 01:12:27,644 -  movie_spider.py[line:143] - WARNING: get 3 movie info, named None.
2020-04-18 01:12:27,646 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['905.52'],
 'boxRate': ['14.9%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1207260],
 'movieName': ['如影随心'],
 'releaseInfo': ['上映首日'],
 'seatRate': ['4.3%'],
 'showInfo': [73594],
 'showRate': ['20.4%'],
 'showView': ['4'],
 'splitBoxInfo': ['851.62'],
 'splitSumBoxInfo': ['860.7'],
 'sumBoxInfo': ['915.0'],
 'yearRate': ['2019-04-19#3']}
{'boxInfo': ['905.52'],
 'boxRate': ['14.9%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1207260],
 'movieName': ['如影随心'],
 'releaseInfo': ['上映首日'],
 'seatRate': ['4.3%'],
 'showInfo': [73594],
 'showRate': ['20.4%'],
 'showView': ['4'],
 'splitBoxInfo': ['851.62'],
 'splitSumBoxInfo': ['860.7'],
 'sumBoxInfo': ['915.0'],
 'yearRate': ['2019-04-19#3']}
2020-04-18 01:12:27,647 -  movie_spider.py[line:143] - WARNING: get 4 movie info, named None.
2020-04-18 01:12:27,648 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['694.98'],
 'boxRate': ['11.4%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1162868],
 'movieName': ['转型团伙'],
 'releaseInfo': ['上映首日'],
 'seatRate': ['5.9%'],
 'showInfo': [49100],
 'showRate': ['13.6%'],
 'showView': ['5'],
 'splitBoxInfo': ['636.16'],
 'splitSumBoxInfo': ['641.7'],
 'sumBoxInfo': ['700.9'],
 'yearRate': ['2019-04-19#4']}
{'boxInfo': ['694.98'],
 'boxRate': ['11.4%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1162868],
 'movieName': ['转型团伙'],
 'releaseInfo': ['上映首日'],
 'seatRate': ['5.9%'],
 'showInfo': [49100],
 'showRate': ['13.6%'],
 'showView': ['5'],
 'splitBoxInfo': ['636.16'],
 'splitSumBoxInfo': ['641.7'],
 'sumBoxInfo': ['700.9'],
 'yearRate': ['2019-04-19#4']}
2020-04-18 01:12:27,650 -  movie_spider.py[line:143] - WARNING: get 5 movie info, named None.
2020-04-18 01:12:27,651 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['296.15'],
 'boxRate': ['4.8%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1205909],
 'movieName': ['祈祷落幕时'],
 'releaseInfo': ['上映8天'],
 'seatRate': ['7.9%'],
 'showInfo': [12467],
 'showRate': ['3.4%'],
 'showView': ['8'],
 'splitBoxInfo': ['268.68'],
 'splitSumBoxInfo': ['4342.2'],
 'sumBoxInfo': ['4814.6'],
 'yearRate': ['2019-04-19#5']}
{'boxInfo': ['296.15'],
 'boxRate': ['4.8%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1205909],
 'movieName': ['祈祷落幕时'],
 'releaseInfo': ['上映8天'],
 'seatRate': ['7.9%'],
 'showInfo': [12467],
 'showRate': ['3.4%'],
 'showView': ['8'],
 'splitBoxInfo': ['268.68'],
 'splitSumBoxInfo': ['4342.2'],
 'sumBoxInfo': ['4814.6'],
 'yearRate': ['2019-04-19#5']}
2020-04-18 01:12:27,652 -  movie_spider.py[line:143] - WARNING: get 6 movie info, named None.
2020-04-18 01:12:27,653 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['254.99'],
 'boxRate': ['4.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1211412],
 'movieName': ['神奇乐园历险记'],
 'releaseInfo': ['上映首日'],
 'seatRate': ['5.3%'],
 'showInfo': [24047],
 'showRate': ['6.6%'],
 'showView': ['4'],
 'splitBoxInfo': ['236.39'],
 'splitSumBoxInfo': ['252.1'],
 'sumBoxInfo': ['271.5'],
 'yearRate': ['2019-04-19#6']}
{'boxInfo': ['254.99'],
 'boxRate': ['4.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1211412],
 'movieName': ['神奇乐园历险记'],
 'releaseInfo': ['上映首日'],
 'seatRate': ['5.3%'],
 'showInfo': [24047],
 'showRate': ['6.6%'],
 'showView': ['4'],
 'splitBoxInfo': ['236.39'],
 'splitSumBoxInfo': ['252.1'],
 'sumBoxInfo': ['271.5'],
 'yearRate': ['2019-04-19#6']}
2020-04-18 01:12:27,654 -  movie_spider.py[line:143] - WARNING: get 7 movie info, named None.
2020-04-18 01:12:27,654 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['251.90'],
 'boxRate': ['4.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [248123],
 'movieName': ['雷霆沙赞！'],
 'releaseInfo': ['上映15天'],
 'seatRate': ['3.6%'],
 'showInfo': [17805],
 'showRate': ['4.9%'],
 'showView': ['4'],
 'splitBoxInfo': ['233.81'],
 'splitSumBoxInfo': ['25900.0'],
 'sumBoxInfo': ['28300.0'],
 'yearRate': ['2019-04-19#7']}
{'boxInfo': ['251.90'],
 'boxRate': ['4.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [248123],
 'movieName': ['雷霆沙赞！'],
 'releaseInfo': ['上映15天'],
 'seatRate': ['3.6%'],
 'showInfo': [17805],
 'showRate': ['4.9%'],
 'showView': ['4'],
 'splitBoxInfo': ['233.81'],
 'splitSumBoxInfo': ['25900.0'],
 'sumBoxInfo': ['28300.0'],
 'yearRate': ['2019-04-19#7']}
2020-04-18 01:12:27,656 -  dupefilters.py[line:70] - DEBUG: Filtered duplicate request: <GET https://movie.douban.com/j/subject_suggest?q=%E5%8F%8D%E8%B4%AA%E9%A3%8E%E6%9A%B44> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2020-04-18 01:12:27,656 -  movie_spider.py[line:143] - WARNING: get 2 movie info, named None.
2020-04-18 01:12:27,657 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['812.48'],
 'boxRate': ['23.2%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1239544],
 'movieName': ['调音师'],
 'releaseInfo': ['上映16天'],
 'seatRate': ['4.7%'],
 'showInfo': [55306],
 'showRate': ['17.6%'],
 'showView': ['5'],
 'splitBoxInfo': ['733.55'],
 'splitSumBoxInfo': ['21400.0'],
 'sumBoxInfo': ['23800.0'],
 'yearRate': ['2019-04-18#2']}
{'boxInfo': ['812.48'],
 'boxRate': ['23.2%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1239544],
 'movieName': ['调音师'],
 'releaseInfo': ['上映16天'],
 'seatRate': ['4.7%'],
 'showInfo': [55306],
 'showRate': ['17.6%'],
 'showView': ['5'],
 'splitBoxInfo': ['733.55'],
 'splitSumBoxInfo': ['21400.0'],
 'sumBoxInfo': ['23800.0'],
 'yearRate': ['2019-04-18#2']}
2020-04-18 01:12:27,658 -  movie_spider.py[line:143] - WARNING: get 3 movie info, named None.
2020-04-18 01:12:27,659 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['349.66'],
 'boxRate': ['10.0%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1205909],
 'movieName': ['祈祷落幕时'],
 'releaseInfo': ['上映7天'],
 'seatRate': ['4.6%'],
 'showInfo': [32841],
 'showRate': ['10.4%'],
 'showView': ['4'],
 'splitBoxInfo': ['315.54'],
 'splitSumBoxInfo': ['4073.5'],
 'sumBoxInfo': ['4518.5'],
 'yearRate': ['2019-04-18#3']}
{'boxInfo': ['349.66'],
 'boxRate': ['10.0%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1205909],
 'movieName': ['祈祷落幕时'],
 'releaseInfo': ['上映7天'],
 'seatRate': ['4.6%'],
 'showInfo': [32841],
 'showRate': ['10.4%'],
 'showView': ['4'],
 'splitBoxInfo': ['315.54'],
 'splitSumBoxInfo': ['4073.5'],
 'sumBoxInfo': ['4518.5'],
 'yearRate': ['2019-04-18#3']}
2020-04-18 01:12:27,660 -  movie_spider.py[line:143] - WARNING: get 4 movie info, named None.
2020-04-18 01:12:27,661 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['218.96'],
 'boxRate': ['6.2%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [248123],
 'movieName': ['雷霆沙赞！'],
 'releaseInfo': ['上映14天'],
 'seatRate': ['3.2%'],
 'showInfo': [34886],
 'showRate': ['11.1%'],
 'showView': ['2'],
 'splitBoxInfo': ['200.61'],
 'splitSumBoxInfo': ['25600.0'],
 'sumBoxInfo': ['28100.0'],
 'yearRate': ['2019-04-18#4']}
{'boxInfo': ['218.96'],
 'boxRate': ['6.2%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [248123],
 'movieName': ['雷霆沙赞！'],
 'releaseInfo': ['上映14天'],
 'seatRate': ['3.2%'],
 'showInfo': [34886],
 'showRate': ['11.1%'],
 'showView': ['2'],
 'splitBoxInfo': ['200.61'],
 'splitSumBoxInfo': ['25600.0'],
 'sumBoxInfo': ['28100.0'],
 'yearRate': ['2019-04-18#4']}
2020-04-18 01:12:27,662 -  movie_spider.py[line:143] - WARNING: get 5 movie info, named None.
2020-04-18 01:12:27,663 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['199.15'],
 'boxRate': ['5.6%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1212492],
 'movieName': ['老师·好'],
 'releaseInfo': ['重映28天'],
 'seatRate': ['4.0%'],
 'showInfo': [19631],
 'showRate': ['6.2%'],
 'showView': ['4'],
 'splitBoxInfo': ['184.15'],
 'splitSumBoxInfo': ['31400.0'],
 'sumBoxInfo': ['34000.0'],
 'yearRate': ['2019-04-18#5']}
{'boxInfo': ['199.15'],
 'boxRate': ['5.6%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1212492],
 'movieName': ['老师·好'],
 'releaseInfo': ['重映28天'],
 'seatRate': ['4.0%'],
 'showInfo': [19631],
 'showRate': ['6.2%'],
 'showView': ['4'],
 'splitBoxInfo': ['184.15'],
 'splitSumBoxInfo': ['31400.0'],
 'sumBoxInfo': ['34000.0'],
 'yearRate': ['2019-04-18#5']}
2020-04-18 01:12:27,664 -  movie_spider.py[line:143] - WARNING: get 6 movie info, named None.
2020-04-18 01:12:27,664 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['78.71'],
 'boxRate': ['2.2%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1228750],
 'movieName': ['照相师'],
 'releaseInfo': [''],
 'seatRate': ['46.5%'],
 'showInfo': [203],
 'showRate': ['<0.1%'],
 'showView': ['102'],
 'splitBoxInfo': ['78.71'],
 'splitSumBoxInfo': ['1756.8'],
 'sumBoxInfo': ['1769.3'],
 'yearRate': ['2019-04-18#6']}
{'boxInfo': ['78.71'],
 'boxRate': ['2.2%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1228750],
 'movieName': ['照相师'],
 'releaseInfo': [''],
 'seatRate': ['46.5%'],
 'showInfo': [203],
 'showRate': ['<0.1%'],
 'showView': ['102'],
 'splitBoxInfo': ['78.71'],
 'splitSumBoxInfo': ['1756.8'],
 'sumBoxInfo': ['1769.3'],
 'yearRate': ['2019-04-18#6']}
2020-04-18 01:12:27,666 -  movie_spider.py[line:143] - WARNING: get 7 movie info, named None.
2020-04-18 01:12:27,666 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['49.98'],
 'boxRate': ['1.4%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [368260],
 'movieName': ['最佳男友进化论'],
 'releaseInfo': ['上映7天'],
 'seatRate': ['2.8%'],
 'showInfo': [17033],
 'showRate': ['5.4%'],
 'showView': ['2'],
 'splitBoxInfo': ['45.56'],
 'splitSumBoxInfo': ['1275.4'],
 'sumBoxInfo': ['1404.9'],
 'yearRate': ['2019-04-18#7']}
{'boxInfo': ['49.98'],
 'boxRate': ['1.4%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [368260],
 'movieName': ['最佳男友进化论'],
 'releaseInfo': ['上映7天'],
 'seatRate': ['2.8%'],
 'showInfo': [17033],
 'showRate': ['5.4%'],
 'showView': ['2'],
 'splitBoxInfo': ['45.56'],
 'splitSumBoxInfo': ['1275.4'],
 'sumBoxInfo': ['1404.9'],
 'yearRate': ['2019-04-18#7']}
2020-04-18 01:12:27,667 -  movie_spider.py[line:143] - WARNING: get 8 movie info, named None.
2020-04-18 01:12:27,668 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['182.63'],
 'boxRate': ['3.0%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1212492],
 'movieName': ['老师·好'],
 'releaseInfo': ['重映29天'],
 'seatRate': ['6.3%'],
 'showInfo': [9234],
 'showRate': ['2.5%'],
 'showView': ['6'],
 'splitBoxInfo': ['170.68'],
 'splitSumBoxInfo': ['31600.0'],
 'sumBoxInfo': ['34200.0'],
 'yearRate': ['2019-04-19#8']}
{'boxInfo': ['182.63'],
 'boxRate': ['3.0%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1212492],
 'movieName': ['老师·好'],
 'releaseInfo': ['重映29天'],
 'seatRate': ['6.3%'],
 'showInfo': [9234],
 'showRate': ['2.5%'],
 'showView': ['6'],
 'splitBoxInfo': ['170.68'],
 'splitSumBoxInfo': ['31600.0'],
 'sumBoxInfo': ['34200.0'],
 'yearRate': ['2019-04-19#8']}
2020-04-18 01:12:27,669 -  movie_spider.py[line:143] - WARNING: get 9 movie info, named None.
2020-04-18 01:12:27,670 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['176.59'],
 'boxRate': ['2.9%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [581264],
 'movieName': ['境·界'],
 'releaseInfo': ['上映首日'],
 'seatRate': ['4.2%'],
 'showInfo': [38871],
 'showRate': ['10.7%'],
 'showView': ['2'],
 'splitBoxInfo': ['165.47'],
 'splitSumBoxInfo': ['169.0'],
 'sumBoxInfo': ['180.3'],
 'yearRate': ['2019-04-19#9']}
{'boxInfo': ['176.59'],
 'boxRate': ['2.9%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [581264],
 'movieName': ['境·界'],
 'releaseInfo': ['上映首日'],
 'seatRate': ['4.2%'],
 'showInfo': [38871],
 'showRate': ['10.7%'],
 'showView': ['2'],
 'splitBoxInfo': ['165.47'],
 'splitSumBoxInfo': ['169.0'],
 'sumBoxInfo': ['180.3'],
 'yearRate': ['2019-04-19#9']}
2020-04-18 01:12:27,671 -  movie_spider.py[line:143] - WARNING: get 10 movie info, named None.
2020-04-18 01:12:27,672 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['70.57'],
 'boxRate': ['1.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1228750],
 'movieName': ['照相师'],
 'releaseInfo': [''],
 'seatRate': ['89.1%'],
 'showInfo': [154],
 'showRate': ['<0.1%'],
 'showView': ['119'],
 'splitBoxInfo': ['70.57'],
 'splitSumBoxInfo': ['1827.4'],
 'sumBoxInfo': ['1839.8'],
 'yearRate': ['2019-04-19#10']}
{'boxInfo': ['70.57'],
 'boxRate': ['1.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1228750],
 'movieName': ['照相师'],
 'releaseInfo': [''],
 'seatRate': ['89.1%'],
 'showInfo': [154],
 'showRate': ['<0.1%'],
 'showView': ['119'],
 'splitBoxInfo': ['70.57'],
 'splitSumBoxInfo': ['1827.4'],
 'sumBoxInfo': ['1839.8'],
 'yearRate': ['2019-04-19#10']}
2020-04-18 01:12:27,673 -  movie_spider.py[line:143] - WARNING: get 11 movie info, named None.
2020-04-18 01:12:27,674 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['54.15'],
 'boxRate': ['0.8%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1219670],
 'movieName': ['海市蜃楼'],
 'releaseInfo': ['上映23天'],
 'seatRate': ['17.1%'],
 'showInfo': [846],
 'showRate': ['0.2%'],
 'showView': ['21'],
 'splitBoxInfo': ['53.21'],
 'splitSumBoxInfo': ['9814.5'],
 'sumBoxInfo': ['10800.0'],
 'yearRate': ['2019-04-19#11']}
{'boxInfo': ['54.15'],
 'boxRate': ['0.8%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1219670],
 'movieName': ['海市蜃楼'],
 'releaseInfo': ['上映23天'],
 'seatRate': ['17.1%'],
 'showInfo': [846],
 'showRate': ['0.2%'],
 'showView': ['21'],
 'splitBoxInfo': ['53.21'],
 'splitSumBoxInfo': ['9814.5'],
 'sumBoxInfo': ['10800.0'],
 'yearRate': ['2019-04-19#11']}
2020-04-18 01:12:27,675 -  movie_spider.py[line:143] - WARNING: get 12 movie info, named None.
2020-04-18 01:12:27,675 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['38.21'],
 'boxRate': ['0.6%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1167831],
 'movieName': ['波西米亚狂想曲'],
 'releaseInfo': ['上映29天'],
 'seatRate': ['6.3%'],
 'showInfo': [1404],
 'showRate': ['0.3%'],
 'showView': ['7'],
 'splitBoxInfo': ['35.12'],
 'splitSumBoxInfo': ['8723.1'],
 'sumBoxInfo': ['9628.7'],
 'yearRate': ['2019-04-19#12']}
{'boxInfo': ['38.21'],
 'boxRate': ['0.6%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1167831],
 'movieName': ['波西米亚狂想曲'],
 'releaseInfo': ['上映29天'],
 'seatRate': ['6.3%'],
 'showInfo': [1404],
 'showRate': ['0.3%'],
 'showView': ['7'],
 'splitBoxInfo': ['35.12'],
 'splitSumBoxInfo': ['8723.1'],
 'sumBoxInfo': ['9628.7'],
 'yearRate': ['2019-04-19#12']}
2020-04-18 01:12:27,676 -  movie_spider.py[line:143] - WARNING: get 13 movie info, named None.
2020-04-18 01:12:27,677 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['37.22'],
 'boxRate': ['0.6%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [346765],
 'movieName': ['小飞象'],
 'releaseInfo': ['上映22天'],
 'seatRate': ['6.2%'],
 'showInfo': [2194],
 'showRate': ['0.6%'],
 'showView': ['5'],
 'splitBoxInfo': ['34.54'],
 'splitSumBoxInfo': ['13000.0'],
 'sumBoxInfo': ['14200.0'],
 'yearRate': ['2019-04-19#13']}
{'boxInfo': ['37.22'],
 'boxRate': ['0.6%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [346765],
 'movieName': ['小飞象'],
 'releaseInfo': ['上映22天'],
 'seatRate': ['6.2%'],
 'showInfo': [2194],
 'showRate': ['0.6%'],
 'showView': ['5'],
 'splitBoxInfo': ['34.54'],
 'splitSumBoxInfo': ['13000.0'],
 'sumBoxInfo': ['14200.0'],
 'yearRate': ['2019-04-19#13']}
2020-04-18 01:12:27,717 -  movie_spider.py[line:143] - WARNING: get 8 movie info, named None.
2020-04-18 01:12:27,718 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['38.73'],
 'boxRate': ['1.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [346765],
 'movieName': ['小飞象'],
 'releaseInfo': ['上映21天'],
 'seatRate': ['3.7%'],
 'showInfo': [5994],
 'showRate': ['1.9%'],
 'showView': ['2'],
 'splitBoxInfo': ['35.60'],
 'splitSumBoxInfo': ['13000.0'],
 'sumBoxInfo': ['14100.0'],
 'yearRate': ['2019-04-18#8']}
{'boxInfo': ['38.73'],
 'boxRate': ['1.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [346765],
 'movieName': ['小飞象'],
 'releaseInfo': ['上映21天'],
 'seatRate': ['3.7%'],
 'showInfo': [5994],
 'showRate': ['1.9%'],
 'showView': ['2'],
 'splitBoxInfo': ['35.60'],
 'splitSumBoxInfo': ['13000.0'],
 'sumBoxInfo': ['14100.0'],
 'yearRate': ['2019-04-18#8']}
2020-04-18 01:12:27,720 -  movie_spider.py[line:143] - WARNING: get 9 movie info, named None.
2020-04-18 01:12:27,720 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['36.80'],
 'boxRate': ['1.0%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1216383],
 'movieName': ['比悲伤更悲伤的故事'],
 'releaseInfo': ['上映36天'],
 'seatRate': ['3.4%'],
 'showInfo': [5956],
 'showRate': ['1.8%'],
 'showView': ['3'],
 'splitBoxInfo': ['33.18'],
 'splitSumBoxInfo': ['85600.0'],
 'sumBoxInfo': ['95500.0'],
 'yearRate': ['2019-04-18#9']}
{'boxInfo': ['36.80'],
 'boxRate': ['1.0%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1216383],
 'movieName': ['比悲伤更悲伤的故事'],
 'releaseInfo': ['上映36天'],
 'seatRate': ['3.4%'],
 'showInfo': [5956],
 'showRate': ['1.8%'],
 'showView': ['3'],
 'splitBoxInfo': ['33.18'],
 'splitSumBoxInfo': ['85600.0'],
 'sumBoxInfo': ['95500.0'],
 'yearRate': ['2019-04-18#9']}
2020-04-18 01:12:27,722 -  movie_spider.py[line:143] - WARNING: get 10 movie info, named None.
2020-04-18 01:12:27,722 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['29.60'],
 'boxRate': ['0.8%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1167831],
 'movieName': ['波西米亚狂想曲'],
 'releaseInfo': ['上映28天'],
 'seatRate': ['4.5%'],
 'showInfo': [2483],
 'showRate': ['0.7%'],
 'showView': ['4'],
 'splitBoxInfo': ['26.69'],
 'splitSumBoxInfo': ['8688.0'],
 'sumBoxInfo': ['9590.5'],
 'yearRate': ['2019-04-18#10']}
{'boxInfo': ['29.60'],
 'boxRate': ['0.8%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1167831],
 'movieName': ['波西米亚狂想曲'],
 'releaseInfo': ['上映28天'],
 'seatRate': ['4.5%'],
 'showInfo': [2483],
 'showRate': ['0.7%'],
 'showView': ['4'],
 'splitBoxInfo': ['26.69'],
 'splitSumBoxInfo': ['8688.0'],
 'sumBoxInfo': ['9590.5'],
 'yearRate': ['2019-04-18#10']}
2020-04-18 01:12:27,723 -  movie_spider.py[line:143] - WARNING: get 11 movie info, named None.
2020-04-18 01:12:27,724 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['29.57'],
 'boxRate': ['0.8%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [345875],
 'movieName': ['风中有朵雨做的云'],
 'releaseInfo': ['上映15天'],
 'seatRate': ['3.5%'],
 'showInfo': [4158],
 'showRate': ['1.3%'],
 'showView': ['2'],
 'splitBoxInfo': ['27.32'],
 'splitSumBoxInfo': ['5880.6'],
 'sumBoxInfo': ['6417.6'],
 'yearRate': ['2019-04-18#11']}
{'boxInfo': ['29.57'],
 'boxRate': ['0.8%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [345875],
 'movieName': ['风中有朵雨做的云'],
 'releaseInfo': ['上映15天'],
 'seatRate': ['3.5%'],
 'showInfo': [4158],
 'showRate': ['1.3%'],
 'showView': ['2'],
 'splitBoxInfo': ['27.32'],
 'splitSumBoxInfo': ['5880.6'],
 'sumBoxInfo': ['6417.6'],
 'yearRate': ['2019-04-18#11']}
2020-04-18 01:12:27,725 -  movie_spider.py[line:143] - WARNING: get 12 movie info, named None.
2020-04-18 01:12:27,726 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['26.64'],
 'boxRate': ['0.7%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1213175],
 'movieName': ['在乎你'],
 'releaseInfo': ['上映7天'],
 'seatRate': ['2.7%'],
 'showInfo': [6366],
 'showRate': ['2.0%'],
 'showView': ['1'],
 'splitBoxInfo': ['25.63'],
 'splitSumBoxInfo': ['593.3'],
 'sumBoxInfo': ['624.5'],
 'yearRate': ['2019-04-18#12']}
{'boxInfo': ['26.64'],
 'boxRate': ['0.7%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1213175],
 'movieName': ['在乎你'],
 'releaseInfo': ['上映7天'],
 'seatRate': ['2.7%'],
 'showInfo': [6366],
 'showRate': ['2.0%'],
 'showView': ['1'],
 'splitBoxInfo': ['25.63'],
 'splitSumBoxInfo': ['593.3'],
 'sumBoxInfo': ['624.5'],
 'yearRate': ['2019-04-18#12']}
2020-04-18 01:12:27,727 -  movie_spider.py[line:143] - WARNING: get 13 movie info, named None.
2020-04-18 01:12:27,728 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['19.80'],
 'boxRate': ['0.5%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1219670],
 'movieName': ['海市蜃楼'],
 'releaseInfo': ['上映22天'],
 'seatRate': ['3.7%'],
 'showInfo': [2511],
 'showRate': ['0.7%'],
 'showView': ['3'],
 'splitBoxInfo': ['17.96'],
 'splitSumBoxInfo': ['9761.3'],
 'sumBoxInfo': ['10800.0'],
 'yearRate': ['2019-04-18#13']}
{'boxInfo': ['19.80'],
 'boxRate': ['0.5%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1219670],
 'movieName': ['海市蜃楼'],
 'releaseInfo': ['上映22天'],
 'seatRate': ['3.7%'],
 'showInfo': [2511],
 'showRate': ['0.7%'],
 'showView': ['3'],
 'splitBoxInfo': ['17.96'],
 'splitSumBoxInfo': ['9761.3'],
 'sumBoxInfo': ['10800.0'],
 'yearRate': ['2019-04-18#13']}
2020-04-18 01:12:27,729 -  movie_spider.py[line:143] - WARNING: get 14 movie info, named None.
2020-04-18 01:12:27,730 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['26.64'],
 'boxRate': ['0.4%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1216383],
 'movieName': ['比悲伤更悲伤的故事'],
 'releaseInfo': ['上映37天'],
 'seatRate': ['4.2%'],
 'showInfo': [2861],
 'showRate': ['0.7%'],
 'showView': ['4'],
 'splitBoxInfo': ['24.19'],
 'splitSumBoxInfo': ['85600.0'],
 'sumBoxInfo': ['95500.0'],
 'yearRate': ['2019-04-19#14']}
{'boxInfo': ['26.64'],
 'boxRate': ['0.4%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1216383],
 'movieName': ['比悲伤更悲伤的故事'],
 'releaseInfo': ['上映37天'],
 'seatRate': ['4.2%'],
 'showInfo': [2861],
 'showRate': ['0.7%'],
 'showView': ['4'],
 'splitBoxInfo': ['24.19'],
 'splitSumBoxInfo': ['85600.0'],
 'sumBoxInfo': ['95500.0'],
 'yearRate': ['2019-04-19#14']}
2020-04-18 01:12:27,731 -  movie_spider.py[line:143] - WARNING: get 15 movie info, named None.
2020-04-18 01:12:27,732 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['15.59'],
 'boxRate': ['0.2%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [248906],
 'movieName': ['流浪地球'],
 'releaseInfo': [''],
 'seatRate': ['5.5%'],
 'showInfo': [932],
 'showRate': ['0.2%'],
 'showView': ['6'],
 'splitBoxInfo': ['14.54'],
 'splitSumBoxInfo': ['435200.0'],
 'sumBoxInfo': ['467800.0'],
 'yearRate': ['2019-04-19#15']}
{'boxInfo': ['15.59'],
 'boxRate': ['0.2%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [248906],
 'movieName': ['流浪地球'],
 'releaseInfo': [''],
 'seatRate': ['5.5%'],
 'showInfo': [932],
 'showRate': ['0.2%'],
 'showView': ['6'],
 'splitBoxInfo': ['14.54'],
 'splitSumBoxInfo': ['435200.0'],
 'sumBoxInfo': ['467800.0'],
 'yearRate': ['2019-04-19#15']}
2020-04-18 01:12:27,733 -  movie_spider.py[line:143] - WARNING: get 16 movie info, named None.
2020-04-18 01:12:27,734 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['15.11'],
 'boxRate': ['0.2%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1238775],
 'movieName': ['特别追踪'],
 'releaseInfo': [''],
 'seatRate': ['47.9%'],
 'showInfo': [71],
 'showRate': ['<0.1%'],
 'showView': ['43'],
 'splitBoxInfo': ['15.10'],
 'splitSumBoxInfo': ['398.1'],
 'sumBoxInfo': ['398.2'],
 'yearRate': ['2019-04-19#16']}
{'boxInfo': ['15.11'],
 'boxRate': ['0.2%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1238775],
 'movieName': ['特别追踪'],
 'releaseInfo': [''],
 'seatRate': ['47.9%'],
 'showInfo': [71],
 'showRate': ['<0.1%'],
 'showView': ['43'],
 'splitBoxInfo': ['15.10'],
 'splitSumBoxInfo': ['398.1'],
 'sumBoxInfo': ['398.2'],
 'yearRate': ['2019-04-19#16']}
2020-04-18 01:12:27,735 -  movie_spider.py[line:143] - WARNING: get 17 movie info, named None.
2020-04-18 01:12:27,736 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['13.73'],
 'boxRate': ['0.2%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [345875],
 'movieName': ['风中有朵雨做的云'],
 'releaseInfo': ['上映16天'],
 'seatRate': ['5.0%'],
 'showInfo': [1094],
 'showRate': ['0.3%'],
 'showView': ['4'],
 'splitBoxInfo': ['12.74'],
 'splitSumBoxInfo': ['5893.4'],
 'sumBoxInfo': ['6431.3'],
 'yearRate': ['2019-04-19#17']}
{'boxInfo': ['13.73'],
 'boxRate': ['0.2%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [345875],
 'movieName': ['风中有朵雨做的云'],
 'releaseInfo': ['上映16天'],
 'seatRate': ['5.0%'],
 'showInfo': [1094],
 'showRate': ['0.3%'],
 'showView': ['4'],
 'splitBoxInfo': ['12.74'],
 'splitSumBoxInfo': ['5893.4'],
 'sumBoxInfo': ['6431.3'],
 'yearRate': ['2019-04-19#17']}
2020-04-18 01:12:27,737 -  movie_spider.py[line:143] - WARNING: get 18 movie info, named None.
2020-04-18 01:12:27,738 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['13.47'],
 'boxRate': ['0.2%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1236912],
 'movieName': ['远去的牧歌'],
 'releaseInfo': [''],
 'seatRate': ['10.2%'],
 'showInfo': [133],
 'showRate': ['<0.1%'],
 'showView': ['24'],
 'splitBoxInfo': ['13.45'],
 'splitSumBoxInfo': ['750.5'],
 'sumBoxInfo': ['755.1'],
 'yearRate': ['2019-04-19#18']}
{'boxInfo': ['13.47'],
 'boxRate': ['0.2%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1236912],
 'movieName': ['远去的牧歌'],
 'releaseInfo': [''],
 'seatRate': ['10.2%'],
 'showInfo': [133],
 'showRate': ['<0.1%'],
 'showView': ['24'],
 'splitBoxInfo': ['13.45'],
 'splitSumBoxInfo': ['750.5'],
 'sumBoxInfo': ['755.1'],
 'yearRate': ['2019-04-19#18']}
2020-04-18 01:12:27,743 -  movie_spider.py[line:143] - WARNING: get 14 movie info, named None.
2020-04-18 01:12:27,744 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['15.89'],
 'boxRate': ['0.4%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1156894],
 'movieName': ['欲念游戏'],
 'releaseInfo': ['上映7天'],
 'seatRate': ['3.0%'],
 'showInfo': [10298],
 'showRate': ['3.2%'],
 'showView': ['1'],
 'splitBoxInfo': ['15.16'],
 'splitSumBoxInfo': ['683.4'],
 'sumBoxInfo': ['728.5'],
 'yearRate': ['2019-04-18#14']}
{'boxInfo': ['15.89'],
 'boxRate': ['0.4%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1156894],
 'movieName': ['欲念游戏'],
 'releaseInfo': ['上映7天'],
 'seatRate': ['3.0%'],
 'showInfo': [10298],
 'showRate': ['3.2%'],
 'showView': ['1'],
 'splitBoxInfo': ['15.16'],
 'splitSumBoxInfo': ['683.4'],
 'sumBoxInfo': ['728.5'],
 'yearRate': ['2019-04-18#14']}
2020-04-18 01:12:27,745 -  movie_spider.py[line:143] - WARNING: get 15 movie info, named None.
2020-04-18 01:12:27,746 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['15.15'],
 'boxRate': ['0.4%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1236912],
 'movieName': ['远去的牧歌'],
 'releaseInfo': [''],
 'seatRate': ['10.4%'],
 'showInfo': [258],
 'showRate': ['<0.1%'],
 'showView': ['14'],
 'splitBoxInfo': ['15.12'],
 'splitSumBoxInfo': ['737.1'],
 'sumBoxInfo': ['741.6'],
 'yearRate': ['2019-04-18#15']}
{'boxInfo': ['15.15'],
 'boxRate': ['0.4%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1236912],
 'movieName': ['远去的牧歌'],
 'releaseInfo': [''],
 'seatRate': ['10.4%'],
 'showInfo': [258],
 'showRate': ['<0.1%'],
 'showView': ['14'],
 'splitBoxInfo': ['15.12'],
 'splitSumBoxInfo': ['737.1'],
 'sumBoxInfo': ['741.6'],
 'yearRate': ['2019-04-18#15']}
2020-04-18 01:12:27,747 -  movie_spider.py[line:143] - WARNING: get 16 movie info, named None.
2020-04-18 01:12:27,747 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['14.19'],
 'boxRate': ['0.4%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [248906],
 'movieName': ['流浪地球'],
 'releaseInfo': [''],
 'seatRate': ['4.4%'],
 'showInfo': [1578],
 'showRate': ['0.5%'],
 'showView': ['3'],
 'splitBoxInfo': ['13.11'],
 'splitSumBoxInfo': ['435200.0'],
 'sumBoxInfo': ['467800.0'],
 'yearRate': ['2019-04-18#16']}
{'boxInfo': ['14.19'],
 'boxRate': ['0.4%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [248906],
 'movieName': ['流浪地球'],
 'releaseInfo': [''],
 'seatRate': ['4.4%'],
 'showInfo': [1578],
 'showRate': ['0.5%'],
 'showView': ['3'],
 'splitBoxInfo': ['13.11'],
 'splitSumBoxInfo': ['435200.0'],
 'sumBoxInfo': ['467800.0'],
 'yearRate': ['2019-04-18#16']}
2020-04-18 01:12:27,748 -  movie_spider.py[line:143] - WARNING: get 17 movie info, named None.
2020-04-18 01:12:27,750 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['7.89'],
 'boxRate': ['0.2%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1206415],
 'movieName': ['虫林大作战'],
 'releaseInfo': ['上映6天'],
 'seatRate': ['2.0%'],
 'showInfo': [3856],
 'showRate': ['1.2%'],
 'showView': ['1'],
 'splitBoxInfo': ['7.33'],
 'splitSumBoxInfo': ['642.0'],
 'sumBoxInfo': ['699.0'],
 'yearRate': ['2019-04-18#17']}
{'boxInfo': ['7.89'],
 'boxRate': ['0.2%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1206415],
 'movieName': ['虫林大作战'],
 'releaseInfo': ['上映6天'],
 'seatRate': ['2.0%'],
 'showInfo': [3856],
 'showRate': ['1.2%'],
 'showView': ['1'],
 'splitBoxInfo': ['7.33'],
 'splitSumBoxInfo': ['642.0'],
 'sumBoxInfo': ['699.0'],
 'yearRate': ['2019-04-18#17']}
2020-04-18 01:12:27,751 -  movie_spider.py[line:143] - WARNING: get 18 movie info, named None.
2020-04-18 01:12:27,752 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['7.02'],
 'boxRate': ['0.2%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1239678],
 'movieName': ['原来如此'],
 'releaseInfo': ['上映7天'],
 'seatRate': ['4.3%'],
 'showInfo': [1309],
 'showRate': ['0.4%'],
 'showView': ['2'],
 'splitBoxInfo': ['6.40'],
 'splitSumBoxInfo': ['80.3'],
 'sumBoxInfo': ['87.5'],
 'yearRate': ['2019-04-18#18']}
{'boxInfo': ['7.02'],
 'boxRate': ['0.2%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1239678],
 'movieName': ['原来如此'],
 'releaseInfo': ['上映7天'],
 'seatRate': ['4.3%'],
 'showInfo': [1309],
 'showRate': ['0.4%'],
 'showView': ['2'],
 'splitBoxInfo': ['6.40'],
 'splitSumBoxInfo': ['80.3'],
 'sumBoxInfo': ['87.5'],
 'yearRate': ['2019-04-18#18']}
2020-04-18 01:12:27,753 -  movie_spider.py[line:143] - WARNING: get 19 movie info, named None.
2020-04-18 01:12:27,754 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['12.79'],
 'boxRate': ['0.2%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1230119],
 'movieName': ['难以置信'],
 'releaseInfo': ['上映9天'],
 'seatRate': ['49.0%'],
 'showInfo': [93],
 'showRate': ['<0.1%'],
 'showView': ['33'],
 'splitBoxInfo': ['12.79'],
 'splitSumBoxInfo': ['47.6'],
 'sumBoxInfo': ['48.7'],
 'yearRate': ['2019-04-19#19']}
{'boxInfo': ['12.79'],
 'boxRate': ['0.2%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1230119],
 'movieName': ['难以置信'],
 'releaseInfo': ['上映9天'],
 'seatRate': ['49.0%'],
 'showInfo': [93],
 'showRate': ['<0.1%'],
 'showView': ['33'],
 'splitBoxInfo': ['12.79'],
 'splitSumBoxInfo': ['47.6'],
 'sumBoxInfo': ['48.7'],
 'yearRate': ['2019-04-19#19']}
2020-04-18 01:12:27,755 -  movie_spider.py[line:143] - WARNING: get 20 movie info, named None.
2020-04-18 01:12:27,756 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['11.85'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1259639],
 'movieName': ['毕业旅行之逍遥骑士'],
 'releaseInfo': ['上映首日'],
 'seatRate': ['5.6%'],
 'showInfo': [1382],
 'showRate': ['0.3%'],
 'showView': ['3'],
 'splitBoxInfo': ['11.76'],
 'splitSumBoxInfo': ['11.7'],
 'sumBoxInfo': ['11.8'],
 'yearRate': ['2019-04-19#20']}
{'boxInfo': ['11.85'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1259639],
 'movieName': ['毕业旅行之逍遥骑士'],
 'releaseInfo': ['上映首日'],
 'seatRate': ['5.6%'],
 'showInfo': [1382],
 'showRate': ['0.3%'],
 'showView': ['3'],
 'splitBoxInfo': ['11.76'],
 'splitSumBoxInfo': ['11.7'],
 'sumBoxInfo': ['11.8'],
 'yearRate': ['2019-04-19#20']}
2020-04-18 01:12:27,757 -  movie_spider.py[line:143] - WARNING: get 21 movie info, named None.
2020-04-18 01:12:27,758 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['11.12'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1213175],
 'movieName': ['在乎你'],
 'releaseInfo': ['上映8天'],
 'seatRate': ['2.5%'],
 'showInfo': [545],
 'showRate': ['0.1%'],
 'showView': ['4'],
 'splitBoxInfo': ['11.00'],
 'splitSumBoxInfo': ['604.3'],
 'sumBoxInfo': ['635.7'],
 'yearRate': ['2019-04-19#21']}
{'boxInfo': ['11.12'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1213175],
 'movieName': ['在乎你'],
 'releaseInfo': ['上映8天'],
 'seatRate': ['2.5%'],
 'showInfo': [545],
 'showRate': ['0.1%'],
 'showView': ['4'],
 'splitBoxInfo': ['11.00'],
 'splitSumBoxInfo': ['604.3'],
 'sumBoxInfo': ['635.7'],
 'yearRate': ['2019-04-19#21']}
2020-04-18 01:12:27,759 -  movie_spider.py[line:143] - WARNING: get 22 movie info, named None.
2020-04-18 01:12:27,760 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['10.68'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1249315],
 'movieName': ['大路朝天'],
 'releaseInfo': [''],
 'seatRate': ['41.3%'],
 'showInfo': [46],
 'showRate': ['<0.1%'],
 'showView': ['117'],
 'splitBoxInfo': ['10.68'],
 'splitSumBoxInfo': ['2563.4'],
 'sumBoxInfo': ['2564.6'],
 'yearRate': ['2019-04-19#22']}
{'boxInfo': ['10.68'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1249315],
 'movieName': ['大路朝天'],
 'releaseInfo': [''],
 'seatRate': ['41.3%'],
 'showInfo': [46],
 'showRate': ['<0.1%'],
 'showView': ['117'],
 'splitBoxInfo': ['10.68'],
 'splitSumBoxInfo': ['2563.4'],
 'sumBoxInfo': ['2564.6'],
 'yearRate': ['2019-04-19#22']}
2020-04-18 01:12:27,761 -  movie_spider.py[line:143] - WARNING: get 23 movie info, named None.
2020-04-18 01:12:27,762 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['7.08'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [330115],
 'movieName': ['我和神马查干'],
 'releaseInfo': [''],
 'seatRate': ['--'],
 'showInfo': [179],
 'showRate': ['<0.1%'],
 'showView': ['15'],
 'splitBoxInfo': ['7.07'],
 'splitSumBoxInfo': ['1629.7'],
 'sumBoxInfo': ['1635.0'],
 'yearRate': ['2019-04-19#23']}
{'boxInfo': ['7.08'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [330115],
 'movieName': ['我和神马查干'],
 'releaseInfo': [''],
 'seatRate': ['--'],
 'showInfo': [179],
 'showRate': ['<0.1%'],
 'showView': ['15'],
 'splitBoxInfo': ['7.07'],
 'splitSumBoxInfo': ['1629.7'],
 'sumBoxInfo': ['1635.0'],
 'yearRate': ['2019-04-19#23']}
2020-04-18 01:12:27,764 -  movie_spider.py[line:143] - WARNING: get 19 movie info, named None.
2020-04-18 01:12:27,765 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['6.65'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1206605],
 'movieName': ['绿皮书'],
 'releaseInfo': ['重映49天'],
 'seatRate': ['4.2%'],
 'showInfo': [794],
 'showRate': ['0.2%'],
 'showView': ['3'],
 'splitBoxInfo': ['6.06'],
 'splitSumBoxInfo': ['43100.0'],
 'sumBoxInfo': ['47800.0'],
 'yearRate': ['2019-04-18#19']}
{'boxInfo': ['6.65'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1206605],
 'movieName': ['绿皮书'],
 'releaseInfo': ['重映49天'],
 'seatRate': ['4.2%'],
 'showInfo': [794],
 'showRate': ['0.2%'],
 'showView': ['3'],
 'splitBoxInfo': ['6.06'],
 'splitSumBoxInfo': ['43100.0'],
 'sumBoxInfo': ['47800.0'],
 'yearRate': ['2019-04-18#19']}
2020-04-18 01:12:27,766 -  movie_spider.py[line:143] - WARNING: get 20 movie info, named None.
2020-04-18 01:12:27,767 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['6.64'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1238775],
 'movieName': ['特别追踪'],
 'releaseInfo': [''],
 'seatRate': ['37.4%'],
 'showInfo': [45],
 'showRate': ['<0.1%'],
 'showView': ['30'],
 'splitBoxInfo': ['6.64'],
 'splitSumBoxInfo': ['383.0'],
 'sumBoxInfo': ['383.1'],
 'yearRate': ['2019-04-18#20']}
{'boxInfo': ['6.64'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1238775],
 'movieName': ['特别追踪'],
 'releaseInfo': [''],
 'seatRate': ['37.4%'],
 'showInfo': [45],
 'showRate': ['<0.1%'],
 'showView': ['30'],
 'splitBoxInfo': ['6.64'],
 'splitSumBoxInfo': ['383.0'],
 'sumBoxInfo': ['383.1'],
 'yearRate': ['2019-04-18#20']}
2020-04-18 01:12:27,768 -  movie_spider.py[line:143] - WARNING: get 21 movie info, named None.
2020-04-18 01:12:27,769 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['6.59'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1249315],
 'movieName': ['大路朝天'],
 'releaseInfo': [''],
 'seatRate': ['35.2%'],
 'showInfo': [31],
 'showRate': ['<0.1%'],
 'showView': ['107'],
 'splitBoxInfo': ['6.59'],
 'splitSumBoxInfo': ['2552.7'],
 'sumBoxInfo': ['2553.9'],
 'yearRate': ['2019-04-18#21']}
{'boxInfo': ['6.59'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1249315],
 'movieName': ['大路朝天'],
 'releaseInfo': [''],
 'seatRate': ['35.2%'],
 'showInfo': [31],
 'showRate': ['<0.1%'],
 'showView': ['107'],
 'splitBoxInfo': ['6.59'],
 'splitSumBoxInfo': ['2552.7'],
 'sumBoxInfo': ['2553.9'],
 'yearRate': ['2019-04-18#21']}
2020-04-18 01:12:27,770 -  movie_spider.py[line:143] - WARNING: get 22 movie info, named None.
2020-04-18 01:12:27,770 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['6.52'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1238926],
 'movieName': ['我们的爱情'],
 'releaseInfo': ['上映42天'],
 'seatRate': ['21.0%'],
 'showInfo': [28],
 'showRate': ['<0.1%'],
 'showView': ['75'],
 'splitBoxInfo': ['6.52'],
 'splitSumBoxInfo': ['35.9'],
 'sumBoxInfo': ['36.2'],
 'yearRate': ['2019-04-18#22']}
{'boxInfo': ['6.52'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1238926],
 'movieName': ['我们的爱情'],
 'releaseInfo': ['上映42天'],
 'seatRate': ['21.0%'],
 'showInfo': [28],
 'showRate': ['<0.1%'],
 'showView': ['75'],
 'splitBoxInfo': ['6.52'],
 'splitSumBoxInfo': ['35.9'],
 'sumBoxInfo': ['36.2'],
 'yearRate': ['2019-04-18#22']}
2020-04-18 01:12:27,771 -  movie_spider.py[line:143] - WARNING: get 23 movie info, named None.
2020-04-18 01:12:27,772 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['5.54'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [330115],
 'movieName': ['我和神马查干'],
 'releaseInfo': [''],
 'seatRate': ['--'],
 'showInfo': [184],
 'showRate': ['<0.1%'],
 'showView': ['11'],
 'splitBoxInfo': ['5.53'],
 'splitSumBoxInfo': ['1622.6'],
 'sumBoxInfo': ['1627.9'],
 'yearRate': ['2019-04-18#23']}
{'boxInfo': ['5.54'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [330115],
 'movieName': ['我和神马查干'],
 'releaseInfo': [''],
 'seatRate': ['--'],
 'showInfo': [184],
 'showRate': ['<0.1%'],
 'showView': ['11'],
 'splitBoxInfo': ['5.53'],
 'splitSumBoxInfo': ['1622.6'],
 'sumBoxInfo': ['1627.9'],
 'yearRate': ['2019-04-18#23']}
2020-04-18 01:12:27,773 -  movie_spider.py[line:143] - WARNING: get 24 movie info, named None.
2020-04-18 01:12:27,774 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['4.86'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1229702],
 'movieName': ['青蛙王子历险记'],
 'releaseInfo': ['上映14天'],
 'seatRate': ['1.4%'],
 'showInfo': [567],
 'showRate': ['0.1%'],
 'showView': ['2'],
 'splitBoxInfo': ['4.83'],
 'splitSumBoxInfo': ['724.2'],
 'sumBoxInfo': ['779.8'],
 'yearRate': ['2019-04-18#24']}
{'boxInfo': ['4.86'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1229702],
 'movieName': ['青蛙王子历险记'],
 'releaseInfo': ['上映14天'],
 'seatRate': ['1.4%'],
 'showInfo': [567],
 'showRate': ['0.1%'],
 'showView': ['2'],
 'splitBoxInfo': ['4.83'],
 'splitSumBoxInfo': ['724.2'],
 'sumBoxInfo': ['779.8'],
 'yearRate': ['2019-04-18#24']}
2020-04-18 01:12:27,775 -  movie_spider.py[line:143] - WARNING: get 24 movie info, named None.
2020-04-18 01:12:27,776 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['7.03'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [368260],
 'movieName': ['最佳男友进化论'],
 'releaseInfo': ['上映8天'],
 'seatRate': ['2.8%'],
 'showInfo': [2678],
 'showRate': ['0.7%'],
 'showView': ['1'],
 'splitBoxInfo': ['6.40'],
 'splitSumBoxInfo': ['1281.8'],
 'sumBoxInfo': ['1411.9'],
 'yearRate': ['2019-04-19#24']}
{'boxInfo': ['7.03'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [368260],
 'movieName': ['最佳男友进化论'],
 'releaseInfo': ['上映8天'],
 'seatRate': ['2.8%'],
 'showInfo': [2678],
 'showRate': ['0.7%'],
 'showView': ['1'],
 'splitBoxInfo': ['6.40'],
 'splitSumBoxInfo': ['1281.8'],
 'sumBoxInfo': ['1411.9'],
 'yearRate': ['2019-04-19#24']}
2020-04-18 01:12:27,777 -  movie_spider.py[line:143] - WARNING: get 25 movie info, named None.
2020-04-18 01:12:27,778 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['6.87'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1239678],
 'movieName': ['原来如此'],
 'releaseInfo': ['上映8天'],
 'seatRate': ['4.6%'],
 'showInfo': [600],
 'showRate': ['0.1%'],
 'showView': ['4'],
 'splitBoxInfo': ['6.37'],
 'splitSumBoxInfo': ['86.7'],
 'sumBoxInfo': ['94.4'],
 'yearRate': ['2019-04-19#25']}
{'boxInfo': ['6.87'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1239678],
 'movieName': ['原来如此'],
 'releaseInfo': ['上映8天'],
 'seatRate': ['4.6%'],
 'showInfo': [600],
 'showRate': ['0.1%'],
 'showView': ['4'],
 'splitBoxInfo': ['6.37'],
 'splitSumBoxInfo': ['86.7'],
 'sumBoxInfo': ['94.4'],
 'yearRate': ['2019-04-19#25']}
2020-04-18 01:12:27,779 -  movie_spider.py[line:143] - WARNING: get 26 movie info, named None.
2020-04-18 01:12:27,779 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['6.13'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1262586],
 'movieName': ['Hello 北京'],
 'releaseInfo': ['上映首日'],
 'seatRate': ['9.2%'],
 'showInfo': [1561],
 'showRate': ['0.4%'],
 'showView': ['2'],
 'splitBoxInfo': ['5.80'],
 'splitSumBoxInfo': ['9.2'],
 'sumBoxInfo': ['9.6'],
 'yearRate': ['2019-04-19#26']}
{'boxInfo': ['6.13'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1262586],
 'movieName': ['Hello 北京'],
 'releaseInfo': ['上映首日'],
 'seatRate': ['9.2%'],
 'showInfo': [1561],
 'showRate': ['0.4%'],
 'showView': ['2'],
 'splitBoxInfo': ['5.80'],
 'splitSumBoxInfo': ['9.2'],
 'sumBoxInfo': ['9.6'],
 'yearRate': ['2019-04-19#26']}
2020-04-18 01:12:27,781 -  movie_spider.py[line:143] - WARNING: get 27 movie info, named None.
2020-04-18 01:12:27,781 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['5.94'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1206605],
 'movieName': ['绿皮书'],
 'releaseInfo': ['重映50天'],
 'seatRate': ['7.0%'],
 'showInfo': [290],
 'showRate': ['<0.1%'],
 'showView': ['6'],
 'splitBoxInfo': ['5.59'],
 'splitSumBoxInfo': ['43100.0'],
 'sumBoxInfo': ['47800.0'],
 'yearRate': ['2019-04-19#27']}
{'boxInfo': ['5.94'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1206605],
 'movieName': ['绿皮书'],
 'releaseInfo': ['重映50天'],
 'seatRate': ['7.0%'],
 'showInfo': [290],
 'showRate': ['<0.1%'],
 'showView': ['6'],
 'splitBoxInfo': ['5.59'],
 'splitSumBoxInfo': ['43100.0'],
 'sumBoxInfo': ['47800.0'],
 'yearRate': ['2019-04-19#27']}
2020-04-18 01:12:27,783 -  movie_spider.py[line:143] - WARNING: get 28 movie info, named None.
2020-04-18 01:12:27,784 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['5.15'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1229702],
 'movieName': ['青蛙王子历险记'],
 'releaseInfo': ['上映15天'],
 'seatRate': ['2.2%'],
 'showInfo': [161],
 'showRate': ['<0.1%'],
 'showView': ['7'],
 'splitBoxInfo': ['5.15'],
 'splitSumBoxInfo': ['729.4'],
 'sumBoxInfo': ['785.0'],
 'yearRate': ['2019-04-19#28']}
{'boxInfo': ['5.15'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1229702],
 'movieName': ['青蛙王子历险记'],
 'releaseInfo': ['上映15天'],
 'seatRate': ['2.2%'],
 'showInfo': [161],
 'showRate': ['<0.1%'],
 'showView': ['7'],
 'splitBoxInfo': ['5.15'],
 'splitSumBoxInfo': ['729.4'],
 'sumBoxInfo': ['785.0'],
 'yearRate': ['2019-04-19#28']}
2020-04-18 01:12:27,785 -  movie_spider.py[line:143] - WARNING: get 29 movie info, named None.
2020-04-18 01:12:27,785 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['4.89'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1257272],
 'movieName': ['暗语者'],
 'releaseInfo': ['上映8天'],
 'seatRate': ['41.3%'],
 'showInfo': [38],
 'showRate': ['<0.1%'],
 'showView': ['50'],
 'splitBoxInfo': ['4.89'],
 'splitSumBoxInfo': ['10.6'],
 'sumBoxInfo': ['10.7'],
 'yearRate': ['2019-04-19#29']}
{'boxInfo': ['4.89'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1257272],
 'movieName': ['暗语者'],
 'releaseInfo': ['上映8天'],
 'seatRate': ['41.3%'],
 'showInfo': [38],
 'showRate': ['<0.1%'],
 'showView': ['50'],
 'splitBoxInfo': ['4.89'],
 'splitSumBoxInfo': ['10.6'],
 'sumBoxInfo': ['10.7'],
 'yearRate': ['2019-04-19#29']}
2020-04-18 01:12:27,789 -  movie_spider.py[line:143] - WARNING: get 25 movie info, named None.
2020-04-18 01:12:27,790 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['4.85'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [343344],
 'movieName': ['我的宠物是大象'],
 'releaseInfo': ['上映7天'],
 'seatRate': ['2.5%'],
 'showInfo': [4623],
 'showRate': ['1.4%'],
 'showView': ['1'],
 'splitBoxInfo': ['4.64'],
 'splitSumBoxInfo': ['236.1'],
 'sumBoxInfo': ['249.1'],
 'yearRate': ['2019-04-18#25']}
{'boxInfo': ['4.85'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [343344],
 'movieName': ['我的宠物是大象'],
 'releaseInfo': ['上映7天'],
 'seatRate': ['2.5%'],
 'showInfo': [4623],
 'showRate': ['1.4%'],
 'showView': ['1'],
 'splitBoxInfo': ['4.64'],
 'splitSumBoxInfo': ['236.1'],
 'sumBoxInfo': ['249.1'],
 'yearRate': ['2019-04-18#25']}
2020-04-18 01:12:27,791 -  movie_spider.py[line:143] - WARNING: get 26 movie info, named None.
2020-04-18 01:12:27,792 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['4.72'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1206939],
 'movieName': ['守灵'],
 'releaseInfo': ['上映15天'],
 'seatRate': ['4.3%'],
 'showInfo': [746],
 'showRate': ['0.2%'],
 'showView': ['3'],
 'splitBoxInfo': ['4.28'],
 'splitSumBoxInfo': ['274.5'],
 'sumBoxInfo': ['305.6'],
 'yearRate': ['2019-04-18#26']}
{'boxInfo': ['4.72'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1206939],
 'movieName': ['守灵'],
 'releaseInfo': ['上映15天'],
 'seatRate': ['4.3%'],
 'showInfo': [746],
 'showRate': ['0.2%'],
 'showView': ['3'],
 'splitBoxInfo': ['4.28'],
 'splitSumBoxInfo': ['274.5'],
 'sumBoxInfo': ['305.6'],
 'yearRate': ['2019-04-18#26']}
2020-04-18 01:12:27,793 -  movie_spider.py[line:143] - WARNING: get 27 movie info, named None.
2020-04-18 01:12:27,794 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['4.63'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [410629],
 'movieName': ['阿丽塔：战斗天使'],
 'releaseInfo': ['上映56天'],
 'seatRate': ['3.8%'],
 'showInfo': [434],
 'showRate': ['0.1%'],
 'showView': ['3'],
 'splitBoxInfo': ['4.25'],
 'splitSumBoxInfo': ['81900.0'],
 'sumBoxInfo': ['89600.0'],
 'yearRate': ['2019-04-18#27']}
{'boxInfo': ['4.63'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [410629],
 'movieName': ['阿丽塔：战斗天使'],
 'releaseInfo': ['上映56天'],
 'seatRate': ['3.8%'],
 'showInfo': [434],
 'showRate': ['0.1%'],
 'showView': ['3'],
 'splitBoxInfo': ['4.25'],
 'splitSumBoxInfo': ['81900.0'],
 'sumBoxInfo': ['89600.0'],
 'yearRate': ['2019-04-18#27']}
2020-04-18 01:12:27,796 -  movie_spider.py[line:143] - WARNING: get 28 movie info, named None.
2020-04-18 01:12:27,797 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['4.23'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1162868],
 'movieName': ['转型团伙'],
 'releaseInfo': ['零点场'],
 'seatRate': ['21.7%'],
 'showInfo': [66],
 'showRate': ['<0.1%'],
 'showView': ['19'],
 'splitBoxInfo': ['3.98'],
 'splitSumBoxInfo': ['5.6'],
 'sumBoxInfo': ['5.9'],
 'yearRate': ['2019-04-18#28']}
{'boxInfo': ['4.23'],
 'boxRate': ['0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [1162868],
 'movieName': ['转型团伙'],
 'releaseInfo': ['零点场'],
 'seatRate': ['21.7%'],
 'showInfo': [66],
 'showRate': ['<0.1%'],
 'showView': ['19'],
 'splitBoxInfo': ['3.98'],
 'splitSumBoxInfo': ['5.6'],
 'sumBoxInfo': ['5.9'],
 'yearRate': ['2019-04-18#28']}
2020-04-18 01:12:27,798 -  movie_spider.py[line:143] - WARNING: get 29 movie info, named None.
2020-04-18 01:12:27,799 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['3.27'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [507792],
 'movieName': ['地久天长'],
 'releaseInfo': ['上映28天'],
 'seatRate': ['5.1%'],
 'showInfo': [418],
 'showRate': ['0.1%'],
 'showView': ['3'],
 'splitBoxInfo': ['3.05'],
 'splitSumBoxInfo': ['4157.6'],
 'sumBoxInfo': ['4477.1'],
 'yearRate': ['2019-04-18#29']}
{'boxInfo': ['3.27'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [507792],
 'movieName': ['地久天长'],
 'releaseInfo': ['上映28天'],
 'seatRate': ['5.1%'],
 'showInfo': [418],
 'showRate': ['0.1%'],
 'showView': ['3'],
 'splitBoxInfo': ['3.05'],
 'splitSumBoxInfo': ['4157.6'],
 'sumBoxInfo': ['4477.1'],
 'yearRate': ['2019-04-18#29']}
2020-04-18 01:12:27,800 -  movie_spider.py[line:143] - WARNING: get 30 movie info, named None.
2020-04-18 01:12:27,800 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['4.69'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1156894],
 'movieName': ['欲念游戏'],
 'releaseInfo': ['上映8天'],
 'seatRate': ['4.3%'],
 'showInfo': [1324],
 'showRate': ['0.3%'],
 'showView': ['2'],
 'splitBoxInfo': ['4.61'],
 'splitSumBoxInfo': ['688.0'],
 'sumBoxInfo': ['733.2'],
 'yearRate': ['2019-04-19#30']}
{'boxInfo': ['4.69'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-19'],
 'movieID': [1156894],
 'movieName': ['欲念游戏'],
 'releaseInfo': ['上映8天'],
 'seatRate': ['4.3%'],
 'showInfo': [1324],
 'showRate': ['0.3%'],
 'showView': ['2'],
 'splitBoxInfo': ['4.61'],
 'splitSumBoxInfo': ['688.0'],
 'sumBoxInfo': ['733.2'],
 'yearRate': ['2019-04-19#30']}
2020-04-18 01:12:27,801 -  movie_spider.py[line:153] - ERROR: boxOffice start parse
2020-04-18 01:12:27,803 -  movie_spider.py[line:143] - WARNING: get 30 movie info, named None.
2020-04-18 01:12:27,804 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'boxInfo': ['2.39'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [643506],
 'movieName': ['古镇画情'],
 'releaseInfo': ['展映'],
 'seatRate': ['--'],
 'showInfo': [72],
 'showRate': ['<0.1%'],
 'showView': ['11'],
 'splitBoxInfo': ['2.39'],
 'splitSumBoxInfo': ['985.4'],
 'sumBoxInfo': ['988.0'],
 'yearRate': ['2019-04-18#30']}
{'boxInfo': ['2.39'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-18'],
 'movieID': [643506],
 'movieName': ['古镇画情'],
 'releaseInfo': ['展映'],
 'seatRate': ['--'],
 'showInfo': [72],
 'showRate': ['<0.1%'],
 'showView': ['11'],
 'splitBoxInfo': ['2.39'],
 'splitSumBoxInfo': ['985.4'],
 'sumBoxInfo': ['988.0'],
 'yearRate': ['2019-04-18#30']}
2020-04-18 01:12:27,804 -  movie_spider.py[line:153] - ERROR: boxOffice start parse
2020-04-18 01:12:27,983 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E8%80%81%E5%B8%88%C2%B7%E5%A5%BD> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:27,999 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%B3%A2%E8%A5%BF%E7%B1%B3%E4%BA%9A%E7%8B%82%E6%83%B3%E6%9B%B2> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,007 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E7%85%A7%E7%9B%B8%E5%B8%88> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,014 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%B5%B7%E5%B8%82%E8%9C%83%E6%A5%BC> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,020 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E5%8F%8D%E8%B4%AA%E9%A3%8E%E6%9A%B44> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,026 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E9%9B%B7%E9%9C%86%E6%B2%99%E8%B5%9E%EF%BC%81> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,031 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E8%B0%83%E9%9F%B3%E5%B8%88> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,042 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E5%A2%83%C2%B7%E7%95%8C> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,085 -  movie_spider.py[line:164] - ERROR: movie_name = 老师·好 and movie_year = 2019-04-18 and tpp_id = 1212492
2020-04-18 01:12:28,085 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E8%80%81%E5%B8%88%C2%B7%E5%A5%BD
2020-04-18 01:12:28,085 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,085 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,099 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E7%A5%9E%E5%A5%87%E4%B9%90%E5%9B%AD%E5%8E%86%E9%99%A9%E8%AE%B0> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,107 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E8%BD%AC%E5%9E%8B%E5%9B%A2%E4%BC%99> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,115 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%B5%81%E6%B5%AA%E5%9C%B0%E7%90%83> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,125 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E7%89%B9%E5%88%AB%E8%BF%BD%E8%B8%AA> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,131 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E9%A3%8E%E4%B8%AD%E6%9C%89%E6%9C%B5%E9%9B%A8%E5%81%9A%E7%9A%84%E4%BA%91> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,136 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E5%A6%82%E5%BD%B1%E9%9A%8F%E5%BF%83> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,141 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E5%9C%A8%E4%B9%8E%E4%BD%A0> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,147 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E7%A5%88%E7%A5%B7%E8%90%BD%E5%B9%95%E6%97%B6> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,147 -  movie_spider.py[line:164] - ERROR: movie_name = 波西米亚狂想曲 and movie_year = 2019-04-19 and tpp_id = 1167831
2020-04-18 01:12:28,147 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E6%B3%A2%E8%A5%BF%E7%B1%B3%E4%BA%9A%E7%8B%82%E6%83%B3%E6%9B%B2
2020-04-18 01:12:28,147 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,147 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,148 -  movie_spider.py[line:164] - ERROR: movie_name = 照相师 and movie_year = 2019-04-18 and tpp_id = 1228750
2020-04-18 01:12:28,148 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E7%85%A7%E7%9B%B8%E5%B8%88
2020-04-18 01:12:28,148 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,148 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,149 -  movie_spider.py[line:164] - ERROR: movie_name = 海市蜃楼 and movie_year = 2019-04-19 and tpp_id = 1219670
2020-04-18 01:12:28,149 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E6%B5%B7%E5%B8%82%E8%9C%83%E6%A5%BC
2020-04-18 01:12:28,149 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,150 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,157 -  movie_spider.py[line:164] - ERROR: movie_name = 反贪风暴4 and movie_year = 2019-04-19 and tpp_id = 1211727
2020-04-18 01:12:28,157 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E5%8F%8D%E8%B4%AA%E9%A3%8E%E6%9A%B44
2020-04-18 01:12:28,157 -  movie_spider.py[line:170] - ERROR: len of text is 1
2020-04-18 01:12:28,157 -  movie_spider.py[line:183] - ERROR: get movie info url = https://movie.douban.com/subject/27202819/?suggest=%E5%8F%8D%E8%B4%AA%E9%A3%8E%E6%9A%B44
2020-04-18 01:12:28,159 -  movie_spider.py[line:164] - ERROR: movie_name = 雷霆沙赞！ and movie_year = 2019-04-18 and tpp_id = 248123
2020-04-18 01:12:28,159 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E9%9B%B7%E9%9C%86%E6%B2%99%E8%B5%9E%EF%BC%81
2020-04-18 01:12:28,159 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,159 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,160 -  movie_spider.py[line:164] - ERROR: movie_name = 调音师 and movie_year = 2019-04-19 and tpp_id = 1239544
2020-04-18 01:12:28,160 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E8%B0%83%E9%9F%B3%E5%B8%88
2020-04-18 01:12:28,160 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,160 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,161 -  movie_spider.py[line:164] - ERROR: movie_name = 境·界 and movie_year = 2019-04-19 and tpp_id = 581264
2020-04-18 01:12:28,161 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E5%A2%83%C2%B7%E7%95%8C
2020-04-18 01:12:28,161 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,161 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,179 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E5%8F%A4%E9%95%87%E7%94%BB%E6%83%85> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,186 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E5%9C%B0%E4%B9%85%E5%A4%A9%E9%95%BF> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,206 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%9A%97%E8%AF%AD%E8%80%85> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,207 -  movie_spider.py[line:164] - ERROR: movie_name = 神奇乐园历险记 and movie_year = 2019-04-19 and tpp_id = 1211412
2020-04-18 01:12:28,207 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E7%A5%9E%E5%A5%87%E4%B9%90%E5%9B%AD%E5%8E%86%E9%99%A9%E8%AE%B0
2020-04-18 01:12:28,207 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,207 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,209 -  movie_spider.py[line:164] - ERROR: movie_name = 转型团伙 and movie_year = 2019-04-19 and tpp_id = 1162868
2020-04-18 01:12:28,209 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E8%BD%AC%E5%9E%8B%E5%9B%A2%E4%BC%99
2020-04-18 01:12:28,209 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,209 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,216 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E9%98%BF%E4%B8%BD%E5%A1%94%EF%BC%9A%E6%88%98%E6%96%97%E5%A4%A9%E4%BD%BF> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,224 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%88%91%E7%9A%84%E5%AE%A0%E7%89%A9%E6%98%AF%E5%A4%A7%E8%B1%A1> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,229 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E5%AE%88%E7%81%B5> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,229 -  movie_spider.py[line:164] - ERROR: movie_name = 流浪地球 and movie_year = 2019-04-19 and tpp_id = 248906
2020-04-18 01:12:28,230 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E6%B5%81%E6%B5%AA%E5%9C%B0%E7%90%83
2020-04-18 01:12:28,230 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,230 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,239 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=Hello%20%E5%8C%97%E4%BA%AC> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,245 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E9%9D%92%E8%9B%99%E7%8E%8B%E5%AD%90%E5%8E%86%E9%99%A9%E8%AE%B0> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,246 -  movie_spider.py[line:164] - ERROR: movie_name = 特别追踪 and movie_year = 2019-04-19 and tpp_id = 1238775
2020-04-18 01:12:28,246 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E7%89%B9%E5%88%AB%E8%BF%BD%E8%B8%AA
2020-04-18 01:12:28,246 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,246 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,253 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E7%BB%BF%E7%9A%AE%E4%B9%A6> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,258 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%88%91%E4%BB%AC%E7%9A%84%E7%88%B1%E6%83%85> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,259 -  movie_spider.py[line:164] - ERROR: movie_name = 风中有朵雨做的云 and movie_year = 2019-04-18 and tpp_id = 345875
2020-04-18 01:12:28,259 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E9%A3%8E%E4%B8%AD%E6%9C%89%E6%9C%B5%E9%9B%A8%E5%81%9A%E7%9A%84%E4%BA%91
2020-04-18 01:12:28,259 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,259 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,260 -  movie_spider.py[line:164] - ERROR: movie_name = 如影随心 and movie_year = 2019-04-19 and tpp_id = 1207260
2020-04-18 01:12:28,260 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E5%A6%82%E5%BD%B1%E9%9A%8F%E5%BF%83
2020-04-18 01:12:28,261 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,261 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,261 -  movie_spider.py[line:164] - ERROR: movie_name = 在乎你 and movie_year = 2019-04-18 and tpp_id = 1213175
2020-04-18 01:12:28,262 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E5%9C%A8%E4%B9%8E%E4%BD%A0
2020-04-18 01:12:28,262 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,262 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,269 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%88%91%E5%92%8C%E7%A5%9E%E9%A9%AC%E6%9F%A5%E5%B9%B2> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,269 -  movie_spider.py[line:164] - ERROR: movie_name = 祈祷落幕时 and movie_year = 2019-04-19 and tpp_id = 1205909
2020-04-18 01:12:28,269 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E7%A5%88%E7%A5%B7%E8%90%BD%E5%B9%95%E6%97%B6
2020-04-18 01:12:28,269 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,269 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,282 -  movie_spider.py[line:164] - ERROR: movie_name = 古镇画情 and movie_year = 2019-04-18 and tpp_id = 643506
2020-04-18 01:12:28,282 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E5%8F%A4%E9%95%87%E7%94%BB%E6%83%85
2020-04-18 01:12:28,283 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,283 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,294 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E5%A4%A7%E8%B7%AF%E6%9C%9D%E5%A4%A9> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,295 -  movie_spider.py[line:164] - ERROR: movie_name = 地久天长 and movie_year = 2019-04-18 and tpp_id = 507792
2020-04-18 01:12:28,295 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E5%9C%B0%E4%B9%85%E5%A4%A9%E9%95%BF
2020-04-18 01:12:28,295 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,295 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,311 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E9%9A%BE%E4%BB%A5%E7%BD%AE%E4%BF%A1> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,317 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E5%8E%9F%E6%9D%A5%E5%A6%82%E6%AD%A4> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,317 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%AF%95%E4%B8%9A%E6%97%85%E8%A1%8C%E4%B9%8B%E9%80%8D%E9%81%A5%E9%AA%91%E5%A3%AB> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190419)
2020-04-18 01:12:28,317 -  movie_spider.py[line:164] - ERROR: movie_name = 暗语者 and movie_year = 2019-04-19 and tpp_id = 1257272
2020-04-18 01:12:28,318 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E6%9A%97%E8%AF%AD%E8%80%85
2020-04-18 01:12:28,318 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,318 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,319 -  movie_spider.py[line:164] - ERROR: movie_name = 阿丽塔：战斗天使 and movie_year = 2019-04-18 and tpp_id = 410629
2020-04-18 01:12:28,319 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E9%98%BF%E4%B8%BD%E5%A1%94%EF%BC%9A%E6%88%98%E6%96%97%E5%A4%A9%E4%BD%BF
2020-04-18 01:12:28,319 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,319 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,321 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E8%99%AB%E6%9E%97%E5%A4%A7%E4%BD%9C%E6%88%98> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,326 -  movie_spider.py[line:164] - ERROR: movie_name = 我的宠物是大象 and movie_year = 2019-04-18 and tpp_id = 343344
2020-04-18 01:12:28,326 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E6%88%91%E7%9A%84%E5%AE%A0%E7%89%A9%E6%98%AF%E5%A4%A7%E8%B1%A1
2020-04-18 01:12:28,326 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,326 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,331 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E8%BF%9C%E5%8E%BB%E7%9A%84%E7%89%A7%E6%AD%8C> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,333 -  movie_spider.py[line:164] - ERROR: movie_name = 守灵 and movie_year = 2019-04-18 and tpp_id = 1206939
2020-04-18 01:12:28,333 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E5%AE%88%E7%81%B5
2020-04-18 01:12:28,333 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,333 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,336 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%AC%B2%E5%BF%B5%E6%B8%B8%E6%88%8F> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,341 -  movie_spider.py[line:164] - ERROR: movie_name = Hello 北京 and movie_year = 2019-04-19 and tpp_id = 1262586
2020-04-18 01:12:28,342 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=Hello%20%E5%8C%97%E4%BA%AC
2020-04-18 01:12:28,342 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,342 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,347 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%AF%94%E6%82%B2%E4%BC%A4%E6%9B%B4%E6%82%B2%E4%BC%A4%E7%9A%84%E6%95%85%E4%BA%8B> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,347 -  movie_spider.py[line:164] - ERROR: movie_name = 青蛙王子历险记 and movie_year = 2019-04-19 and tpp_id = 1229702
2020-04-18 01:12:28,347 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E9%9D%92%E8%9B%99%E7%8E%8B%E5%AD%90%E5%8E%86%E9%99%A9%E8%AE%B0
2020-04-18 01:12:28,347 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,347 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,354 -  movie_spider.py[line:164] - ERROR: movie_name = 绿皮书 and movie_year = 2019-04-18 and tpp_id = 1206605
2020-04-18 01:12:28,354 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E7%BB%BF%E7%9A%AE%E4%B9%A6
2020-04-18 01:12:28,354 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,355 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,360 -  movie_spider.py[line:164] - ERROR: movie_name = 我们的爱情 and movie_year = 2019-04-18 and tpp_id = 1238926
2020-04-18 01:12:28,361 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E6%88%91%E4%BB%AC%E7%9A%84%E7%88%B1%E6%83%85
2020-04-18 01:12:28,361 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,361 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,370 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%9C%80%E4%BD%B3%E7%94%B7%E5%8F%8B%E8%BF%9B%E5%8C%96%E8%AE%BA> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,371 -  movie_spider.py[line:164] - ERROR: movie_name = 我和神马查干 and movie_year = 2019-04-19 and tpp_id = 330115
2020-04-18 01:12:28,371 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E6%88%91%E5%92%8C%E7%A5%9E%E9%A9%AC%E6%9F%A5%E5%B9%B2
2020-04-18 01:12:28,371 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,371 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,376 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E5%B0%8F%E9%A3%9E%E8%B1%A1> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190418)
2020-04-18 01:12:28,396 -  movie_spider.py[line:164] - ERROR: movie_name = 大路朝天 and movie_year = 2019-04-19 and tpp_id = 1249315
2020-04-18 01:12:28,396 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E5%A4%A7%E8%B7%AF%E6%9C%9D%E5%A4%A9
2020-04-18 01:12:28,396 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,396 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,413 -  movie_spider.py[line:164] - ERROR: movie_name = 难以置信 and movie_year = 2019-04-19 and tpp_id = 1230119
2020-04-18 01:12:28,413 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E9%9A%BE%E4%BB%A5%E7%BD%AE%E4%BF%A1
2020-04-18 01:12:28,413 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,413 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,418 -  movie_spider.py[line:164] - ERROR: movie_name = 原来如此 and movie_year = 2019-04-18 and tpp_id = 1239678
2020-04-18 01:12:28,418 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E5%8E%9F%E6%9D%A5%E5%A6%82%E6%AD%A4
2020-04-18 01:12:28,418 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,418 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,421 -  movie_spider.py[line:164] - ERROR: movie_name = 毕业旅行之逍遥骑士 and movie_year = 2019-04-19 and tpp_id = 1259639
2020-04-18 01:12:28,421 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E6%AF%95%E4%B8%9A%E6%97%85%E8%A1%8C%E4%B9%8B%E9%80%8D%E9%81%A5%E9%AA%91%E5%A3%AB
2020-04-18 01:12:28,421 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,421 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,425 -  movie_spider.py[line:164] - ERROR: movie_name = 虫林大作战 and movie_year = 2019-04-18 and tpp_id = 1206415
2020-04-18 01:12:28,425 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E8%99%AB%E6%9E%97%E5%A4%A7%E4%BD%9C%E6%88%98
2020-04-18 01:12:28,425 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,425 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,433 -  movie_spider.py[line:164] - ERROR: movie_name = 远去的牧歌 and movie_year = 2019-04-18 and tpp_id = 1236912
2020-04-18 01:12:28,433 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E8%BF%9C%E5%8E%BB%E7%9A%84%E7%89%A7%E6%AD%8C
2020-04-18 01:12:28,434 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,434 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,439 -  movie_spider.py[line:164] - ERROR: movie_name = 欲念游戏 and movie_year = 2019-04-18 and tpp_id = 1156894
2020-04-18 01:12:28,439 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E6%AC%B2%E5%BF%B5%E6%B8%B8%E6%88%8F
2020-04-18 01:12:28,439 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,439 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,448 -  movie_spider.py[line:164] - ERROR: movie_name = 比悲伤更悲伤的故事 and movie_year = 2019-04-18 and tpp_id = 1216383
2020-04-18 01:12:28,449 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E6%AF%94%E6%82%B2%E4%BC%A4%E6%9B%B4%E6%82%B2%E4%BC%A4%E7%9A%84%E6%95%85%E4%BA%8B
2020-04-18 01:12:28,449 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,449 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,472 -  movie_spider.py[line:164] - ERROR: movie_name = 最佳男友进化论 and movie_year = 2019-04-18 and tpp_id = 368260
2020-04-18 01:12:28,472 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E6%9C%80%E4%BD%B3%E7%94%B7%E5%8F%8B%E8%BF%9B%E5%8C%96%E8%AE%BA
2020-04-18 01:12:28,472 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,472 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:28,478 -  movie_spider.py[line:164] - ERROR: movie_name = 小飞象 and movie_year = 2019-04-18 and tpp_id = 346765
2020-04-18 01:12:28,479 -  movie_spider.py[line:169] - ERROR: the url is https://movie.douban.com/j/subject_suggest?q=%E5%B0%8F%E9%A3%9E%E8%B1%A1
2020-04-18 01:12:28,479 -  movie_spider.py[line:170] - ERROR: len of text is 0
2020-04-18 01:12:28,479 -  movie_spider.py[line:172] - ERROR: not response scraped
2020-04-18 01:12:29,446 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/subject/27202819/?suggest=%E5%8F%8D%E8%B4%AA%E9%A3%8E%E6%9A%B44> (referer: https://movie.douban.com/j/subject_suggest?q=%E5%8F%8D%E8%B4%AA%E9%A3%8E%E6%9A%B44)
2020-04-18 01:12:29,548 -  movie_spider.py[line:187] - CRITICAL: crawled movie info of https://movie.douban.com/subject/27202819/?suggest=%E5%8F%8D%E8%B4%AA%E9%A3%8E%E6%9A%B44
2020-04-18 01:12:29,592 -  movie_spider.py[line:191] - CRITICAL: type of data is <class 'str'>
2020-04-18 01:12:29,593 -  movie_spider.py[line:199] - ERROR: len of movie info = 13
2020-04-18 01:12:29,595 -  pipelines.py[line:50] - INFO: item is new
2020-04-18 01:12:29,596 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.MovieInfoItem'>
2020-04-18 01:12:29,598 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-18 01:12:29,649 -  log.py[line:110] - INFO: SHOW VARIABLES LIKE 'sql_mode'
2020-04-18 01:12:29,649 -  log.py[line:110] - INFO: {}
2020-04-18 01:12:29,664 -  log.py[line:110] - INFO: SHOW VARIABLES LIKE 'lower_case_table_names'
2020-04-18 01:12:29,665 -  log.py[line:110] - INFO: {}
2020-04-18 01:12:29,670 -  log.py[line:110] - INFO: SELECT DATABASE()
2020-04-18 01:12:29,670 -  log.py[line:110] - INFO: {}
2020-04-18 01:12:29,672 -  log.py[line:110] - INFO: show collation where `Charset` = 'utf8mb4' and `Collation` = 'utf8mb4_bin'
2020-04-18 01:12:29,672 -  log.py[line:110] - INFO: {}
2020-04-18 01:12:29,688 -  log.py[line:110] - INFO: SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1
2020-04-18 01:12:29,689 -  log.py[line:110] - INFO: {}
2020-04-18 01:12:29,690 -  log.py[line:110] - INFO: SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1
2020-04-18 01:12:29,691 -  log.py[line:110] - INFO: {}
2020-04-18 01:12:29,693 -  log.py[line:110] - INFO: SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8mb4) COLLATE utf8mb4_bin AS anon_1
2020-04-18 01:12:29,693 -  log.py[line:110] - INFO: {}
2020-04-18 01:12:29,726 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-18 01:12:29,729 -  log.py[line:110] - INFO: INSERT INTO `movieInfo` (`dbMovieID`, `tppMovieID`, `movieName`, directors, writers, actors, genre, area, duration, `publishedDate`, `rateCount`, `doubanRate`) VALUES (%(dbMovieID)s, %(tppMovieID)s, %(movieName)s, %(directors)s, %(writers)s, %(actors)s, %(genre)s, %(area)s, %(duration)s, %(publishedDate)s, %(rateCount)s, %(doubanRate)s)
2020-04-18 01:12:29,730 -  log.py[line:110] - INFO: {'dbMovieID': '27202819', 'tppMovieID': 1211727, 'movieName': '反贪风暴4 P風暴', 'directors': '林德禄', 'writers': '黄浩华/何文龙', 'actors': '古天乐/郑嘉颖/林峯/林家栋/周秀娜/张智霖/谭耀文/张继聪/蔡瀚亿/廖启智', 'genre': '动作/犯罪', 'area': ' 中国香港 / 中国大陆', 'duration': 96, 'publishedDate': '2019-04-04', 'rateCount': '131408', 'doubanRate': '6.0'}
2020-04-18 01:12:29,732 -  log.py[line:110] - INFO: ROLLBACK
2020-04-18 01:12:29,787 -  scraper.py[line:236] - ERROR: Error processing {'actors': '古天乐/郑嘉颖/林峯/林家栋/周秀娜/张智霖/谭耀文/张继聪/蔡瀚亿/廖启智',
 'area': ' 中国香港 / 中国大陆',
 'dbMovieID': ['27202819'],
 'directors': '林德禄',
 'doubanRate': ['6.0'],
 'duration': [96],
 'genre': '动作/犯罪',
 'movieName': ['反贪风暴4 P風暴'],
 'publishedDate': ['2019-04-04'],
 'rateCount': ['131408'],
 'tppMovieID': [1211727],
 'writers': '黄浩华/何文龙'}
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1244, in _execute_context
    cursor, statement, parameters, context
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 552, in do_execute
    cursor.execute(statement, parameters)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/pymysql/cursors.py", line 170, in execute
    result = self._query(query)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/pymysql/cursors.py", line 328, in _query
    conn.query(q)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/pymysql/connections.py", line 517, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/pymysql/connections.py", line 732, in _read_query_result
    result.read()
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/pymysql/connections.py", line 1075, in read
    first_packet = self.connection._read_packet()
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/pymysql/connections.py", line 684, in _read_packet
    packet.check_error()
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/pymysql/protocol.py", line 220, in check_error
    err.raise_mysql_exception(self._data)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/pymysql/err.py", line 109, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.IntegrityError: (1062, "Duplicate entry '27202819' for key 'movieInfo.movieInfo_dbMocieID_uindex'")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/pipelines.py", line 84, in process_item
    self.session.commit()
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1026, in commit
    self.transaction.commit()
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 493, in commit
    self._prepare_impl()
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 472, in _prepare_impl
    self.session.flush()
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2451, in flush
    self._flush(objects)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2589, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 129, in reraise
    raise value
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2549, in _flush
    flush_context.execute()
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1120, in _emit_insert_statements
    statement, params
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 988, in execute
    return meth(self, multiparams, params)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 287, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1107, in _execute_clauseelement
    distilled_params,
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1248, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1466, in _handle_dbapi_exception
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 383, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 128, in reraise
    raise value.with_traceback(tb)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1244, in _execute_context
    cursor, statement, parameters, context
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 552, in do_execute
    cursor.execute(statement, parameters)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/pymysql/cursors.py", line 170, in execute
    result = self._query(query)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/pymysql/cursors.py", line 328, in _query
    conn.query(q)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/pymysql/connections.py", line 517, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/pymysql/connections.py", line 732, in _read_query_result
    result.read()
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/pymysql/connections.py", line 1075, in read
    first_packet = self.connection._read_packet()
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/pymysql/connections.py", line 684, in _read_packet
    packet.check_error()
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/pymysql/protocol.py", line 220, in check_error
    err.raise_mysql_exception(self._data)
  File "/home/dawinia/anaconda3/lib/python3.7/site-packages/pymysql/err.py", line 109, in raise_mysql_exception
    raise errorclass(errno, errval)
sqlalchemy.exc.IntegrityError: (pymysql.err.IntegrityError) (1062, "Duplicate entry '27202819' for key 'movieInfo.movieInfo_dbMocieID_uindex'")
[SQL: INSERT INTO `movieInfo` (`dbMovieID`, `tppMovieID`, `movieName`, directors, writers, actors, genre, area, duration, `publishedDate`, `rateCount`, `doubanRate`) VALUES (%(dbMovieID)s, %(tppMovieID)s, %(movieName)s, %(directors)s, %(writers)s, %(actors)s, %(genre)s, %(area)s, %(duration)s, %(publishedDate)s, %(rateCount)s, %(doubanRate)s)]
[parameters: {'dbMovieID': '27202819', 'tppMovieID': 1211727, 'movieName': '反贪风暴4 P風暴', 'directors': '林德禄', 'writers': '黄浩华/何文龙', 'actors': '古天乐/郑嘉颖/林峯/林家栋/周秀娜/张智霖/谭耀文/张继聪/蔡瀚亿/廖启智', 'genre': '动作/犯罪', 'area': ' 中国香港 / 中国大陆', 'duration': 96, 'publishedDate': '2019-04-04', 'rateCount': '131408', 'doubanRate': '6.0'}]
(Background on this error at: http://sqlalche.me/e/gkpj)
2020-04-18 01:12:29,806 -  engine.py[line:296] - INFO: Closing spider (finished)
2020-04-18 01:12:29,810 -  statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 48700,
 'downloader/request_count': 40,
 'downloader/request_method_count/GET': 40,
 'downloader/response_bytes': 50559,
 'downloader/response_count': 40,
 'downloader/response_status_count/200': 40,
 'dupefilter/filtered': 23,
 'elapsed_time_seconds': 2.667461,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 4, 17, 17, 12, 29, 809011),
 'item_dropped_count': 60,
 'item_dropped_reasons_count/DropItem': 60,
 'log_count/CRITICAL': 3,
 'log_count/ERROR': 158,
 'log_count/WARNING': 121,
 'memusage/max': 72245248,
 'memusage/startup': 72245248,
 'request_depth_max': 2,
 'response_received_count': 40,
 'scheduler/dequeued': 40,
 'scheduler/dequeued/memory': 40,
 'scheduler/enqueued': 40,
 'scheduler/enqueued/memory': 40,
 'start_time': datetime.datetime(2020, 4, 17, 17, 12, 27, 141550)}
2020-04-18 01:12:29,810 -  engine.py[line:327] - INFO: Spider closed (finished)
2020-04-24 00:44:31,271 -  log.py[line:146] - INFO: Scrapy 1.8.0 started (bot: movie)
2020-04-24 00:44:31,301 -  log.py[line:149] - INFO: Versions: lxml 4.4.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.8, Platform Linux-5.3.0-46-generic-x86_64-with-debian-buster-sid
2020-04-24 00:44:31,305 -  crawler.py[line:49] - INFO: Overridden settings: {'BOT_NAME': 'movie', 'COMMANDS_MODULE': 'movie.commands', 'LOG_LEVEL': 'WARNING', 'NEWSPIDER_MODULE': 'movie.spiders', 'SPIDER_MODULES': ['movie.spiders']}
2020-04-24 00:44:31,795 -  telnet.py[line:60] - INFO: Telnet Password: 01cc96ff7557e3d7
2020-04-24 00:44:32,997 -  middleware.py[line:48] - INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-04-24 00:44:43,088 -  utils.py[line:80] - DEBUG: Error occurred during fetching https://www.w3schools.com/browsers/default.asp
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1319, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1252, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1298, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1247, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 966, in send
    self.connect()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1414, in connect
    super().connect()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 938, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/socket.py", line 707, in create_connection
    for res in getaddrinfo(host, port, 0, SOCK_STREAM):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/socket.py", line 752, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/fake_useragent/utils.py", line 67, in get
    context=context,
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 222, in urlopen
    return opener.open(url, data, timeout)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 525, in open
    response = self._open(req, data)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 543, in _open
    '_open', req)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 503, in _call_chain
    result = func(*args)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1362, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1321, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno -2] Name or service not known>
2020-04-24 00:44:43,116 -  utils.py[line:88] - DEBUG: Sleeping for 0.1 seconds
2020-04-24 00:44:53,233 -  utils.py[line:80] - DEBUG: Error occurred during fetching https://www.w3schools.com/browsers/default.asp
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1319, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1252, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1298, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1247, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 966, in send
    self.connect()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1414, in connect
    super().connect()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 938, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/socket.py", line 707, in create_connection
    for res in getaddrinfo(host, port, 0, SOCK_STREAM):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/socket.py", line 752, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/fake_useragent/utils.py", line 67, in get
    context=context,
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 222, in urlopen
    return opener.open(url, data, timeout)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 525, in open
    response = self._open(req, data)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 543, in _open
    '_open', req)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 503, in _call_chain
    result = func(*args)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1362, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1321, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno -2] Name or service not known>
2020-04-24 00:44:53,233 -  utils.py[line:184] - WARNING: Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1319, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1252, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1298, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1247, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 966, in send
    self.connect()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1414, in connect
    super().connect()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 938, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/socket.py", line 707, in create_connection
    for res in getaddrinfo(host, port, 0, SOCK_STREAM):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/socket.py", line 752, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/fake_useragent/utils.py", line 67, in get
    context=context,
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 222, in urlopen
    return opener.open(url, data, timeout)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 525, in open
    response = self._open(req, data)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 543, in _open
    '_open', req)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 503, in _call_chain
    result = func(*args)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1362, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1321, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno -2] Name or service not known>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/fake_useragent/utils.py", line 154, in load
    for item in get_browsers(verify_ssl=verify_ssl):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/fake_useragent/utils.py", line 97, in get_browsers
    html = get(settings.BROWSERS_STATS_PAGE, verify_ssl=verify_ssl)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/fake_useragent/utils.py", line 84, in get
    raise FakeUserAgentError('Maximum amount of retries reached')
fake_useragent.errors.FakeUserAgentError: Maximum amount of retries reached
2020-04-24 00:45:03,253 -  utils.py[line:80] - DEBUG: Error occurred during fetching https://fake-useragent.herokuapp.com/browsers/0.1.11
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1319, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1252, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1298, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1247, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 966, in send
    self.connect()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1414, in connect
    super().connect()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 938, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/socket.py", line 707, in create_connection
    for res in getaddrinfo(host, port, 0, SOCK_STREAM):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/socket.py", line 752, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/fake_useragent/utils.py", line 67, in get
    context=context,
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 222, in urlopen
    return opener.open(url, data, timeout)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 525, in open
    response = self._open(req, data)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 543, in _open
    '_open', req)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 503, in _call_chain
    result = func(*args)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1362, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1321, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno -2] Name or service not known>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/fake_useragent/utils.py", line 154, in load
    for item in get_browsers(verify_ssl=verify_ssl):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/fake_useragent/utils.py", line 97, in get_browsers
    html = get(settings.BROWSERS_STATS_PAGE, verify_ssl=verify_ssl)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/fake_useragent/utils.py", line 84, in get
    raise FakeUserAgentError('Maximum amount of retries reached')
fake_useragent.errors.FakeUserAgentError: Maximum amount of retries reached

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1319, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1252, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1298, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1247, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 966, in send
    self.connect()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1414, in connect
    super().connect()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 938, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/socket.py", line 707, in create_connection
    for res in getaddrinfo(host, port, 0, SOCK_STREAM):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/socket.py", line 752, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -2] Name or service not known

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/fake_useragent/utils.py", line 67, in get
    context=context,
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 222, in urlopen
    return opener.open(url, data, timeout)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 525, in open
    response = self._open(req, data)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 543, in _open
    '_open', req)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 503, in _call_chain
    result = func(*args)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1362, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1321, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno -2] Name or service not known>
2020-04-24 00:45:03,253 -  utils.py[line:88] - DEBUG: Sleeping for 0.1 seconds
2020-04-24 00:45:20,815 -  crawler.py[line:276] - INFO: Received SIGINT, shutting down gracefully. Send again to force 
2020-04-24 00:45:23,570 -  crawler.py[line:283] - INFO: Received SIGINT twice, forcing unclean shutdown
2020-04-24 00:45:25,263 -  middleware.py[line:48] - INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'movie.middlewares.UserAgentMiddleware',
 'movie.middlewares.MovieDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-04-24 00:45:25,284 -  middleware.py[line:48] - INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'movie.middlewares.MovieSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-04-24 00:45:25,319 -  pipelines.py[line:16] - ERROR: start to use judge duplicate
2020-04-24 00:45:25,319 -  pipelines.py[line:62] - ERROR: setting: custom boxOffice setting
2020-04-24 00:45:25,463 -  pipelines.py[line:58] - ERROR: pipeline init
2020-04-24 00:45:25,464 -  middleware.py[line:48] - INFO: Enabled item pipelines:
['movie.pipelines.BoxOfficePipeline', 'movie.pipelines.MySQLPipeline']
2020-04-24 00:45:25,464 -  engine.py[line:257] - INFO: Spider opened
2020-04-24 00:45:25,470 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-24 00:45:25,471 -  signal.py[line:57] - ERROR: Error caught on signal handler: <bound method MovieDownloaderMiddleware.spider_opened of <movie.middlewares.MovieDownloaderMiddleware object at 0x7f86401f8b90>>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/middlewares.py", line 112, in spider_opened
    spider.logger.info('Spider opened: %s' % spider.module_name)
AttributeError: 'MovieSpider' object has no attribute 'module_name'
2020-04-24 00:45:25,501 -  signal.py[line:57] - ERROR: Error caught on signal handler: <bound method MovieSpiderMiddleware.spider_opened of <movie.middlewares.MovieSpiderMiddleware object at 0x7f86401c9890>>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/middlewares.py", line 65, in spider_opened
    spider.logger.info('Spider opened: %s' % spider.module_name)
AttributeError: 'MovieSpider' object has no attribute 'module_name'
2020-04-24 00:45:25,504 -  telnet.py[line:74] - INFO: Telnet console listening on 127.0.0.1:6023
2020-04-24 00:45:25,523 -  engine.py[line:296] - INFO: Closing spider (shutdown)
2020-04-24 00:45:25,525 -  statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 0.054225,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 4, 23, 16, 45, 25, 524501),
 'log_count/ERROR': 5,
 'log_count/WARNING': 1,
 'memusage/max': 74948608,
 'memusage/startup': 74948608,
 'start_time': datetime.datetime(2020, 4, 23, 16, 45, 25, 470276)}
2020-04-24 00:45:25,525 -  engine.py[line:327] - INFO: Spider closed (shutdown)
2020-04-24 00:46:34,880 -  log.py[line:146] - INFO: Scrapy 1.8.0 started (bot: movie)
2020-04-24 00:46:34,883 -  log.py[line:149] - INFO: Versions: lxml 4.4.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.8, Platform Linux-5.3.0-46-generic-x86_64-with-debian-buster-sid
2020-04-24 00:46:34,886 -  crawler.py[line:49] - INFO: Overridden settings: {'BOT_NAME': 'movie', 'COMMANDS_MODULE': 'movie.commands', 'LOG_LEVEL': 'WARNING', 'NEWSPIDER_MODULE': 'movie.spiders', 'SPIDER_MODULES': ['movie.spiders']}
2020-04-24 00:46:34,894 -  telnet.py[line:60] - INFO: Telnet Password: 2da1d5268136f348
2020-04-24 00:46:34,903 -  middleware.py[line:48] - INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-04-24 00:46:35,005 -  middleware.py[line:48] - INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'movie.middlewares.UserAgentMiddleware',
 'movie.middlewares.ProxyMiddleware',
 'movie.middlewares.MovieDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-04-24 00:46:35,007 -  middleware.py[line:48] - INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'movie.middlewares.MovieSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-04-24 00:46:35,013 -  pipelines.py[line:16] - ERROR: start to use judge duplicate
2020-04-24 00:46:35,013 -  pipelines.py[line:62] - ERROR: setting: custom boxOffice setting
2020-04-24 00:46:35,026 -  pipelines.py[line:58] - ERROR: pipeline init
2020-04-24 00:46:35,026 -  middleware.py[line:48] - INFO: Enabled item pipelines:
['movie.pipelines.BoxOfficePipeline', 'movie.pipelines.MySQLPipeline']
2020-04-24 00:46:35,026 -  engine.py[line:257] - INFO: Spider opened
2020-04-24 00:46:35,028 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-24 00:46:35,028 -  signal.py[line:57] - ERROR: Error caught on signal handler: <bound method MovieDownloaderMiddleware.spider_opened of <movie.middlewares.MovieDownloaderMiddleware object at 0x7fee9be92210>>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/middlewares.py", line 113, in spider_opened
    spider.logger.info('Spider opened: %s' % spider.module_name)
AttributeError: 'MovieSpider' object has no attribute 'module_name'
2020-04-24 00:46:35,030 -  telnet.py[line:74] - INFO: Telnet console listening on 127.0.0.1:6023
2020-04-24 00:46:35,032 -  movie_spider.py[line:96] - ERROR: boxOffice start request for url = http://piaofang.maoyan.com/dashboard-ajax/movie?
2020-04-24 00:46:35,168 -  scraper.py[line:209] - ERROR: Error downloading <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190424>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/middleware.py", line 38, in process_request
    response = yield method(request=request, spider=spider)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/middlewares.py", line 196, in process_request
    proxy = request.get('http://localhost:21642/random')
AttributeError: 'Request' object has no attribute 'get'
2020-04-24 00:46:35,185 -  scraper.py[line:209] - ERROR: Error downloading <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190425>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/middleware.py", line 38, in process_request
    response = yield method(request=request, spider=spider)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/middlewares.py", line 196, in process_request
    proxy = request.get('http://localhost:21642/random')
AttributeError: 'Request' object has no attribute 'get'
2020-04-24 00:46:35,287 -  engine.py[line:296] - INFO: Closing spider (finished)
2020-04-24 00:46:35,289 -  statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/builtins.AttributeError': 2,
 'elapsed_time_seconds': 0.260277,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 4, 23, 16, 46, 35, 288703),
 'log_count/ERROR': 7,
 'memusage/max': 73699328,
 'memusage/startup': 73699328,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2020, 4, 23, 16, 46, 35, 28426)}
2020-04-24 00:46:35,289 -  engine.py[line:327] - INFO: Spider closed (finished)
2020-04-24 00:46:53,543 -  log.py[line:146] - INFO: Scrapy 1.8.0 started (bot: movie)
2020-04-24 00:46:53,547 -  log.py[line:149] - INFO: Versions: lxml 4.4.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.8, Platform Linux-5.3.0-46-generic-x86_64-with-debian-buster-sid
2020-04-24 00:46:53,550 -  crawler.py[line:49] - INFO: Overridden settings: {'BOT_NAME': 'movie', 'COMMANDS_MODULE': 'movie.commands', 'LOG_LEVEL': 'WARNING', 'NEWSPIDER_MODULE': 'movie.spiders', 'SPIDER_MODULES': ['movie.spiders']}
2020-04-24 00:46:53,557 -  telnet.py[line:60] - INFO: Telnet Password: 7f763ec4b7333077
2020-04-24 00:46:53,566 -  middleware.py[line:48] - INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-04-24 00:46:53,667 -  middleware.py[line:48] - INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'movie.middlewares.UserAgentMiddleware',
 'movie.middlewares.ProxyMiddleware',
 'movie.middlewares.MovieDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-04-24 00:46:53,669 -  middleware.py[line:48] - INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'movie.middlewares.MovieSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-04-24 00:46:53,677 -  pipelines.py[line:16] - ERROR: start to use judge duplicate
2020-04-24 00:46:53,677 -  pipelines.py[line:62] - ERROR: setting: custom boxOffice setting
2020-04-24 00:46:53,691 -  pipelines.py[line:58] - ERROR: pipeline init
2020-04-24 00:46:53,691 -  middleware.py[line:48] - INFO: Enabled item pipelines:
['movie.pipelines.BoxOfficePipeline', 'movie.pipelines.MySQLPipeline']
2020-04-24 00:46:53,691 -  engine.py[line:257] - INFO: Spider opened
2020-04-24 00:46:53,693 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-24 00:46:53,694 -  signal.py[line:57] - ERROR: Error caught on signal handler: <bound method MovieDownloaderMiddleware.spider_opened of <movie.middlewares.MovieDownloaderMiddleware object at 0x7efd585372d0>>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/middlewares.py", line 113, in spider_opened
    spider.logger.info('Spider opened: %s' % spider.module_name)
AttributeError: 'MovieSpider' object has no attribute 'module_name'
2020-04-24 00:46:53,695 -  telnet.py[line:74] - INFO: Telnet console listening on 127.0.0.1:6023
2020-04-24 00:46:53,697 -  movie_spider.py[line:96] - ERROR: boxOffice start request for url = http://piaofang.maoyan.com/dashboard-ajax/movie?
2020-04-24 00:46:53,701 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 00:46:53,704 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 20
2020-04-24 00:46:53,707 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 00:46:53,711 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 24
2020-04-24 00:46:53,806 -  scraper.py[line:209] - ERROR: Error downloading <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190424>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/middleware.py", line 38, in process_request
    response = yield method(request=request, spider=spider)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/middlewares.py", line 199, in process_request
    logger.debug('使用代理 ' + proxy)
TypeError: can only concatenate str (not "Response") to str
2020-04-24 00:46:53,813 -  scraper.py[line:209] - ERROR: Error downloading <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190425>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/middleware.py", line 38, in process_request
    response = yield method(request=request, spider=spider)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/middlewares.py", line 199, in process_request
    logger.debug('使用代理 ' + proxy)
TypeError: can only concatenate str (not "Response") to str
2020-04-24 00:46:53,914 -  engine.py[line:296] - INFO: Closing spider (finished)
2020-04-24 00:46:53,915 -  statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/builtins.TypeError': 2,
 'elapsed_time_seconds': 0.221626,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 4, 23, 16, 46, 53, 915202),
 'log_count/ERROR': 7,
 'memusage/max': 73818112,
 'memusage/startup': 73818112,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2020, 4, 23, 16, 46, 53, 693576)}
2020-04-24 00:46:53,915 -  engine.py[line:327] - INFO: Spider closed (finished)
2020-04-24 01:00:07,111 -  log.py[line:146] - INFO: Scrapy 1.8.0 started (bot: movie)
2020-04-24 01:00:07,136 -  log.py[line:149] - INFO: Versions: lxml 4.4.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.8, Platform Linux-5.3.0-46-generic-x86_64-with-debian-buster-sid
2020-04-24 01:00:07,139 -  crawler.py[line:49] - INFO: Overridden settings: {'BOT_NAME': 'movie', 'COMMANDS_MODULE': 'movie.commands', 'LOG_LEVEL': 'WARNING', 'NEWSPIDER_MODULE': 'movie.spiders', 'SPIDER_MODULES': ['movie.spiders']}
2020-04-24 01:00:07,147 -  telnet.py[line:60] - INFO: Telnet Password: 6f281e4485ecda78
2020-04-24 01:00:07,156 -  middleware.py[line:48] - INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-04-24 01:00:07,258 -  middleware.py[line:48] - INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'movie.middlewares.UserAgentMiddleware',
 'movie.middlewares.ProxyMiddleware',
 'movie.middlewares.MovieDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-04-24 01:00:07,260 -  middleware.py[line:48] - INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'movie.middlewares.MovieSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-04-24 01:00:07,266 -  pipelines.py[line:16] - ERROR: start to use judge duplicate
2020-04-24 01:00:07,267 -  pipelines.py[line:62] - ERROR: setting: custom boxOffice setting
2020-04-24 01:00:07,280 -  pipelines.py[line:58] - ERROR: pipeline init
2020-04-24 01:00:07,280 -  middleware.py[line:48] - INFO: Enabled item pipelines:
['movie.pipelines.BoxOfficePipeline', 'movie.pipelines.MySQLPipeline']
2020-04-24 01:00:07,280 -  engine.py[line:257] - INFO: Spider opened
2020-04-24 01:00:07,281 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-24 01:00:07,281 -  signal.py[line:57] - ERROR: Error caught on signal handler: <bound method MovieDownloaderMiddleware.spider_opened of <movie.middlewares.MovieDownloaderMiddleware object at 0x7f059dbc4310>>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/middlewares.py", line 113, in spider_opened
    spider.logger.info('Spider opened: %s' % spider.module_name)
AttributeError: 'MovieSpider' object has no attribute 'module_name'
2020-04-24 01:00:07,283 -  telnet.py[line:74] - INFO: Telnet console listening on 127.0.0.1:6023
2020-04-24 01:00:07,285 -  movie_spider.py[line:96] - ERROR: boxOffice start request for url = http://piaofang.maoyan.com/dashboard-ajax/movie?
2020-04-24 01:00:07,288 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 01:00:07,292 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 20
2020-04-24 01:00:07,293 -  middlewares.py[line:198] - DEBUG: 使用代理 b'60.31.213.115':808
2020-04-24 01:00:07,296 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 01:00:07,299 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 22
2020-04-24 01:00:07,300 -  middlewares.py[line:198] - DEBUG: 使用代理 b'36.112.139.146':3128
2020-04-24 01:00:07,496 -  scraper.py[line:209] - ERROR: Error downloading <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190424>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/python/failure.py", line 512, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/middleware.py", line 44, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/utils/defer.py", line 45, in mustbe_deferred
    result = f(*args, **kw)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/__init__.py", line 71, in download_request
    return handler.download_request(request, spider)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 75, in download_request
    return agent.download_request(request)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 345, in download_request
    d = agent.request(method, to_bytes(url, encoding='ascii'), headers, bodyproducer)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 254, in request
    endpoint=self._getEndpoint(self._proxyURI),
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/web/client.py", line 1721, in _getEndpoint
    return self._endpointFactory.endpointForURI(uri)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/web/client.py", line 1599, in endpointForURI
    raise SchemeNotSupported("Unsupported scheme: %r" % (uri.scheme,))
twisted.web.error.SchemeNotSupported: Unsupported scheme: b''
2020-04-24 01:00:07,564 -  scraper.py[line:209] - ERROR: Error downloading <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190425>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/python/failure.py", line 512, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/middleware.py", line 44, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/utils/defer.py", line 45, in mustbe_deferred
    result = f(*args, **kw)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/__init__.py", line 71, in download_request
    return handler.download_request(request, spider)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 75, in download_request
    return agent.download_request(request)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 345, in download_request
    d = agent.request(method, to_bytes(url, encoding='ascii'), headers, bodyproducer)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 254, in request
    endpoint=self._getEndpoint(self._proxyURI),
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/web/client.py", line 1721, in _getEndpoint
    return self._endpointFactory.endpointForURI(uri)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/web/client.py", line 1599, in endpointForURI
    raise SchemeNotSupported("Unsupported scheme: %r" % (uri.scheme,))
twisted.web.error.SchemeNotSupported: Unsupported scheme: b''
2020-04-24 01:00:07,667 -  engine.py[line:296] - INFO: Closing spider (finished)
2020-04-24 01:00:07,669 -  statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.web.error.SchemeNotSupported': 2,
 'downloader/request_bytes': 576,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'elapsed_time_seconds': 0.386641,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 4, 23, 17, 0, 7, 668311),
 'log_count/ERROR': 7,
 'memusage/max': 73658368,
 'memusage/startup': 73658368,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2020, 4, 23, 17, 0, 7, 281670)}
2020-04-24 01:00:07,669 -  engine.py[line:327] - INFO: Spider closed (finished)
2020-04-24 01:05:58,782 -  log.py[line:146] - INFO: Scrapy 1.8.0 started (bot: movie)
2020-04-24 01:05:58,785 -  log.py[line:149] - INFO: Versions: lxml 4.4.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.8, Platform Linux-5.3.0-46-generic-x86_64-with-debian-buster-sid
2020-04-24 01:05:58,788 -  crawler.py[line:49] - INFO: Overridden settings: {'BOT_NAME': 'movie', 'COMMANDS_MODULE': 'movie.commands', 'LOG_LEVEL': 'WARNING', 'NEWSPIDER_MODULE': 'movie.spiders', 'SPIDER_MODULES': ['movie.spiders']}
2020-04-24 01:05:58,795 -  telnet.py[line:60] - INFO: Telnet Password: 5de6b4abc4d9f3c6
2020-04-24 01:05:58,804 -  middleware.py[line:48] - INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-04-24 01:05:59,001 -  middleware.py[line:48] - INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'movie.middlewares.UserAgentMiddleware',
 'movie.middlewares.ProxyMiddleware',
 'movie.middlewares.MovieDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-04-24 01:05:59,003 -  middleware.py[line:48] - INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'movie.middlewares.MovieSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-04-24 01:05:59,010 -  pipelines.py[line:16] - ERROR: start to use judge duplicate
2020-04-24 01:05:59,010 -  pipelines.py[line:62] - ERROR: setting: custom boxOffice setting
2020-04-24 01:05:59,023 -  pipelines.py[line:58] - ERROR: pipeline init
2020-04-24 01:05:59,023 -  middleware.py[line:48] - INFO: Enabled item pipelines:
['movie.pipelines.BoxOfficePipeline', 'movie.pipelines.MySQLPipeline']
2020-04-24 01:05:59,023 -  engine.py[line:257] - INFO: Spider opened
2020-04-24 01:05:59,025 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-24 01:05:59,025 -  signal.py[line:57] - ERROR: Error caught on signal handler: <bound method MovieDownloaderMiddleware.spider_opened of <movie.middlewares.MovieDownloaderMiddleware object at 0x7f20b4f7b310>>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/middlewares.py", line 114, in spider_opened
    spider.logger.info('Spider opened: %s' % spider.module_name)
AttributeError: 'MovieSpider' object has no attribute 'module_name'
2020-04-24 01:05:59,026 -  telnet.py[line:74] - INFO: Telnet console listening on 127.0.0.1:6023
2020-04-24 01:05:59,029 -  movie_spider.py[line:96] - ERROR: boxOffice start request for url = http://piaofang.maoyan.com/dashboard-ajax/movie?
2020-04-24 01:05:59,032 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 01:05:59,037 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 23
2020-04-24 01:05:59,039 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 01:05:59,042 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 23
2020-04-24 01:05:59,139 -  scraper.py[line:209] - ERROR: Error downloading <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190424>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/middleware.py", line 38, in process_request
    response = yield method(request=request, spider=spider)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/middlewares.py", line 201, in process_request
    logger.debug('使用代理 ' + proxy)
TypeError: can only concatenate str (not "Proxy") to str
2020-04-24 01:05:59,144 -  scraper.py[line:209] - ERROR: Error downloading <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190425>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/middleware.py", line 38, in process_request
    response = yield method(request=request, spider=spider)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/middlewares.py", line 201, in process_request
    logger.debug('使用代理 ' + proxy)
TypeError: can only concatenate str (not "Proxy") to str
2020-04-24 01:05:59,246 -  engine.py[line:296] - INFO: Closing spider (finished)
2020-04-24 01:05:59,248 -  statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/builtins.TypeError': 2,
 'elapsed_time_seconds': 0.222738,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 4, 23, 17, 5, 59, 247937),
 'log_count/ERROR': 7,
 'memusage/max': 73777152,
 'memusage/startup': 73777152,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2020, 4, 23, 17, 5, 59, 25199)}
2020-04-24 01:05:59,249 -  engine.py[line:327] - INFO: Spider closed (finished)
2020-04-24 01:06:20,957 -  log.py[line:146] - INFO: Scrapy 1.8.0 started (bot: movie)
2020-04-24 01:06:20,961 -  log.py[line:149] - INFO: Versions: lxml 4.4.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.8, Platform Linux-5.3.0-46-generic-x86_64-with-debian-buster-sid
2020-04-24 01:06:20,963 -  crawler.py[line:49] - INFO: Overridden settings: {'BOT_NAME': 'movie', 'COMMANDS_MODULE': 'movie.commands', 'LOG_LEVEL': 'WARNING', 'NEWSPIDER_MODULE': 'movie.spiders', 'SPIDER_MODULES': ['movie.spiders']}
2020-04-24 01:06:20,971 -  telnet.py[line:60] - INFO: Telnet Password: dcb724ffe7339044
2020-04-24 01:06:20,980 -  middleware.py[line:48] - INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-04-24 01:06:21,085 -  middleware.py[line:48] - INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'movie.middlewares.UserAgentMiddleware',
 'movie.middlewares.ProxyMiddleware',
 'movie.middlewares.MovieDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-04-24 01:06:21,087 -  middleware.py[line:48] - INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'movie.middlewares.MovieSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-04-24 01:06:21,093 -  pipelines.py[line:16] - ERROR: start to use judge duplicate
2020-04-24 01:06:21,094 -  pipelines.py[line:62] - ERROR: setting: custom boxOffice setting
2020-04-24 01:06:21,106 -  pipelines.py[line:58] - ERROR: pipeline init
2020-04-24 01:06:21,107 -  middleware.py[line:48] - INFO: Enabled item pipelines:
['movie.pipelines.BoxOfficePipeline', 'movie.pipelines.MySQLPipeline']
2020-04-24 01:06:21,107 -  engine.py[line:257] - INFO: Spider opened
2020-04-24 01:06:21,108 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-24 01:06:21,108 -  signal.py[line:57] - ERROR: Error caught on signal handler: <bound method MovieDownloaderMiddleware.spider_opened of <movie.middlewares.MovieDownloaderMiddleware object at 0x7f28a72661d0>>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 151, in maybeDeferred
    result = f(*args, **kw)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/middlewares.py", line 114, in spider_opened
    spider.logger.info('Spider opened: %s' % spider.module_name)
AttributeError: 'MovieSpider' object has no attribute 'module_name'
2020-04-24 01:06:21,109 -  telnet.py[line:74] - INFO: Telnet console listening on 127.0.0.1:6023
2020-04-24 01:06:21,111 -  movie_spider.py[line:96] - ERROR: boxOffice start request for url = http://piaofang.maoyan.com/dashboard-ajax/movie?
2020-04-24 01:06:21,115 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 01:06:21,119 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 20
2020-04-24 01:06:21,120 -  middlewares.py[line:201] - DEBUG: 使用代理 36.6.89.143:65309
2020-04-24 01:06:21,122 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 01:06:21,126 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 22
2020-04-24 01:06:21,126 -  middlewares.py[line:201] - DEBUG: 使用代理 27.154.34.146:31527
2020-04-24 01:06:21,370 -  scraper.py[line:209] - ERROR: Error downloading <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190424>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/python/failure.py", line 512, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/middleware.py", line 44, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/utils/defer.py", line 45, in mustbe_deferred
    result = f(*args, **kw)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/__init__.py", line 71, in download_request
    return handler.download_request(request, spider)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 75, in download_request
    return agent.download_request(request)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 345, in download_request
    d = agent.request(method, to_bytes(url, encoding='ascii'), headers, bodyproducer)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 254, in request
    endpoint=self._getEndpoint(self._proxyURI),
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/web/client.py", line 1721, in _getEndpoint
    return self._endpointFactory.endpointForURI(uri)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/web/client.py", line 1599, in endpointForURI
    raise SchemeNotSupported("Unsupported scheme: %r" % (uri.scheme,))
twisted.web.error.SchemeNotSupported: Unsupported scheme: b''
2020-04-24 01:06:21,372 -  scraper.py[line:209] - ERROR: Error downloading <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190425>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/python/failure.py", line 512, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/middleware.py", line 44, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/utils/defer.py", line 45, in mustbe_deferred
    result = f(*args, **kw)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/__init__.py", line 71, in download_request
    return handler.download_request(request, spider)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 75, in download_request
    return agent.download_request(request)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 345, in download_request
    d = agent.request(method, to_bytes(url, encoding='ascii'), headers, bodyproducer)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 254, in request
    endpoint=self._getEndpoint(self._proxyURI),
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/web/client.py", line 1721, in _getEndpoint
    return self._endpointFactory.endpointForURI(uri)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/web/client.py", line 1599, in endpointForURI
    raise SchemeNotSupported("Unsupported scheme: %r" % (uri.scheme,))
twisted.web.error.SchemeNotSupported: Unsupported scheme: b''
2020-04-24 01:06:21,473 -  engine.py[line:296] - INFO: Closing spider (finished)
2020-04-24 01:06:21,475 -  statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.web.error.SchemeNotSupported': 2,
 'downloader/request_bytes': 576,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'elapsed_time_seconds': 0.366111,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 4, 23, 17, 6, 21, 474390),
 'log_count/ERROR': 7,
 'memusage/max': 73621504,
 'memusage/startup': 73621504,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2020, 4, 23, 17, 6, 21, 108279)}
2020-04-24 01:06:21,475 -  engine.py[line:327] - INFO: Spider closed (finished)
2020-04-24 01:08:10,421 -  log.py[line:146] - INFO: Scrapy 1.8.0 started (bot: movie)
2020-04-24 01:08:10,425 -  log.py[line:149] - INFO: Versions: lxml 4.4.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.8, Platform Linux-5.3.0-46-generic-x86_64-with-debian-buster-sid
2020-04-24 01:08:10,428 -  crawler.py[line:49] - INFO: Overridden settings: {'BOT_NAME': 'movie', 'COMMANDS_MODULE': 'movie.commands', 'LOG_LEVEL': 'WARNING', 'NEWSPIDER_MODULE': 'movie.spiders', 'SPIDER_MODULES': ['movie.spiders']}
2020-04-24 01:08:10,435 -  telnet.py[line:60] - INFO: Telnet Password: 5dd0f149c44e2014
2020-04-24 01:08:10,444 -  middleware.py[line:48] - INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-04-24 01:08:10,550 -  middleware.py[line:48] - INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'movie.middlewares.UserAgentMiddleware',
 'movie.middlewares.ProxyMiddleware',
 'movie.middlewares.MovieDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-04-24 01:08:10,552 -  middleware.py[line:48] - INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'movie.middlewares.MovieSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-04-24 01:08:10,560 -  pipelines.py[line:16] - ERROR: start to use judge duplicate
2020-04-24 01:08:10,560 -  pipelines.py[line:62] - ERROR: setting: custom boxOffice setting
2020-04-24 01:08:10,573 -  pipelines.py[line:58] - ERROR: pipeline init
2020-04-24 01:08:10,573 -  middleware.py[line:48] - INFO: Enabled item pipelines:
['movie.pipelines.BoxOfficePipeline', 'movie.pipelines.MySQLPipeline']
2020-04-24 01:08:10,573 -  engine.py[line:257] - INFO: Spider opened
2020-04-24 01:08:10,574 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-24 01:08:10,575 -  telnet.py[line:74] - INFO: Telnet console listening on 127.0.0.1:6023
2020-04-24 01:08:10,577 -  movie_spider.py[line:96] - ERROR: boxOffice start request for url = http://piaofang.maoyan.com/dashboard-ajax/movie?
2020-04-24 01:08:10,582 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 01:08:10,586 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 20
2020-04-24 01:08:10,587 -  middlewares.py[line:202] - DEBUG: 使用代理 59.38.62.183:9797
2020-04-24 01:08:10,589 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 01:08:10,593 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 21
2020-04-24 01:08:10,593 -  middlewares.py[line:202] - DEBUG: 使用代理 223.241.79.75:8010
2020-04-24 01:08:10,790 -  scraper.py[line:209] - ERROR: Error downloading <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190424>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/python/failure.py", line 512, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/middleware.py", line 44, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/utils/defer.py", line 45, in mustbe_deferred
    result = f(*args, **kw)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/__init__.py", line 71, in download_request
    return handler.download_request(request, spider)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 75, in download_request
    return agent.download_request(request)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 345, in download_request
    d = agent.request(method, to_bytes(url, encoding='ascii'), headers, bodyproducer)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 254, in request
    endpoint=self._getEndpoint(self._proxyURI),
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/web/client.py", line 1721, in _getEndpoint
    return self._endpointFactory.endpointForURI(uri)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/web/client.py", line 1599, in endpointForURI
    raise SchemeNotSupported("Unsupported scheme: %r" % (uri.scheme,))
twisted.web.error.SchemeNotSupported: Unsupported scheme: b''
2020-04-24 01:08:10,815 -  scraper.py[line:209] - ERROR: Error downloading <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190425>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/python/failure.py", line 512, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/middleware.py", line 44, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/utils/defer.py", line 45, in mustbe_deferred
    result = f(*args, **kw)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/__init__.py", line 71, in download_request
    return handler.download_request(request, spider)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 75, in download_request
    return agent.download_request(request)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 345, in download_request
    d = agent.request(method, to_bytes(url, encoding='ascii'), headers, bodyproducer)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 254, in request
    endpoint=self._getEndpoint(self._proxyURI),
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/web/client.py", line 1721, in _getEndpoint
    return self._endpointFactory.endpointForURI(uri)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/web/client.py", line 1599, in endpointForURI
    raise SchemeNotSupported("Unsupported scheme: %r" % (uri.scheme,))
twisted.web.error.SchemeNotSupported: Unsupported scheme: b''
2020-04-24 01:08:10,916 -  engine.py[line:296] - INFO: Closing spider (finished)
2020-04-24 01:08:10,918 -  statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.web.error.SchemeNotSupported': 2,
 'downloader/request_bytes': 576,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'elapsed_time_seconds': 0.343155,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 4, 23, 17, 8, 10, 917649),
 'log_count/ERROR': 6,
 'memusage/max': 73457664,
 'memusage/startup': 73457664,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2020, 4, 23, 17, 8, 10, 574494)}
2020-04-24 01:08:10,918 -  engine.py[line:327] - INFO: Spider closed (finished)
2020-04-24 01:09:17,450 -  log.py[line:146] - INFO: Scrapy 1.8.0 started (bot: movie)
2020-04-24 01:09:17,454 -  log.py[line:149] - INFO: Versions: lxml 4.4.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.8, Platform Linux-5.3.0-46-generic-x86_64-with-debian-buster-sid
2020-04-24 01:09:17,458 -  crawler.py[line:49] - INFO: Overridden settings: {'BOT_NAME': 'movie', 'COMMANDS_MODULE': 'movie.commands', 'LOG_LEVEL': 'WARNING', 'NEWSPIDER_MODULE': 'movie.spiders', 'SPIDER_MODULES': ['movie.spiders']}
2020-04-24 01:09:17,465 -  telnet.py[line:60] - INFO: Telnet Password: c6a01365630113c9
2020-04-24 01:09:17,478 -  middleware.py[line:48] - INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-04-24 01:09:18,017 -  middleware.py[line:48] - INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'movie.middlewares.UserAgentMiddleware',
 'movie.middlewares.ProxyMiddleware',
 'movie.middlewares.MovieDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-04-24 01:09:18,019 -  middleware.py[line:48] - INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'movie.middlewares.MovieSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-04-24 01:09:18,026 -  pipelines.py[line:16] - ERROR: start to use judge duplicate
2020-04-24 01:09:18,026 -  pipelines.py[line:62] - ERROR: setting: custom boxOffice setting
2020-04-24 01:09:18,040 -  pipelines.py[line:58] - ERROR: pipeline init
2020-04-24 01:09:18,040 -  middleware.py[line:48] - INFO: Enabled item pipelines:
['movie.pipelines.BoxOfficePipeline', 'movie.pipelines.MySQLPipeline']
2020-04-24 01:09:18,040 -  engine.py[line:257] - INFO: Spider opened
2020-04-24 01:09:18,041 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-24 01:09:18,042 -  telnet.py[line:74] - INFO: Telnet console listening on 127.0.0.1:6023
2020-04-24 01:09:18,044 -  movie_spider.py[line:96] - ERROR: boxOffice start request for url = http://piaofang.maoyan.com/dashboard-ajax/movie?
2020-04-24 01:09:18,048 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 01:09:18,053 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 24
2020-04-24 01:09:18,054 -  middlewares.py[line:202] - DEBUG: 使用代理 http://223.241.116.201:18118
2020-04-24 01:09:18,057 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 01:09:18,060 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 21
2020-04-24 01:09:18,061 -  middlewares.py[line:202] - DEBUG: 使用代理 http://58.61.154.153:8080
2020-04-24 01:10:18,042 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-24 01:11:09,089 -  crawler.py[line:276] - INFO: Received SIGINT, shutting down gracefully. Send again to force 
2020-04-24 01:11:09,090 -  engine.py[line:296] - INFO: Closing spider (shutdown)
2020-04-24 01:11:18,042 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-24 01:11:29,101 -  retry.py[line:73] - DEBUG: Retrying <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190424> (failed 1 times): TCP connection timed out: 110: Connection timed out.
2020-04-24 01:11:51,903 -  crawler.py[line:283] - INFO: Received SIGINT twice, forcing unclean shutdown
2020-04-24 01:11:52,182 -  http11.py[line:496] - WARNING: Got data loss in http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190425. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2020-04-24 01:11:52,183 -  retry.py[line:73] - DEBUG: Retrying <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190425> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>, <twisted.python.failure.Failure twisted.web.http._DataLoss: >]
2020-04-24 01:11:52,186 -  statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 1,
 'downloader/request_bytes': 576,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'elapsed_time_seconds': 154.143823,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2020, 4, 23, 17, 11, 52, 185360),
 'log_count/ERROR': 4,
 'log_count/WARNING': 1,
 'memusage/max': 74395648,
 'memusage/startup': 73551872,
 'retry/count': 2,
 'retry/reason_count/twisted.internet.error.TCPTimedOutError': 1,
 'retry/reason_count/twisted.web._newclient.ResponseFailed': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2020, 4, 23, 17, 9, 18, 41537)}
2020-04-24 01:11:52,186 -  engine.py[line:327] - INFO: Spider closed (shutdown)
2020-04-24 01:13:37,086 -  log.py[line:146] - INFO: Scrapy 1.8.0 started (bot: movie)
2020-04-24 01:13:37,089 -  log.py[line:149] - INFO: Versions: lxml 4.4.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.8, Platform Linux-5.3.0-46-generic-x86_64-with-debian-buster-sid
2020-04-24 01:13:37,092 -  crawler.py[line:49] - INFO: Overridden settings: {'BOT_NAME': 'movie', 'COMMANDS_MODULE': 'movie.commands', 'LOG_LEVEL': 'WARNING', 'NEWSPIDER_MODULE': 'movie.spiders', 'SPIDER_MODULES': ['movie.spiders']}
2020-04-24 01:13:37,099 -  telnet.py[line:60] - INFO: Telnet Password: 40204656fd2d24b3
2020-04-24 01:13:37,109 -  middleware.py[line:48] - INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-04-24 01:13:37,211 -  middleware.py[line:48] - INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'movie.middlewares.UserAgentMiddleware',
 'movie.middlewares.ProxyMiddleware',
 'movie.middlewares.MovieDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-04-24 01:13:37,213 -  middleware.py[line:48] - INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'movie.middlewares.MovieSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-04-24 01:13:37,220 -  pipelines.py[line:16] - ERROR: start to use judge duplicate
2020-04-24 01:13:37,220 -  pipelines.py[line:62] - ERROR: setting: custom boxOffice setting
2020-04-24 01:13:37,232 -  pipelines.py[line:58] - ERROR: pipeline init
2020-04-24 01:13:37,233 -  middleware.py[line:48] - INFO: Enabled item pipelines:
['movie.pipelines.BoxOfficePipeline', 'movie.pipelines.MySQLPipeline']
2020-04-24 01:13:37,233 -  engine.py[line:257] - INFO: Spider opened
2020-04-24 01:13:37,234 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-24 01:13:37,235 -  telnet.py[line:74] - INFO: Telnet console listening on 127.0.0.1:6023
2020-04-24 01:13:37,237 -  movie_spider.py[line:96] - ERROR: boxOffice start request for url = http://piaofang.maoyan.com/dashboard-ajax/movie?
2020-04-24 01:13:37,240 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 01:13:37,244 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 20
2020-04-24 01:13:37,245 -  middlewares.py[line:202] - DEBUG: 使用代理 http://49.75.223.20:8010
2020-04-24 01:13:37,248 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 01:13:37,252 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 22
2020-04-24 01:13:37,252 -  middlewares.py[line:202] - DEBUG: 使用代理 http://36.112.128.235:3128
2020-04-24 01:13:37,577 -  retry.py[line:73] - DEBUG: Retrying <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190424> (failed 1 times): Connection was refused by other side: 111: Connection refused.
2020-04-24 01:13:37,584 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 01:13:37,594 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 24
2020-04-24 01:13:37,595 -  middlewares.py[line:202] - DEBUG: 使用代理 http://223.241.117.118:18118
2020-04-24 01:14:37,235 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-24 01:15:37,235 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-24 01:15:47,144 -  retry.py[line:73] - DEBUG: Retrying <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190425> (failed 1 times): TCP connection timed out: 110: Connection timed out.
2020-04-24 01:15:47,145 -  retry.py[line:73] - DEBUG: Retrying <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190424> (failed 2 times): TCP connection timed out: 110: Connection timed out.
2020-04-24 01:15:47,147 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 01:15:47,150 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 21
2020-04-24 01:15:47,151 -  middlewares.py[line:202] - DEBUG: 使用代理 http://61.164.39.69:53281
2020-04-24 01:15:47,154 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 01:15:47,157 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 22
2020-04-24 01:15:47,158 -  middlewares.py[line:202] - DEBUG: 使用代理 http://223.245.38.18:65309
2020-04-24 01:15:47,460 -  retry.py[line:89] - DEBUG: Gave up retrying <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190424> (failed 3 times): Connection was refused by other side: 111: Connection refused.
2020-04-24 01:15:47,561 -  scraper.py[line:209] - ERROR: Error downloading <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190424>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/middleware.py", line 44, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 111: Connection refused.
2020-04-24 01:16:37,235 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-24 01:17:37,235 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-24 01:17:58,216 -  retry.py[line:73] - DEBUG: Retrying <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190425> (failed 2 times): TCP connection timed out: 110: Connection timed out.
2020-04-24 01:17:58,222 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-24 01:17:58,235 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 20
2020-04-24 01:17:58,236 -  middlewares.py[line:202] - DEBUG: 使用代理 http://27.184.125.7:8118
2020-04-24 01:18:33,340 -  crawler.py[line:276] - INFO: Received SIGINT, shutting down gracefully. Send again to force 
2020-04-24 01:18:33,340 -  engine.py[line:296] - INFO: Closing spider (shutdown)
2020-04-24 01:18:35,096 -  crawler.py[line:283] - INFO: Received SIGINT twice, forcing unclean shutdown
2020-04-24 01:18:35,098 -  retry.py[line:89] - DEBUG: Gave up retrying <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190425> (failed 3 times): An error occurred while connecting: [Failure instance: Traceback (failure with no frames): <class 'twisted.internet.error.ConnectionLost'>: Connection to the other side was lost in a non-clean fashion: Connection lost.
].
2020-04-26 23:40:53,628 -  log.py[line:146] - INFO: Scrapy 1.8.0 started (bot: movie)
2020-04-26 23:40:53,683 -  log.py[line:149] - INFO: Versions: lxml 4.4.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.8, Platform Linux-5.3.0-46-generic-x86_64-with-debian-buster-sid
2020-04-26 23:40:53,696 -  crawler.py[line:49] - INFO: Overridden settings: {'BOT_NAME': 'movie', 'COMMANDS_MODULE': 'movie.commands', 'LOG_LEVEL': 'WARNING', 'NEWSPIDER_MODULE': 'movie.spiders', 'SPIDER_MODULES': ['movie.spiders']}
2020-04-26 23:40:53,756 -  telnet.py[line:60] - INFO: Telnet Password: 9ebc2bf2ebc812c8
2020-04-26 23:40:53,909 -  middleware.py[line:48] - INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-04-26 23:40:59,020 -  utils.py[line:80] - DEBUG: Error occurred during fetching https://www.w3schools.com/browsers/default.asp
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1319, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1252, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1298, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1247, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 966, in send
    self.connect()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1414, in connect
    super().connect()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 938, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/socket.py", line 728, in create_connection
    raise err
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/socket.py", line 716, in create_connection
    sock.connect(sa)
OSError: [Errno 101] Network is unreachable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/fake_useragent/utils.py", line 67, in get
    context=context,
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 222, in urlopen
    return opener.open(url, data, timeout)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 525, in open
    response = self._open(req, data)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 543, in _open
    '_open', req)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 503, in _call_chain
    result = func(*args)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1362, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1321, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno 101] Network is unreachable>
2020-04-26 23:40:59,133 -  utils.py[line:88] - DEBUG: Sleeping for 0.1 seconds
2020-04-26 23:41:04,246 -  utils.py[line:80] - DEBUG: Error occurred during fetching https://www.w3schools.com/browsers/default.asp
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1319, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1252, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1298, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1247, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 966, in send
    self.connect()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1414, in connect
    super().connect()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 938, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/socket.py", line 728, in create_connection
    raise err
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/socket.py", line 716, in create_connection
    sock.connect(sa)
OSError: [Errno 101] Network is unreachable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/fake_useragent/utils.py", line 67, in get
    context=context,
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 222, in urlopen
    return opener.open(url, data, timeout)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 525, in open
    response = self._open(req, data)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 543, in _open
    '_open', req)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 503, in _call_chain
    result = func(*args)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1362, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1321, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno 101] Network is unreachable>
2020-04-26 23:41:04,247 -  utils.py[line:184] - WARNING: Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1319, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1252, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1298, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1247, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 966, in send
    self.connect()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1414, in connect
    super().connect()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 938, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/socket.py", line 728, in create_connection
    raise err
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/socket.py", line 716, in create_connection
    sock.connect(sa)
OSError: [Errno 101] Network is unreachable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/fake_useragent/utils.py", line 67, in get
    context=context,
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 222, in urlopen
    return opener.open(url, data, timeout)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 525, in open
    response = self._open(req, data)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 543, in _open
    '_open', req)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 503, in _call_chain
    result = func(*args)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1362, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/urllib/request.py", line 1321, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno 101] Network is unreachable>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/fake_useragent/utils.py", line 154, in load
    for item in get_browsers(verify_ssl=verify_ssl):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/fake_useragent/utils.py", line 97, in get_browsers
    html = get(settings.BROWSERS_STATS_PAGE, verify_ssl=verify_ssl)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/fake_useragent/utils.py", line 84, in get
    raise FakeUserAgentError('Maximum amount of retries reached')
fake_useragent.errors.FakeUserAgentError: Maximum amount of retries reached
2020-04-26 23:41:14,244 -  middleware.py[line:48] - INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'movie.middlewares.UserAgentMiddleware',
 'movie.middlewares.ProxyMiddleware',
 'movie.middlewares.MovieDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-04-26 23:41:14,253 -  middleware.py[line:48] - INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'movie.middlewares.MovieSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-04-26 23:41:14,291 -  pipelines.py[line:16] - ERROR: start to use judge duplicate
2020-04-26 23:41:14,291 -  pipelines.py[line:62] - ERROR: setting: custom boxOffice setting
2020-04-26 23:41:14,406 -  pipelines.py[line:58] - ERROR: pipeline init
2020-04-26 23:41:14,406 -  middleware.py[line:48] - INFO: Enabled item pipelines:
['movie.pipelines.BoxOfficePipeline', 'movie.pipelines.MySQLPipeline']
2020-04-26 23:41:14,407 -  engine.py[line:257] - INFO: Spider opened
2020-04-26 23:41:14,412 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-26 23:41:14,414 -  telnet.py[line:74] - INFO: Telnet console listening on 127.0.0.1:6023
2020-04-26 23:41:14,421 -  movie_spider.py[line:96] - ERROR: boxOffice start request for url = http://piaofang.maoyan.com/dashboard-ajax/movie?
2020-04-26 23:41:14,427 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-26 23:41:14,428 -  retry.py[line:73] - DEBUG: Retrying <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190426> (failed 1 times): HTTPConnectionPool(host='localhost', port=21642): Max retries exceeded with url: /random (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8ec622c110>: Failed to establish a new connection: [Errno 111] Connection refused'))
2020-04-26 23:41:14,430 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-26 23:41:14,431 -  retry.py[line:73] - DEBUG: Retrying <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190426> (failed 2 times): HTTPConnectionPool(host='localhost', port=21642): Max retries exceeded with url: /random (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8ec61ba410>: Failed to establish a new connection: [Errno 111] Connection refused'))
2020-04-26 23:41:14,433 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-26 23:41:14,433 -  retry.py[line:89] - DEBUG: Gave up retrying <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190426> (failed 3 times): HTTPConnectionPool(host='localhost', port=21642): Max retries exceeded with url: /random (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8ec61c26d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2020-04-26 23:41:14,535 -  scraper.py[line:209] - ERROR: Error downloading <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190426>
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/urllib3/connection.py", line 157, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/urllib3/util/connection.py", line 84, in create_connection
    raise err
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/urllib3/util/connection.py", line 74, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/urllib3/connectionpool.py", line 672, in urlopen
    chunked=chunked,
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/urllib3/connectionpool.py", line 387, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1252, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1298, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1247, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/http/client.py", line 966, in send
    self.connect()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/urllib3/connection.py", line 184, in connect
    conn = self._new_conn()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/urllib3/connection.py", line 169, in _new_conn
    self, "Failed to establish a new connection: %s" % e
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f8ec61c26d0>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/urllib3/connectionpool.py", line 720, in urlopen
    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/urllib3/util/retry.py", line 436, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=21642): Max retries exceeded with url: /random (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8ec61c26d0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/downloader/middleware.py", line 38, in process_request
    response = yield method(request=request, spider=spider)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/middlewares.py", line 197, in process_request
    proxy = requests.get('http://localhost:21642/random').text.strip()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/requests/api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/requests/api.py", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/requests/sessions.py", line 533, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/requests/sessions.py", line 646, in send
    r = adapter.send(request, **kwargs)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/requests/adapters.py", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=21642): Max retries exceeded with url: /random (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8ec61c26d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2020-04-26 23:41:14,822 -  engine.py[line:296] - INFO: Closing spider (finished)
2020-04-26 23:41:14,824 -  statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/requests.exceptions.ConnectionError': 3,
 'elapsed_time_seconds': 0.410817,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 4, 26, 15, 41, 14, 823101),
 'log_count/ERROR': 5,
 'log_count/WARNING': 1,
 'memusage/max': 74817536,
 'memusage/startup': 74817536,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/requests.exceptions.ConnectionError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2020, 4, 26, 15, 41, 14, 412284)}
2020-04-26 23:41:14,824 -  engine.py[line:327] - INFO: Spider closed (finished)
2020-04-26 23:43:02,976 -  log.py[line:146] - INFO: Scrapy 1.8.0 started (bot: movie)
2020-04-26 23:43:02,979 -  log.py[line:149] - INFO: Versions: lxml 4.4.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.8, Platform Linux-5.3.0-46-generic-x86_64-with-debian-buster-sid
2020-04-26 23:43:02,982 -  crawler.py[line:49] - INFO: Overridden settings: {'BOT_NAME': 'movie', 'COMMANDS_MODULE': 'movie.commands', 'LOG_LEVEL': 'WARNING', 'NEWSPIDER_MODULE': 'movie.spiders', 'SPIDER_MODULES': ['movie.spiders']}
2020-04-26 23:43:02,990 -  telnet.py[line:60] - INFO: Telnet Password: 7f7aaf276141f1c4
2020-04-26 23:43:02,998 -  middleware.py[line:48] - INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-04-26 23:43:03,103 -  middleware.py[line:48] - INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'movie.middlewares.UserAgentMiddleware',
 'movie.middlewares.ProxyMiddleware',
 'movie.middlewares.MovieDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-04-26 23:43:03,105 -  middleware.py[line:48] - INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'movie.middlewares.MovieSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-04-26 23:43:03,112 -  pipelines.py[line:16] - ERROR: start to use judge duplicate
2020-04-26 23:43:03,112 -  pipelines.py[line:62] - ERROR: setting: custom boxOffice setting
2020-04-26 23:43:03,125 -  pipelines.py[line:58] - ERROR: pipeline init
2020-04-26 23:43:03,125 -  middleware.py[line:48] - INFO: Enabled item pipelines:
['movie.pipelines.BoxOfficePipeline', 'movie.pipelines.MySQLPipeline']
2020-04-26 23:43:03,125 -  engine.py[line:257] - INFO: Spider opened
2020-04-26 23:43:03,127 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-26 23:43:03,127 -  telnet.py[line:74] - INFO: Telnet console listening on 127.0.0.1:6023
2020-04-26 23:43:03,129 -  movie_spider.py[line:96] - ERROR: boxOffice start request for url = http://piaofang.maoyan.com/dashboard-ajax/movie?
2020-04-26 23:43:03,132 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-26 23:43:03,139 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 21
2020-04-26 23:43:03,140 -  middlewares.py[line:202] - DEBUG: 使用代理 http://27.191.234.69:9999
2020-04-26 23:44:03,128 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-26 23:45:03,127 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-26 23:45:13,887 -  retry.py[line:73] - DEBUG: Retrying <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190426> (failed 1 times): TCP connection timed out: 110: Connection timed out.
2020-04-26 23:45:13,891 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-26 23:45:13,895 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 21
2020-04-26 23:45:13,895 -  middlewares.py[line:202] - DEBUG: 使用代理 http://58.251.230.20:9797
2020-04-26 23:46:03,128 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-26 23:47:03,127 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-26 23:47:24,958 -  retry.py[line:73] - DEBUG: Retrying <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190426> (failed 2 times): TCP connection timed out: 110: Connection timed out.
2020-04-26 23:47:24,960 -  connectionpool.py[line:225] - DEBUG: Starting new HTTP connection (1): localhost:21642
2020-04-26 23:47:24,965 -  connectionpool.py[line:437] - DEBUG: http://localhost:21642 "GET /random HTTP/1.1" 200 22
2020-04-26 23:47:24,966 -  middlewares.py[line:202] - DEBUG: 使用代理 http://58.251.231.241:9797
2020-04-26 23:48:03,128 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-26 23:49:03,127 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-26 23:49:06,344 -  crawler.py[line:276] - INFO: Received SIGINT, shutting down gracefully. Send again to force 
2020-04-26 23:49:06,344 -  engine.py[line:296] - INFO: Closing spider (shutdown)
2020-04-26 23:49:07,715 -  crawler.py[line:283] - INFO: Received SIGINT twice, forcing unclean shutdown
2020-04-26 23:49:07,718 -  retry.py[line:89] - DEBUG: Gave up retrying <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190426> (failed 3 times): An error occurred while connecting: [Failure instance: Traceback (failure with no frames): <class 'twisted.internet.error.ConnectionLost'>: Connection to the other side was lost in a non-clean fashion: Connection lost.
].
2020-04-27 00:19:52,111 -  log.py[line:146] - INFO: Scrapy 1.8.0 started (bot: movie)
2020-04-27 00:19:52,114 -  log.py[line:149] - INFO: Versions: lxml 4.4.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.8, Platform Linux-5.3.0-46-generic-x86_64-with-debian-buster-sid
2020-04-27 00:19:52,117 -  crawler.py[line:49] - INFO: Overridden settings: {'BOT_NAME': 'movie', 'COMMANDS_MODULE': 'movie.commands', 'LOG_LEVEL': 'WARNING', 'NEWSPIDER_MODULE': 'movie.spiders', 'SPIDER_MODULES': ['movie.spiders']}
2020-04-27 00:19:52,125 -  telnet.py[line:60] - INFO: Telnet Password: b80e67a977d4cbc7
2020-04-27 00:19:52,134 -  middleware.py[line:48] - INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2020-04-27 00:19:52,244 -  middleware.py[line:48] - INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'movie.middlewares.UserAgentMiddleware',
 'movie.middlewares.MovieDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2020-04-27 00:19:52,246 -  middleware.py[line:48] - INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'movie.middlewares.MovieSpiderMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2020-04-27 00:19:52,252 -  pipelines.py[line:16] - ERROR: start to use judge duplicate
2020-04-27 00:19:52,253 -  pipelines.py[line:62] - ERROR: setting: custom boxOffice setting
2020-04-27 00:19:52,266 -  pipelines.py[line:58] - ERROR: pipeline init
2020-04-27 00:19:52,266 -  middleware.py[line:48] - INFO: Enabled item pipelines:
['movie.pipelines.BoxOfficePipeline', 'movie.pipelines.MySQLPipeline']
2020-04-27 00:19:52,266 -  engine.py[line:257] - INFO: Spider opened
2020-04-27 00:19:52,268 -  logstats.py[line:48] - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-04-27 00:19:52,268 -  telnet.py[line:74] - INFO: Telnet console listening on 127.0.0.1:6023
2020-04-27 00:19:52,271 -  movie_spider.py[line:96] - ERROR: boxOffice start request for url = http://piaofang.maoyan.com/dashboard-ajax/movie?
2020-04-27 00:19:52,459 -  engine.py[line:239] - DEBUG: Crawled (200) <GET http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427> (referer: None)
2020-04-27 00:19:53,255 -  movie_spider.py[line:115] - ERROR: now crawl url : http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427
2020-04-27 00:19:53,282 -  movie_spider.py[line:143] - WARNING: get 1 movie info, named None.
2020-04-27 00:19:53,287 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:19:53,288 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:19:53,296 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:19:53,301 -  log.py[line:110] - INFO: SHOW VARIABLES LIKE 'sql_mode'
2020-04-27 00:19:53,301 -  log.py[line:110] - INFO: {}
2020-04-27 00:19:53,679 -  log.py[line:110] - INFO: SHOW VARIABLES LIKE 'lower_case_table_names'
2020-04-27 00:19:53,680 -  log.py[line:110] - INFO: {}
2020-04-27 00:19:53,688 -  log.py[line:110] - INFO: SELECT DATABASE()
2020-04-27 00:19:53,689 -  log.py[line:110] - INFO: {}
2020-04-27 00:19:53,692 -  log.py[line:110] - INFO: show collation where `Charset` = 'utf8mb4' and `Collation` = 'utf8mb4_bin'
2020-04-27 00:19:53,692 -  log.py[line:110] - INFO: {}
2020-04-27 00:19:53,724 -  log.py[line:110] - INFO: SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1
2020-04-27 00:19:53,724 -  log.py[line:110] - INFO: {}
2020-04-27 00:19:53,725 -  log.py[line:110] - INFO: SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1
2020-04-27 00:19:53,725 -  log.py[line:110] - INFO: {}
2020-04-27 00:19:53,726 -  log.py[line:110] - INFO: SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8mb4) COLLATE utf8mb4_bin AS anon_1
2020-04-27 00:19:53,726 -  log.py[line:110] - INFO: {}
2020-04-27 00:19:53,728 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:19:53,730 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:19:53,730 -  log.py[line:110] - INFO: {'movieID': 248172, 'movieName': '复仇者联盟4：终局之战', 'seatRate': '35.0%', 'boxInfo': '55271.07', 'boxRate': '97.2%', 'releaseInfo': '上映4天', 'showInfo': 231778, 'showRate': '81.4%', 'splitBoxInfo': '52432.29', 'splitSumBoxInfo': '192200.0', 'sumBoxInfo': '202300.0', 'showView': '47', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#1'}
2020-04-27 00:19:53,818 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:19:53,949 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:19:54,550 -  movie_spider.py[line:143] - WARNING: get 2 movie info, named None.
2020-04-27 00:19:54,552 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:19:54,552 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:19:54,552 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:19:54,553 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:19:54,553 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:19:54,553 -  log.py[line:110] - INFO: {'movieID': 1211727, 'movieName': '反贪风暴4', 'seatRate': '8.8%', 'boxInfo': '366.46', 'boxRate': '0.6%', 'releaseInfo': '上映24天', 'showInfo': 14450, 'showRate': '5.0%', 'splitBoxInfo': '338.89', 'splitSumBoxInfo': '71800.0', 'sumBoxInfo': '77700.0', 'showView': '8', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#2'}
2020-04-27 00:19:54,554 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:19:54,733 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:19:54,940 -  movie_spider.py[line:143] - WARNING: get 3 movie info, named None.
2020-04-27 00:19:54,941 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:19:54,941 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:19:54,942 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:19:54,942 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:19:54,942 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:19:54,943 -  log.py[line:110] - INFO: {'movieID': 1211412, 'movieName': '神奇乐园历险记', 'seatRate': '11.4%', 'boxInfo': '313.75', 'boxRate': '0.5%', 'releaseInfo': '上映9天', 'showInfo': 10374, 'showRate': '3.6%', 'splitBoxInfo': '291.34', 'splitSumBoxInfo': '2584.8', 'sumBoxInfo': '2774.8', 'showView': '10', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#3'}
2020-04-27 00:19:54,944 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:19:55,048 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:19:55,729 -  movie_spider.py[line:143] - WARNING: get 4 movie info, named None.
2020-04-27 00:19:55,730 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:19:55,730 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:19:55,730 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:19:55,731 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:19:55,731 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:19:55,731 -  log.py[line:110] - INFO: {'movieID': 1239544, 'movieName': '调音师', 'seatRate': '13.3%', 'boxInfo': '279.09', 'boxRate': '0.4%', 'releaseInfo': '上映25天', 'showInfo': 7376, 'showRate': '2.5%', 'splitBoxInfo': '254.23', 'splitSumBoxInfo': '28100.0', 'sumBoxInfo': '31100.0', 'showView': '12', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#4'}
2020-04-27 00:19:55,732 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:19:55,858 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:19:56,521 -  movie_spider.py[line:143] - WARNING: get 5 movie info, named None.
2020-04-27 00:19:56,523 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:19:56,523 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:19:56,523 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:19:56,524 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:19:56,524 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:19:56,525 -  log.py[line:110] - INFO: {'movieID': 1230152, 'movieName': '撞死了一只羊', 'seatRate': '9.4%', 'boxInfo': '163.42', 'boxRate': '0.2%', 'releaseInfo': '上映2天', 'showInfo': 5027, 'showRate': '1.7%', 'splitBoxInfo': '156.03', 'splitSumBoxInfo': '364.5', 'sumBoxInfo': '381.1', 'showView': '9', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#5'}
2020-04-27 00:19:56,526 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:19:56,705 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:19:57,226 -  movie_spider.py[line:143] - WARNING: get 6 movie info, named None.
2020-04-27 00:19:57,228 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:19:57,228 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:19:57,228 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:19:57,228 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:19:57,229 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:19:57,229 -  log.py[line:110] - INFO: {'movieID': 1205909, 'movieName': '祈祷落幕时', 'seatRate': '13.4%', 'boxInfo': '55.76', 'boxRate': '<0.1%', 'releaseInfo': '上映16天', 'showInfo': 1439, 'showRate': '0.5%', 'splitBoxInfo': '50.80', 'splitSumBoxInfo': '5935.4', 'sumBoxInfo': '6565.3', 'showView': '12', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#6'}
2020-04-27 00:19:57,230 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:19:57,417 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:19:57,454 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E5%A4%8D%E4%BB%87%E8%80%85%E8%81%94%E7%9B%9F4%EF%BC%9A%E7%BB%88%E5%B1%80%E4%B9%8B%E6%88%98> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:19:57,907 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E5%8F%8D%E8%B4%AA%E9%A3%8E%E6%9A%B44> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:19:57,915 -  movie_spider.py[line:143] - WARNING: get 7 movie info, named None.
2020-04-27 00:19:57,916 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:19:57,916 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:19:57,917 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:19:57,917 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:19:57,917 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:19:57,918 -  log.py[line:110] - INFO: {'movieID': 1212492, 'movieName': '老师·好', 'seatRate': '12.9%', 'boxInfo': '46.97', 'boxRate': '<0.1%', 'releaseInfo': '重映37天', 'showInfo': 1460, 'showRate': '0.5%', 'splitBoxInfo': '43.94', 'splitSumBoxInfo': '32500.0', 'sumBoxInfo': '35200.0', 'showView': '10', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#7'}
2020-04-27 00:19:57,919 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:19:58,031 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:19:58,033 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E7%A5%88%E7%A5%B7%E8%90%BD%E5%B9%95%E6%97%B6> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:19:58,033 -  movie_spider.py[line:166] - INFO: movie_name = 复仇者联盟4：终局之战 and movie_year = 2019-04-27 and tpp_id = 248172
2020-04-27 00:19:58,033 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E5%A4%8D%E4%BB%87%E8%80%85%E8%81%94%E7%9B%9F4%EF%BC%9A%E7%BB%88%E5%B1%80%E4%B9%8B%E6%88%98
2020-04-27 00:19:58,034 -  movie_spider.py[line:172] - INFO: len of text is 1
2020-04-27 00:19:58,034 -  movie_spider.py[line:185] - ERROR: get movie info url = https://movie.douban.com/subject/26100958/?suggest=%E5%A4%8D%E4%BB%87%E8%80%85%E8%81%94%E7%9B%9F4%EF%BC%9A%E7%BB%88%E5%B1%80%E4%B9%8B%E6%88%98
2020-04-27 00:19:59,077 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E7%A5%9E%E5%A5%87%E4%B9%90%E5%9B%AD%E5%8E%86%E9%99%A9%E8%AE%B0> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:19:59,081 -  movie_spider.py[line:166] - INFO: movie_name = 反贪风暴4 and movie_year = 2019-04-27 and tpp_id = 1211727
2020-04-27 00:19:59,082 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E5%8F%8D%E8%B4%AA%E9%A3%8E%E6%9A%B44
2020-04-27 00:19:59,082 -  movie_spider.py[line:172] - INFO: len of text is 1
2020-04-27 00:19:59,082 -  movie_spider.py[line:185] - ERROR: get movie info url = https://movie.douban.com/subject/27202819/?suggest=%E5%8F%8D%E8%B4%AA%E9%A3%8E%E6%9A%B44
2020-04-27 00:19:59,656 -  movie_spider.py[line:143] - WARNING: get 8 movie info, named None.
2020-04-27 00:19:59,658 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:19:59,658 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:19:59,658 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:19:59,659 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:19:59,659 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:19:59,659 -  log.py[line:110] - INFO: {'movieID': 330115, 'movieName': '我和神马查干', 'seatRate': '--', 'boxInfo': '41.09', 'boxRate': '<0.1%', 'releaseInfo': '', 'showInfo': 253, 'showRate': '<0.1%', 'splitBoxInfo': '40.95', 'splitSumBoxInfo': '1933.6', 'sumBoxInfo': '1940.2', 'showView': '50', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#8'}
2020-04-27 00:19:59,661 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:19:59,778 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:19:59,823 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E8%80%81%E5%B8%88%C2%B7%E5%A5%BD> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:19:59,827 -  movie_spider.py[line:166] - INFO: movie_name = 祈祷落幕时 and movie_year = 2019-04-27 and tpp_id = 1205909
2020-04-27 00:19:59,827 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E7%A5%88%E7%A5%B7%E8%90%BD%E5%B9%95%E6%97%B6
2020-04-27 00:19:59,827 -  movie_spider.py[line:172] - INFO: len of text is 1
2020-04-27 00:19:59,827 -  movie_spider.py[line:183] - ERROR: url wrong that url = 
2020-04-27 00:20:00,606 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E8%B0%83%E9%9F%B3%E5%B8%88> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:00,613 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/subject/27202819/?suggest=%E5%8F%8D%E8%B4%AA%E9%A3%8E%E6%9A%B44> (referer: https://movie.douban.com/j/subject_suggest?q=%E5%8F%8D%E8%B4%AA%E9%A3%8E%E6%9A%B44)
2020-04-27 00:20:00,616 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/subject/26100958/?suggest=%E5%A4%8D%E4%BB%87%E8%80%85%E8%81%94%E7%9B%9F4%EF%BC%9A%E7%BB%88%E5%B1%80%E4%B9%8B%E6%88%98> (referer: https://movie.douban.com/j/subject_suggest?q=%E5%A4%8D%E4%BB%87%E8%80%85%E8%81%94%E7%9B%9F4%EF%BC%9A%E7%BB%88%E5%B1%80%E4%B9%8B%E6%88%98)
2020-04-27 00:20:00,616 -  movie_spider.py[line:166] - INFO: movie_name = 神奇乐园历险记 and movie_year = 2019-04-27 and tpp_id = 1211412
2020-04-27 00:20:00,616 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E7%A5%9E%E5%A5%87%E4%B9%90%E5%9B%AD%E5%8E%86%E9%99%A9%E8%AE%B0
2020-04-27 00:20:00,616 -  movie_spider.py[line:172] - INFO: len of text is 1
2020-04-27 00:20:00,617 -  movie_spider.py[line:185] - ERROR: get movie info url = https://movie.douban.com/subject/26662282/?suggest=%E7%A5%9E%E5%A5%87%E4%B9%90%E5%9B%AD%E5%8E%86%E9%99%A9%E8%AE%B0
2020-04-27 00:20:01,313 -  movie_spider.py[line:143] - WARNING: get 9 movie info, named None.
2020-04-27 00:20:01,315 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:01,315 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:01,316 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:01,317 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:01,317 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:20:01,318 -  log.py[line:110] - INFO: {'movieID': 1263355, 'movieName': '狗眼看人心', 'seatRate': '6.1%', 'boxInfo': '38.38', 'boxRate': '<0.1%', 'releaseInfo': '上映8天', 'showInfo': 2740, 'showRate': '0.9%', 'splitBoxInfo': '35.72', 'splitSumBoxInfo': '1646.2', 'sumBoxInfo': '1773.4', 'showView': '5', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#9'}
2020-04-27 00:20:01,319 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:01,460 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:20:01,499 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%88%91%E5%92%8C%E7%A5%9E%E9%A9%AC%E6%9F%A5%E5%B9%B2> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:01,505 -  movie_spider.py[line:166] - INFO: movie_name = 老师·好 and movie_year = 2019-04-27 and tpp_id = 1212492
2020-04-27 00:20:01,505 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E8%80%81%E5%B8%88%C2%B7%E5%A5%BD
2020-04-27 00:20:01,505 -  movie_spider.py[line:172] - INFO: len of text is 6
2020-04-27 00:20:01,505 -  movie_spider.py[line:185] - ERROR: get movie info url = https://movie.douban.com/subject/27663742/?suggest=%E8%80%81%E5%B8%88%C2%B7%E5%A5%BD
2020-04-27 00:20:01,856 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%92%9E%E6%AD%BB%E4%BA%86%E4%B8%80%E5%8F%AA%E7%BE%8A> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:01,856 -  movie_spider.py[line:166] - INFO: movie_name = 调音师 and movie_year = 2019-04-27 and tpp_id = 1239544
2020-04-27 00:20:01,856 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E8%B0%83%E9%9F%B3%E5%B8%88
2020-04-27 00:20:01,856 -  movie_spider.py[line:172] - INFO: len of text is 6
2020-04-27 00:20:01,856 -  movie_spider.py[line:183] - ERROR: url wrong that url = 
2020-04-27 00:20:01,857 -  movie_spider.py[line:190] - CRITICAL: crawled movie info of https://movie.douban.com/subject/27202819/?suggest=%E5%8F%8D%E8%B4%AA%E9%A3%8E%E6%9A%B44
2020-04-27 00:20:01,870 -  movie_spider.py[line:194] - CRITICAL: type of data is <class 'str'>
2020-04-27 00:20:01,871 -  movie_spider.py[line:202] - ERROR: len of movie info = 13
2020-04-27 00:20:01,873 -  scraper.py[line:229] - WARNING: Dropped: Duplicate item found :{'actors': '古天乐/郑嘉颖/林峯/林家栋/周秀娜/张智霖/谭耀文/张继聪/蔡瀚亿/廖启智',
 'area': ' 中国香港 / 中国大陆',
 'dbMovieID': ['27202819'],
 'directors': '林德禄',
 'doubanRate': ['6.0'],
 'duration': [96],
 'genre': '动作/犯罪',
 'movieName': ['反贪风暴4 P風暴'],
 'publishedDate': ['2019-04-04'],
 'rateCount': ['131623'],
 'tppMovieID': [1211727],
 'writers': '黄浩华/何文龙'}
{'actors': '古天乐/郑嘉颖/林峯/林家栋/周秀娜/张智霖/谭耀文/张继聪/蔡瀚亿/廖启智',
 'area': ' 中国香港 / 中国大陆',
 'dbMovieID': ['27202819'],
 'directors': '林德禄',
 'doubanRate': ['6.0'],
 'duration': [96],
 'genre': '动作/犯罪',
 'movieName': ['反贪风暴4 P風暴'],
 'publishedDate': ['2019-04-04'],
 'rateCount': ['131623'],
 'tppMovieID': [1211727],
 'writers': '黄浩华/何文龙'}
2020-04-27 00:20:01,874 -  movie_spider.py[line:190] - CRITICAL: crawled movie info of https://movie.douban.com/subject/26100958/?suggest=%E5%A4%8D%E4%BB%87%E8%80%85%E8%81%94%E7%9B%9F4%EF%BC%9A%E7%BB%88%E5%B1%80%E4%B9%8B%E6%88%98
2020-04-27 00:20:01,890 -  movie_spider.py[line:194] - CRITICAL: type of data is <class 'str'>
2020-04-27 00:20:01,891 -  movie_spider.py[line:202] - ERROR: len of movie info = 13
2020-04-27 00:20:01,894 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:01,894 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.MovieInfoItem'>
2020-04-27 00:20:01,894 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:01,895 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:01,896 -  log.py[line:110] - INFO: INSERT INTO `movieInfo` (`dbMovieID`, `tppMovieID`, `movieName`, directors, writers, actors, genre, area, duration, `publishedDate`, `rateCount`, `doubanRate`) VALUES (%(dbMovieID)s, %(tppMovieID)s, %(movieName)s, %(directors)s, %(writers)s, %(actors)s, %(genre)s, %(area)s, %(duration)s, %(publishedDate)s, %(rateCount)s, %(doubanRate)s)
2020-04-27 00:20:01,896 -  log.py[line:110] - INFO: {'dbMovieID': '26100958', 'tppMovieID': 248172, 'movieName': '复仇者联盟4：终局之战 Avengers: Endgame', 'directors': '安东尼·罗素/乔·罗素', 'writers': '克里斯托弗·马库斯/斯蒂芬·麦克菲利/斯坦·李/杰克·科比/吉姆·斯特林', 'actors': '小罗伯特·唐尼/克里斯·埃文斯/马克·鲁弗洛/克里斯·海姆斯沃斯/斯嘉丽·约翰逊/杰瑞米·雷纳/保罗·路德/凯伦·吉兰/唐·钱德尔/布丽·拉尔森', 'genre': '动作/科幻/奇幻/冒险', 'area': ' 美国', 'duration': 181, 'publishedDate': '2019-04-24', 'rateCount': '830155', 'doubanRate': '8.5'}
2020-04-27 00:20:02,001 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:02,178 -  scraper.py[line:243] - DEBUG: Scraped from <200 https://movie.douban.com/subject/26100958/?suggest=%E5%A4%8D%E4%BB%87%E8%80%85%E8%81%94%E7%9B%9F4%EF%BC%9A%E7%BB%88%E5%B1%80%E4%B9%8B%E6%88%98>
None
2020-04-27 00:20:02,187 -  movie_spider.py[line:143] - WARNING: get 10 movie info, named None.
2020-04-27 00:20:02,190 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:02,191 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:02,192 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:02,193 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:02,194 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:20:02,194 -  log.py[line:110] - INFO: {'movieID': 1229702, 'movieName': '青蛙王子历险记', 'seatRate': '10.8%', 'boxInfo': '27.82', 'boxRate': '<0.1%', 'releaseInfo': '上映23天', 'showInfo': 529, 'showRate': '0.1%', 'splitBoxInfo': '27.65', 'splitSumBoxInfo': '959.7', 'sumBoxInfo': '1015.9', 'showView': '11', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#10'}
2020-04-27 00:20:02,197 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:02,374 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:20:02,381 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E7%8B%97%E7%9C%BC%E7%9C%8B%E4%BA%BA%E5%BF%83> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:02,389 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/subject/26662282/?suggest=%E7%A5%9E%E5%A5%87%E4%B9%90%E5%9B%AD%E5%8E%86%E9%99%A9%E8%AE%B0> (referer: https://movie.douban.com/j/subject_suggest?q=%E7%A5%9E%E5%A5%87%E4%B9%90%E5%9B%AD%E5%8E%86%E9%99%A9%E8%AE%B0)
2020-04-27 00:20:02,402 -  movie_spider.py[line:166] - INFO: movie_name = 我和神马查干 and movie_year = 2019-04-27 and tpp_id = 330115
2020-04-27 00:20:02,406 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E6%88%91%E5%92%8C%E7%A5%9E%E9%A9%AC%E6%9F%A5%E5%B9%B2
2020-04-27 00:20:02,406 -  movie_spider.py[line:172] - INFO: len of text is 1
2020-04-27 00:20:02,406 -  movie_spider.py[line:183] - ERROR: url wrong that url = 
2020-04-27 00:20:03,046 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/subject/27663742/?suggest=%E8%80%81%E5%B8%88%C2%B7%E5%A5%BD> (referer: https://movie.douban.com/j/subject_suggest?q=%E8%80%81%E5%B8%88%C2%B7%E5%A5%BD)
2020-04-27 00:20:03,046 -  movie_spider.py[line:166] - INFO: movie_name = 撞死了一只羊 and movie_year = 2019-04-27 and tpp_id = 1230152
2020-04-27 00:20:03,046 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E6%92%9E%E6%AD%BB%E4%BA%86%E4%B8%80%E5%8F%AA%E7%BE%8A
2020-04-27 00:20:03,046 -  movie_spider.py[line:172] - INFO: len of text is 1
2020-04-27 00:20:03,046 -  movie_spider.py[line:183] - ERROR: url wrong that url = 
2020-04-27 00:20:03,049 -  movie_spider.py[line:143] - WARNING: get 11 movie info, named None.
2020-04-27 00:20:03,050 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:03,050 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:03,050 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:03,051 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:03,051 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:20:03,051 -  log.py[line:110] - INFO: {'movieID': 346765, 'movieName': '小飞象', 'seatRate': '10.2%', 'boxInfo': '24.86', 'boxRate': '<0.1%', 'releaseInfo': '上映30天', 'showInfo': 853, 'showRate': '0.2%', 'splitBoxInfo': '23.14', 'splitSumBoxInfo': '13500.0', 'sumBoxInfo': '14700.0', 'showView': '9', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#11'}
2020-04-27 00:20:03,052 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:03,229 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:20:03,230 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E9%9D%92%E8%9B%99%E7%8E%8B%E5%AD%90%E5%8E%86%E9%99%A9%E8%AE%B0> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:03,231 -  movie_spider.py[line:166] - INFO: movie_name = 狗眼看人心 and movie_year = 2019-04-27 and tpp_id = 1263355
2020-04-27 00:20:03,231 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E7%8B%97%E7%9C%BC%E7%9C%8B%E4%BA%BA%E5%BF%83
2020-04-27 00:20:03,231 -  movie_spider.py[line:172] - INFO: len of text is 1
2020-04-27 00:20:03,231 -  movie_spider.py[line:185] - ERROR: get movie info url = https://movie.douban.com/subject/30479973/?suggest=%E7%8B%97%E7%9C%BC%E7%9C%8B%E4%BA%BA%E5%BF%83
2020-04-27 00:20:03,644 -  movie_spider.py[line:190] - CRITICAL: crawled movie info of https://movie.douban.com/subject/26662282/?suggest=%E7%A5%9E%E5%A5%87%E4%B9%90%E5%9B%AD%E5%8E%86%E9%99%A9%E8%AE%B0
2020-04-27 00:20:03,664 -  movie_spider.py[line:194] - CRITICAL: type of data is <class 'str'>
2020-04-27 00:20:03,664 -  movie_spider.py[line:202] - ERROR: len of movie info = 13
2020-04-27 00:20:03,667 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:03,667 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.MovieInfoItem'>
2020-04-27 00:20:03,667 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:03,668 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:03,668 -  log.py[line:110] - INFO: INSERT INTO `movieInfo` (`dbMovieID`, `tppMovieID`, `movieName`, directors, writers, actors, genre, area, duration, `publishedDate`, `rateCount`, `doubanRate`) VALUES (%(dbMovieID)s, %(tppMovieID)s, %(movieName)s, %(directors)s, %(writers)s, %(actors)s, %(genre)s, %(area)s, %(duration)s, %(publishedDate)s, %(rateCount)s, %(doubanRate)s)
2020-04-27 00:20:03,668 -  log.py[line:110] - INFO: {'dbMovieID': '26662282', 'tppMovieID': 1211412, 'movieName': '神奇乐园历险记 Wonder Park', 'directors': '迪兰·布朗', 'writers': '乔什·阿佩尔鲍姆/安德烈·内梅克/罗伯特·戈登', 'actors': '索菲亚·玛丽/詹妮弗·加纳/肯·哈德森·坎贝尔/基南·汤普森/米拉·库尼斯/约翰·奥利弗/郑肯/诺贝特·里奥·布茨/马修·布罗德里克/凯特·麦克格雷格-斯图尔特', 'genre': '喜剧/动画/奇幻/冒险', 'area': ' 美国 / 西班牙', 'duration': 86, 'publishedDate': '2019-03-15', 'rateCount': '2944', 'doubanRate': '6.3'}
2020-04-27 00:20:03,669 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:03,855 -  scraper.py[line:243] - DEBUG: Scraped from <200 https://movie.douban.com/subject/26662282/?suggest=%E7%A5%9E%E5%A5%87%E4%B9%90%E5%9B%AD%E5%8E%86%E9%99%A9%E8%AE%B0>
None
2020-04-27 00:20:04,703 -  movie_spider.py[line:190] - CRITICAL: crawled movie info of https://movie.douban.com/subject/27663742/?suggest=%E8%80%81%E5%B8%88%C2%B7%E5%A5%BD
2020-04-27 00:20:04,718 -  movie_spider.py[line:194] - CRITICAL: type of data is <class 'str'>
2020-04-27 00:20:04,718 -  movie_spider.py[line:202] - ERROR: len of movie info = 13
2020-04-27 00:20:04,720 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:04,720 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.MovieInfoItem'>
2020-04-27 00:20:04,720 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:04,721 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:04,721 -  log.py[line:110] - INFO: INSERT INTO `movieInfo` (`dbMovieID`, `tppMovieID`, `movieName`, directors, writers, actors, genre, area, duration, `publishedDate`, `rateCount`, `doubanRate`) VALUES (%(dbMovieID)s, %(tppMovieID)s, %(movieName)s, %(directors)s, %(writers)s, %(actors)s, %(genre)s, %(area)s, %(duration)s, %(publishedDate)s, %(rateCount)s, %(doubanRate)s)
2020-04-27 00:20:04,721 -  log.py[line:110] - INFO: {'dbMovieID': '27663742', 'tppMovieID': 1212492, 'movieName': '老师·好', 'directors': '张栾', 'writers': '张栾/徐伟', 'actors': '于谦/汤梦佳/王广源/秦鸣悦/徐子力/孙艺杨/徐紫茵/郝鹏飞/郜玄铭/黑妹', 'genre': '剧情', 'area': ' 中国大陆', 'duration': 111, 'publishedDate': '2019-03-22', 'rateCount': '215985', 'doubanRate': '6.7'}
2020-04-27 00:20:04,722 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:04,864 -  scraper.py[line:243] - DEBUG: Scraped from <200 https://movie.douban.com/subject/27663742/?suggest=%E8%80%81%E5%B8%88%C2%B7%E5%A5%BD>
None
2020-04-27 00:20:04,867 -  movie_spider.py[line:143] - WARNING: get 12 movie info, named None.
2020-04-27 00:20:04,869 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:04,870 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:04,870 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:04,871 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:04,873 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:20:04,873 -  log.py[line:110] - INFO: {'movieID': 1236912, 'movieName': '远去的牧歌', 'seatRate': '39.6%', 'boxInfo': '20.60', 'boxRate': '<0.1%', 'releaseInfo': '', 'showInfo': 65, 'showRate': '<0.1%', 'splitBoxInfo': '20.60', 'splitSumBoxInfo': '882.3', 'sumBoxInfo': '886.9', 'showView': '68', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#12'}
2020-04-27 00:20:04,877 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:05,011 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:20:05,014 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E5%B0%8F%E9%A3%9E%E8%B1%A1> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:05,015 -  movie_spider.py[line:166] - INFO: movie_name = 青蛙王子历险记 and movie_year = 2019-04-27 and tpp_id = 1229702
2020-04-27 00:20:05,015 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E9%9D%92%E8%9B%99%E7%8E%8B%E5%AD%90%E5%8E%86%E9%99%A9%E8%AE%B0
2020-04-27 00:20:05,015 -  movie_spider.py[line:172] - INFO: len of text is 1
2020-04-27 00:20:05,015 -  movie_spider.py[line:185] - ERROR: get movie info url = https://movie.douban.com/subject/30288751/?suggest=%E9%9D%92%E8%9B%99%E7%8E%8B%E5%AD%90%E5%8E%86%E9%99%A9%E8%AE%B0
2020-04-27 00:20:06,821 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/subject/30479973/?suggest=%E7%8B%97%E7%9C%BC%E7%9C%8B%E4%BA%BA%E5%BF%83> (referer: https://movie.douban.com/j/subject_suggest?q=%E7%8B%97%E7%9C%BC%E7%9C%8B%E4%BA%BA%E5%BF%83)
2020-04-27 00:20:06,838 -  movie_spider.py[line:143] - WARNING: get 13 movie info, named None.
2020-04-27 00:20:06,839 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:06,839 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:06,840 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:06,840 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:06,840 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:20:06,840 -  log.py[line:110] - INFO: {'movieID': 643506, 'movieName': '古镇画情', 'seatRate': '--', 'boxInfo': '20.09', 'boxRate': '<0.1%', 'releaseInfo': '展映', 'showInfo': 77, 'showRate': '<0.1%', 'splitBoxInfo': '20.05', 'splitSumBoxInfo': '1069.9', 'sumBoxInfo': '1072.7', 'showView': '60', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#13'}
2020-04-27 00:20:06,841 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:07,053 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:20:07,053 -  movie_spider.py[line:166] - INFO: movie_name = 小飞象 and movie_year = 2019-04-27 and tpp_id = 346765
2020-04-27 00:20:07,053 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E5%B0%8F%E9%A3%9E%E8%B1%A1
2020-04-27 00:20:07,054 -  movie_spider.py[line:172] - INFO: len of text is 6
2020-04-27 00:20:07,054 -  movie_spider.py[line:185] - ERROR: get movie info url = https://movie.douban.com/subject/25924056/?suggest=%E5%B0%8F%E9%A3%9E%E8%B1%A1
2020-04-27 00:20:08,203 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E8%BF%9C%E5%8E%BB%E7%9A%84%E7%89%A7%E6%AD%8C> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:08,209 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/subject/30288751/?suggest=%E9%9D%92%E8%9B%99%E7%8E%8B%E5%AD%90%E5%8E%86%E9%99%A9%E8%AE%B0> (referer: https://movie.douban.com/j/subject_suggest?q=%E9%9D%92%E8%9B%99%E7%8E%8B%E5%AD%90%E5%8E%86%E9%99%A9%E8%AE%B0)
2020-04-27 00:20:08,232 -  movie_spider.py[line:190] - CRITICAL: crawled movie info of https://movie.douban.com/subject/30479973/?suggest=%E7%8B%97%E7%9C%BC%E7%9C%8B%E4%BA%BA%E5%BF%83
2020-04-27 00:20:08,244 -  movie_spider.py[line:194] - CRITICAL: type of data is <class 'str'>
2020-04-27 00:20:08,244 -  movie_spider.py[line:202] - ERROR: len of movie info = 13
2020-04-27 00:20:08,246 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:08,246 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.MovieInfoItem'>
2020-04-27 00:20:08,246 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:08,247 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:08,247 -  log.py[line:110] - INFO: INSERT INTO `movieInfo` (`dbMovieID`, `tppMovieID`, `movieName`, directors, writers, actors, genre, area, duration, `publishedDate`, `rateCount`, `doubanRate`) VALUES (%(dbMovieID)s, %(tppMovieID)s, %(movieName)s, %(directors)s, %(writers)s, %(actors)s, %(genre)s, %(area)s, %(duration)s, %(publishedDate)s, %(rateCount)s, %(doubanRate)s)
2020-04-27 00:20:08,247 -  log.py[line:110] - INFO: {'dbMovieID': '30479973', 'tppMovieID': 1263355, 'movieName': '狗眼看人心', 'directors': '吴楠', 'writers': '吴楠', 'actors': '黄磊/闫妮/韩童生/崔新琴/沙俊伯/丁嘉丽/冯嘉怡/沙溢/果靖霖/胡可', 'genre': '剧情/家庭', 'area': ' 中国大陆', 'duration': 91, 'publishedDate': '2019-04-20', 'rateCount': '6287', 'doubanRate': '5.9'}
2020-04-27 00:20:08,248 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:08,374 -  scraper.py[line:243] - DEBUG: Scraped from <200 https://movie.douban.com/subject/30479973/?suggest=%E7%8B%97%E7%9C%BC%E7%9C%8B%E4%BA%BA%E5%BF%83>
None
2020-04-27 00:20:08,376 -  movie_spider.py[line:143] - WARNING: get 14 movie info, named None.
2020-04-27 00:20:08,379 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:08,379 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:08,380 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:08,381 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:08,381 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:20:08,382 -  log.py[line:110] - INFO: {'movieID': 836, 'movieName': '毕业那年', 'seatRate': '--', 'boxInfo': '14.93', 'boxRate': '<0.1%', 'releaseInfo': '', 'showInfo': 55, 'showRate': '<0.1%', 'splitBoxInfo': '14.87', 'splitSumBoxInfo': '989.2', 'sumBoxInfo': '990.7', 'showView': '71', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#14'}
2020-04-27 00:20:08,384 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:08,521 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:20:09,445 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E5%8F%A4%E9%95%87%E7%94%BB%E6%83%85> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:09,446 -  movie_spider.py[line:166] - INFO: movie_name = 远去的牧歌 and movie_year = 2019-04-27 and tpp_id = 1236912
2020-04-27 00:20:09,447 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E8%BF%9C%E5%8E%BB%E7%9A%84%E7%89%A7%E6%AD%8C
2020-04-27 00:20:09,447 -  movie_spider.py[line:172] - INFO: len of text is 1
2020-04-27 00:20:09,447 -  movie_spider.py[line:183] - ERROR: url wrong that url = 
2020-04-27 00:20:09,451 -  movie_spider.py[line:190] - CRITICAL: crawled movie info of https://movie.douban.com/subject/30288751/?suggest=%E9%9D%92%E8%9B%99%E7%8E%8B%E5%AD%90%E5%8E%86%E9%99%A9%E8%AE%B0
2020-04-27 00:20:09,486 -  movie_spider.py[line:194] - CRITICAL: type of data is <class 'str'>
2020-04-27 00:20:09,487 -  movie_spider.py[line:198] - ERROR: json decode error in url = https://movie.douban.com/subject/30288751/?suggest=%E9%9D%92%E8%9B%99%E7%8E%8B%E5%AD%90%E5%8E%86%E9%99%A9%E8%AE%B0
2020-04-27 00:20:09,487 -  scraper.py[line:159] - ERROR: Spider error processing <GET https://movie.douban.com/subject/30288751/?suggest=%E9%9D%92%E8%9B%99%E7%8E%8B%E5%AD%90%E5%8E%86%E9%99%A9%E8%AE%B0> (referer: https://movie.douban.com/j/subject_suggest?q=%E9%9D%92%E8%9B%99%E7%8E%8B%E5%AD%90%E5%8E%86%E9%99%A9%E8%AE%B0)
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/middlewares.py", line 45, in process_spider_output
    for i in result:
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/spiders/movie_spider.py", line 200, in parse_movie_info
    text = json.loads(data)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/json/__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Invalid control character at: line 32 column 73 (char 646)
2020-04-27 00:20:09,730 -  movie_spider.py[line:143] - WARNING: get 15 movie info, named None.
2020-04-27 00:20:09,731 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:09,731 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:09,731 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:09,732 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:09,732 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:20:09,732 -  log.py[line:110] - INFO: {'movieID': 1207260, 'movieName': '如影随心', 'seatRate': '3.1%', 'boxInfo': '12.61', 'boxRate': '<0.1%', 'releaseInfo': '上映9天', 'showInfo': 1851, 'showRate': '0.6%', 'splitBoxInfo': '11.92', 'splitSumBoxInfo': '2191.0', 'sumBoxInfo': '2343.4', 'showView': '2', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#15'}
2020-04-27 00:20:09,733 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:09,842 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:20:10,423 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%AF%95%E4%B8%9A%E9%82%A3%E5%B9%B4> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:10,425 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/subject/25924056/?suggest=%E5%B0%8F%E9%A3%9E%E8%B1%A1> (referer: https://movie.douban.com/j/subject_suggest?q=%E5%B0%8F%E9%A3%9E%E8%B1%A1)
2020-04-27 00:20:10,425 -  movie_spider.py[line:166] - INFO: movie_name = 古镇画情 and movie_year = 2019-04-27 and tpp_id = 643506
2020-04-27 00:20:10,425 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E5%8F%A4%E9%95%87%E7%94%BB%E6%83%85
2020-04-27 00:20:10,425 -  movie_spider.py[line:172] - INFO: len of text is 1
2020-04-27 00:20:10,425 -  movie_spider.py[line:183] - ERROR: url wrong that url = 
2020-04-27 00:20:10,434 -  movie_spider.py[line:143] - WARNING: get 16 movie info, named None.
2020-04-27 00:20:10,435 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:10,435 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:10,435 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:10,435 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:10,436 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:20:10,436 -  log.py[line:110] - INFO: {'movieID': 476263, 'movieName': '天上再见', 'seatRate': '79.8%', 'boxInfo': '12.10', 'boxRate': '<0.1%', 'releaseInfo': '点映', 'showInfo': 45, 'showRate': '<0.1%', 'splitBoxInfo': '11.98', 'splitSumBoxInfo': '12.3', 'sumBoxInfo': '12.5', 'showView': '71', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#16'}
2020-04-27 00:20:10,436 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:10,632 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:20:11,525 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E5%A6%82%E5%BD%B1%E9%9A%8F%E5%BF%83> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:11,526 -  movie_spider.py[line:166] - INFO: movie_name = 毕业那年 and movie_year = 2019-04-27 and tpp_id = 836
2020-04-27 00:20:11,526 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E6%AF%95%E4%B8%9A%E9%82%A3%E5%B9%B4
2020-04-27 00:20:11,526 -  movie_spider.py[line:172] - INFO: len of text is 2
2020-04-27 00:20:11,526 -  movie_spider.py[line:183] - ERROR: url wrong that url = 
2020-04-27 00:20:11,528 -  movie_spider.py[line:190] - CRITICAL: crawled movie info of https://movie.douban.com/subject/25924056/?suggest=%E5%B0%8F%E9%A3%9E%E8%B1%A1
2020-04-27 00:20:11,549 -  movie_spider.py[line:194] - CRITICAL: type of data is <class 'str'>
2020-04-27 00:20:11,550 -  movie_spider.py[line:198] - ERROR: json decode error in url = https://movie.douban.com/subject/25924056/?suggest=%E5%B0%8F%E9%A3%9E%E8%B1%A1
2020-04-27 00:20:11,550 -  scraper.py[line:159] - ERROR: Spider error processing <GET https://movie.douban.com/subject/25924056/?suggest=%E5%B0%8F%E9%A3%9E%E8%B1%A1> (referer: https://movie.douban.com/j/subject_suggest?q=%E5%B0%8F%E9%A3%9E%E8%B1%A1)
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/middlewares.py", line 45, in process_spider_output
    for i in result:
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/spiders/movie_spider.py", line 200, in parse_movie_info
    text = json.loads(data)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/json/__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/json/decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Invalid control character at: line 132 column 39 (char 2524)
2020-04-27 00:20:11,560 -  movie_spider.py[line:143] - WARNING: get 17 movie info, named None.
2020-04-27 00:20:11,561 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:11,561 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:11,562 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:11,562 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:11,562 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:20:11,562 -  log.py[line:110] - INFO: {'movieID': 1239171, 'movieName': '长官传奇', 'seatRate': '50.4%', 'boxInfo': '8.26', 'boxRate': '<0.1%', 'releaseInfo': '上映23天', 'showInfo': 32, 'showRate': '<0.1%', 'splitBoxInfo': '8.26', 'splitSumBoxInfo': '34.2', 'sumBoxInfo': '34.3', 'showView': '56', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#17'}
2020-04-27 00:20:11,563 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:11,755 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:20:12,195 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E5%A4%A9%E4%B8%8A%E5%86%8D%E8%A7%81> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:12,216 -  movie_spider.py[line:166] - INFO: movie_name = 如影随心 and movie_year = 2019-04-27 and tpp_id = 1207260
2020-04-27 00:20:12,216 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E5%A6%82%E5%BD%B1%E9%9A%8F%E5%BF%83
2020-04-27 00:20:12,216 -  movie_spider.py[line:172] - INFO: len of text is 1
2020-04-27 00:20:12,216 -  movie_spider.py[line:185] - ERROR: get movie info url = https://movie.douban.com/subject/26871669/?suggest=%E5%A6%82%E5%BD%B1%E9%9A%8F%E5%BF%83
2020-04-27 00:20:12,415 -  movie_spider.py[line:143] - WARNING: get 18 movie info, named None.
2020-04-27 00:20:12,417 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:12,417 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:12,418 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:12,418 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:12,419 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:20:12,419 -  log.py[line:110] - INFO: {'movieID': 1206415, 'movieName': '虫林大作战', 'seatRate': '4.8%', 'boxInfo': '7.69', 'boxRate': '<0.1%', 'releaseInfo': '上映15天', 'showInfo': 670, 'showRate': '0.2%', 'splitBoxInfo': '7.32', 'splitSumBoxInfo': '693.6', 'sumBoxInfo': '752.9', 'showView': '5', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#18'}
2020-04-27 00:20:12,421 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:12,536 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:20:13,394 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E9%95%BF%E5%AE%98%E4%BC%A0%E5%A5%87> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:13,396 -  movie_spider.py[line:166] - INFO: movie_name = 天上再见 and movie_year = 2019-04-27 and tpp_id = 476263
2020-04-27 00:20:13,396 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E5%A4%A9%E4%B8%8A%E5%86%8D%E8%A7%81
2020-04-27 00:20:13,396 -  movie_spider.py[line:172] - INFO: len of text is 1
2020-04-27 00:20:13,396 -  movie_spider.py[line:183] - ERROR: url wrong that url = 
2020-04-27 00:20:13,423 -  movie_spider.py[line:143] - WARNING: get 19 movie info, named None.
2020-04-27 00:20:13,424 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:13,425 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:13,425 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:13,425 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:13,426 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:20:13,426 -  log.py[line:110] - INFO: {'movieID': 1216383, 'movieName': '比悲伤更悲伤的故事', 'seatRate': '7.1%', 'boxInfo': '7.45', 'boxRate': '<0.1%', 'releaseInfo': '上映45天', 'showInfo': 622, 'showRate': '0.2%', 'splitBoxInfo': '6.83', 'splitSumBoxInfo': '85800.0', 'sumBoxInfo': '95700.0', 'showView': '5', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#19'}
2020-04-27 00:20:13,427 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:13,656 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:20:14,519 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E8%99%AB%E6%9E%97%E5%A4%A7%E4%BD%9C%E6%88%98> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:14,521 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/subject/26871669/?suggest=%E5%A6%82%E5%BD%B1%E9%9A%8F%E5%BF%83> (referer: https://movie.douban.com/j/subject_suggest?q=%E5%A6%82%E5%BD%B1%E9%9A%8F%E5%BF%83)
2020-04-27 00:20:14,521 -  movie_spider.py[line:166] - INFO: movie_name = 长官传奇 and movie_year = 2019-04-27 and tpp_id = 1239171
2020-04-27 00:20:14,522 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E9%95%BF%E5%AE%98%E4%BC%A0%E5%A5%87
2020-04-27 00:20:14,522 -  movie_spider.py[line:172] - INFO: len of text is 1
2020-04-27 00:20:14,522 -  movie_spider.py[line:185] - ERROR: get movie info url = https://movie.douban.com/subject/30486462/?suggest=%E9%95%BF%E5%AE%98%E4%BC%A0%E5%A5%87
2020-04-27 00:20:15,543 -  movie_spider.py[line:143] - WARNING: get 20 movie info, named None.
2020-04-27 00:20:15,544 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:15,544 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:15,544 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:15,544 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:15,545 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:20:15,545 -  log.py[line:110] - INFO: {'movieID': 1167831, 'movieName': '波西米亚狂想曲', 'seatRate': '17.9%', 'boxInfo': '6.52', 'boxRate': '<0.1%', 'releaseInfo': '上映37天', 'showInfo': 123, 'showRate': '<0.1%', 'splitBoxInfo': '5.99', 'splitSumBoxInfo': '8922.9', 'sumBoxInfo': '9846.2', 'showView': '14', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#20'}
2020-04-27 00:20:15,546 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:15,644 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:20:16,189 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%AF%94%E6%82%B2%E4%BC%A4%E6%9B%B4%E6%82%B2%E4%BC%A4%E7%9A%84%E6%95%85%E4%BA%8B> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:16,190 -  movie_spider.py[line:166] - INFO: movie_name = 虫林大作战 and movie_year = 2019-04-27 and tpp_id = 1206415
2020-04-27 00:20:16,190 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E8%99%AB%E6%9E%97%E5%A4%A7%E4%BD%9C%E6%88%98
2020-04-27 00:20:16,191 -  movie_spider.py[line:172] - INFO: len of text is 1
2020-04-27 00:20:16,191 -  movie_spider.py[line:183] - ERROR: url wrong that url = 
2020-04-27 00:20:16,195 -  movie_spider.py[line:190] - CRITICAL: crawled movie info of https://movie.douban.com/subject/26871669/?suggest=%E5%A6%82%E5%BD%B1%E9%9A%8F%E5%BF%83
2020-04-27 00:20:16,222 -  movie_spider.py[line:194] - CRITICAL: type of data is <class 'str'>
2020-04-27 00:20:16,222 -  movie_spider.py[line:202] - ERROR: len of movie info = 13
2020-04-27 00:20:16,224 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:16,224 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.MovieInfoItem'>
2020-04-27 00:20:16,225 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:16,225 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:16,225 -  log.py[line:110] - INFO: INSERT INTO `movieInfo` (`dbMovieID`, `tppMovieID`, `movieName`, directors, writers, actors, genre, area, duration, `publishedDate`, `rateCount`, `doubanRate`) VALUES (%(dbMovieID)s, %(tppMovieID)s, %(movieName)s, %(directors)s, %(writers)s, %(actors)s, %(genre)s, %(area)s, %(duration)s, %(publishedDate)s, %(rateCount)s, %(doubanRate)s)
2020-04-27 00:20:16,225 -  log.py[line:110] - INFO: {'dbMovieID': '26871669', 'tppMovieID': 1207260, 'movieName': '如影随心', 'directors': '霍建起', 'writers': '霍建起', 'actors': '陈晓/杜鹃/王嘉/马苏/华少/谢依霖/关晓彤/高晓攀/赵震/庞奕欣', 'genre': '剧情/爱情', 'area': ' 中国大陆', 'duration': 96, 'publishedDate': '2019-04-19', 'rateCount': '19371', 'doubanRate': '4.9'}
2020-04-27 00:20:16,226 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:16,340 -  scraper.py[line:243] - DEBUG: Scraped from <200 https://movie.douban.com/subject/26871669/?suggest=%E5%A6%82%E5%BD%B1%E9%9A%8F%E5%BF%83>
None
2020-04-27 00:20:16,359 -  movie_spider.py[line:143] - WARNING: get 21 movie info, named None.
2020-04-27 00:20:16,361 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:16,361 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:16,362 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:16,363 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:16,363 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:20:16,363 -  log.py[line:110] - INFO: {'movieID': 343987, 'movieName': '精灵怪物：疯狂之旅', 'seatRate': '26.9%', 'boxInfo': '5.55', 'boxRate': '<0.1%', 'releaseInfo': '上映30天', 'showInfo': 141, 'showRate': '<0.1%', 'splitBoxInfo': '5.52', 'splitSumBoxInfo': '584.9', 'sumBoxInfo': '625.8', 'showView': '16', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#21'}
2020-04-27 00:20:16,365 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:16,489 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:20:16,498 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/subject/30486462/?suggest=%E9%95%BF%E5%AE%98%E4%BC%A0%E5%A5%87> (referer: https://movie.douban.com/j/subject_suggest?q=%E9%95%BF%E5%AE%98%E4%BC%A0%E5%A5%87)
2020-04-27 00:20:16,962 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%B3%A2%E8%A5%BF%E7%B1%B3%E4%BA%9A%E7%8B%82%E6%83%B3%E6%9B%B2> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:16,963 -  movie_spider.py[line:166] - INFO: movie_name = 比悲伤更悲伤的故事 and movie_year = 2019-04-27 and tpp_id = 1216383
2020-04-27 00:20:16,963 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E6%AF%94%E6%82%B2%E4%BC%A4%E6%9B%B4%E6%82%B2%E4%BC%A4%E7%9A%84%E6%95%85%E4%BA%8B
2020-04-27 00:20:16,963 -  movie_spider.py[line:172] - INFO: len of text is 2
2020-04-27 00:20:16,963 -  movie_spider.py[line:183] - ERROR: url wrong that url = 
2020-04-27 00:20:16,966 -  movie_spider.py[line:143] - WARNING: get 22 movie info, named None.
2020-04-27 00:20:16,967 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:16,967 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:16,968 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:16,968 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:16,969 -  log.py[line:110] - INFO: INSERT INTO `boxOffice` (`movieID`, `movieName`, `seatRate`, `boxInfo`, `boxRate`, `releaseInfo`, `showInfo`, `showRate`, `splitBoxInfo`, `splitSumBoxInfo`, `sumBoxInfo`, `showView`, `crawlDate`, `yearRate`) VALUES (%(movieID)s, %(movieName)s, %(seatRate)s, %(boxInfo)s, %(boxRate)s, %(releaseInfo)s, %(showInfo)s, %(showRate)s, %(splitBoxInfo)s, %(splitSumBoxInfo)s, %(sumBoxInfo)s, %(showView)s, %(crawlDate)s, %(yearRate)s)
2020-04-27 00:20:16,969 -  log.py[line:110] - INFO: {'movieID': 1215746, 'movieName': '海门深处', 'seatRate': '27.9%', 'boxInfo': '5.07', 'boxRate': '<0.1%', 'releaseInfo': '上映首日', 'showInfo': 85, 'showRate': '<0.1%', 'splitBoxInfo': '4.98', 'splitSumBoxInfo': '4.9', 'sumBoxInfo': '5.0', 'showView': '22', 'crawlDate': '2019-04-27', 'yearRate': '2019-04-27#22'}
2020-04-27 00:20:16,970 -  log.py[line:110] - INFO: COMMIT
2020-04-27 00:20:17,124 -  scraper.py[line:243] - DEBUG: Scraped from <200 http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427>
None
2020-04-27 00:20:17,126 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E7%B2%BE%E7%81%B5%E6%80%AA%E7%89%A9%EF%BC%9A%E7%96%AF%E7%8B%82%E4%B9%8B%E6%97%85> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:17,126 -  movie_spider.py[line:190] - CRITICAL: crawled movie info of https://movie.douban.com/subject/30486462/?suggest=%E9%95%BF%E5%AE%98%E4%BC%A0%E5%A5%87
2020-04-27 00:20:17,139 -  movie_spider.py[line:194] - CRITICAL: type of data is <class 'str'>
2020-04-27 00:20:17,139 -  movie_spider.py[line:202] - ERROR: len of movie info = 13
2020-04-27 00:20:17,141 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:17,141 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.MovieInfoItem'>
2020-04-27 00:20:17,141 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:17,141 -  log.py[line:110] - INFO: BEGIN (implicit)
2020-04-27 00:20:17,142 -  log.py[line:110] - INFO: INSERT INTO `movieInfo` (`dbMovieID`, `tppMovieID`, `movieName`, directors, writers, actors, genre, area, duration, `publishedDate`, `rateCount`, `doubanRate`) VALUES (%(dbMovieID)s, %(tppMovieID)s, %(movieName)s, %(directors)s, %(writers)s, %(actors)s, %(genre)s, %(area)s, %(duration)s, %(publishedDate)s, %(rateCount)s, %(doubanRate)s)
2020-04-27 00:20:17,142 -  log.py[line:110] - INFO: {'dbMovieID': '30486462', 'tppMovieID': 1239171, 'movieName': '长官传奇', 'directors': '杨议', 'writers': '李延亮/崔伟/廖庆煌', 'actors': '丛林/王璐瑶/杨议/谢林彤/杨少华/张海燕/管晓龙/龙梅子/阿尔法/郝梓含', 'genre': '传记/历史', 'area': ' 中国大陆', 'duration': 90, 'publishedDate': '2019-04-05', 'rateCount': '0', 'doubanRate': ''}
2020-04-27 00:20:17,142 -  log.py[line:110] - INFO: ROLLBACK
2020-04-27 00:20:17,144 -  scraper.py[line:236] - ERROR: Error processing {'actors': '丛林/王璐瑶/杨议/谢林彤/杨少华/张海燕/管晓龙/龙梅子/阿尔法/郝梓含',
 'area': ' 中国大陆',
 'dbMovieID': ['30486462'],
 'directors': '杨议',
 'doubanRate': [''],
 'duration': [90],
 'genre': '传记/历史',
 'movieName': ['长官传奇'],
 'publishedDate': ['2019-04-05'],
 'rateCount': ['0'],
 'tppMovieID': [1239171],
 'writers': '李延亮/崔伟/廖庆煌'}
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1246, in _execute_context
    cursor, statement, parameters, context
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 588, in do_execute
    cursor.execute(statement, parameters)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pymysql/cursors.py", line 170, in execute
    result = self._query(query)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pymysql/cursors.py", line 328, in _query
    conn.query(q)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pymysql/connections.py", line 517, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pymysql/connections.py", line 732, in _read_query_result
    result.read()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pymysql/connections.py", line 1075, in read
    first_packet = self.connection._read_packet()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pymysql/connections.py", line 684, in _read_packet
    packet.check_error()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pymysql/protocol.py", line 220, in check_error
    err.raise_mysql_exception(self._data)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pymysql/err.py", line 109, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.DataError: (1265, "Data truncated for column 'doubanRate' at row 1")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/pipelines.py", line 84, in process_item
    self.session.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1036, in commit
    self.transaction.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 503, in commit
    self._prepare_impl()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 482, in _prepare_impl
    self.session.flush()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2479, in flush
    self._flush(objects)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2617, in _flush
    transaction.rollback(_capture_exception=True)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 2577, in _flush
    flush_context.execute()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 422, in execute
    rec.execute(self)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/unitofwork.py", line 589, in execute
    uow,
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    insert,
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/persistence.py", line 1137, in _emit_insert_statements
    statement, params
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 982, in execute
    return meth(self, multiparams, params)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/sql/elements.py", line 293, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1101, in _execute_clauseelement
    distilled_params,
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1250, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1476, in _handle_dbapi_exception
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 152, in reraise
    raise value.with_traceback(tb)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1246, in _execute_context
    cursor, statement, parameters, context
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 588, in do_execute
    cursor.execute(statement, parameters)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pymysql/cursors.py", line 170, in execute
    result = self._query(query)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pymysql/cursors.py", line 328, in _query
    conn.query(q)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pymysql/connections.py", line 517, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pymysql/connections.py", line 732, in _read_query_result
    result.read()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pymysql/connections.py", line 1075, in read
    first_packet = self.connection._read_packet()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pymysql/connections.py", line 684, in _read_packet
    packet.check_error()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pymysql/protocol.py", line 220, in check_error
    err.raise_mysql_exception(self._data)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/pymysql/err.py", line 109, in raise_mysql_exception
    raise errorclass(errno, errval)
sqlalchemy.exc.DataError: (pymysql.err.DataError) (1265, "Data truncated for column 'doubanRate' at row 1")
[SQL: INSERT INTO `movieInfo` (`dbMovieID`, `tppMovieID`, `movieName`, directors, writers, actors, genre, area, duration, `publishedDate`, `rateCount`, `doubanRate`) VALUES (%(dbMovieID)s, %(tppMovieID)s, %(movieName)s, %(directors)s, %(writers)s, %(actors)s, %(genre)s, %(area)s, %(duration)s, %(publishedDate)s, %(rateCount)s, %(doubanRate)s)]
[parameters: {'dbMovieID': '30486462', 'tppMovieID': 1239171, 'movieName': '长官传奇', 'directors': '杨议', 'writers': '李延亮/崔伟/廖庆煌', 'actors': '丛林/王璐瑶/杨议/谢林彤/杨少华/张海燕/管晓龙/龙梅子/阿尔法/郝梓含', 'genre': '传记/历史', 'area': ' 中国大陆', 'duration': 90, 'publishedDate': '2019-04-05', 'rateCount': '0', 'doubanRate': ''}]
(Background on this error at: http://sqlalche.me/e/9h9h)
2020-04-27 00:20:17,898 -  movie_spider.py[line:166] - INFO: movie_name = 波西米亚狂想曲 and movie_year = 2019-04-27 and tpp_id = 1167831
2020-04-27 00:20:17,899 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E6%B3%A2%E8%A5%BF%E7%B1%B3%E4%BA%9A%E7%8B%82%E6%83%B3%E6%9B%B2
2020-04-27 00:20:17,899 -  movie_spider.py[line:172] - INFO: len of text is 1
2020-04-27 00:20:17,899 -  movie_spider.py[line:183] - ERROR: url wrong that url = 
2020-04-27 00:20:17,901 -  movie_spider.py[line:143] - WARNING: get 23 movie info, named None.
2020-04-27 00:20:17,903 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:17,903 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:17,903 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:17,903 -  scraper.py[line:236] - ERROR: Error processing {'boxInfo': ['4.80'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-27'],
 'movieID': [1162868],
 'movieName': ['转型团伙'],
 'releaseInfo': ['上映9天'],
 'seatRate': ['2.9%'],
 'showInfo': [1223],
 'showRate': ['0.4%'],
 'showView': ['2'],
 'splitBoxInfo': ['4.53'],
 'splitSumBoxInfo': ['1529.2'],
 'sumBoxInfo': ['1664.9'],
 'yearRate': ['2019-04-27#23']}
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/pipelines.py", line 84, in process_item
    self.session.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1036, in commit
    self.transaction.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 501, in commit
    self._assert_active(prepared_ok=True)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 295, in _assert_active
    code="7s2a",
sqlalchemy.exc.InvalidRequestError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (pymysql.err.DataError) (1265, "Data truncated for column 'doubanRate' at row 1")
[SQL: INSERT INTO `movieInfo` (`dbMovieID`, `tppMovieID`, `movieName`, directors, writers, actors, genre, area, duration, `publishedDate`, `rateCount`, `doubanRate`) VALUES (%(dbMovieID)s, %(tppMovieID)s, %(movieName)s, %(directors)s, %(writers)s, %(actors)s, %(genre)s, %(area)s, %(duration)s, %(publishedDate)s, %(rateCount)s, %(doubanRate)s)]
[parameters: {'dbMovieID': '30486462', 'tppMovieID': 1239171, 'movieName': '长官传奇', 'directors': '杨议', 'writers': '李延亮/崔伟/廖庆煌', 'actors': '丛林/王璐瑶/杨议/谢林彤/杨少华/张海燕/管晓龙/龙梅子/阿尔法/郝梓含', 'genre': '传记/历史', 'area': ' 中国大陆', 'duration': 90, 'publishedDate': '2019-04-05', 'rateCount': '0', 'doubanRate': ''}]
(Background on this error at: http://sqlalche.me/e/9h9h) (Background on this error at: http://sqlalche.me/e/7s2a)
2020-04-27 00:20:18,403 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%B5%B7%E9%97%A8%E6%B7%B1%E5%A4%84> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:18,404 -  movie_spider.py[line:166] - INFO: movie_name = 精灵怪物：疯狂之旅 and movie_year = 2019-04-27 and tpp_id = 343987
2020-04-27 00:20:18,404 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E7%B2%BE%E7%81%B5%E6%80%AA%E7%89%A9%EF%BC%9A%E7%96%AF%E7%8B%82%E4%B9%8B%E6%97%85
2020-04-27 00:20:18,404 -  movie_spider.py[line:172] - INFO: len of text is 1
2020-04-27 00:20:18,405 -  movie_spider.py[line:183] - ERROR: url wrong that url = 
2020-04-27 00:20:18,424 -  movie_spider.py[line:143] - WARNING: get 24 movie info, named None.
2020-04-27 00:20:18,426 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:18,426 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:18,426 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:18,427 -  scraper.py[line:236] - ERROR: Error processing {'boxInfo': ['4.27'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-27'],
 'movieID': [1213175],
 'movieName': ['在乎你'],
 'releaseInfo': ['上映16天'],
 'seatRate': ['7.6%'],
 'showInfo': [57],
 'showRate': ['<0.1%'],
 'showView': ['13'],
 'splitBoxInfo': ['4.25'],
 'splitSumBoxInfo': ['652.4'],
 'sumBoxInfo': ['684.1'],
 'yearRate': ['2019-04-27#24']}
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/pipelines.py", line 84, in process_item
    self.session.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1036, in commit
    self.transaction.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 501, in commit
    self._assert_active(prepared_ok=True)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 295, in _assert_active
    code="7s2a",
sqlalchemy.exc.InvalidRequestError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (pymysql.err.DataError) (1265, "Data truncated for column 'doubanRate' at row 1")
[SQL: INSERT INTO `movieInfo` (`dbMovieID`, `tppMovieID`, `movieName`, directors, writers, actors, genre, area, duration, `publishedDate`, `rateCount`, `doubanRate`) VALUES (%(dbMovieID)s, %(tppMovieID)s, %(movieName)s, %(directors)s, %(writers)s, %(actors)s, %(genre)s, %(area)s, %(duration)s, %(publishedDate)s, %(rateCount)s, %(doubanRate)s)]
[parameters: {'dbMovieID': '30486462', 'tppMovieID': 1239171, 'movieName': '长官传奇', 'directors': '杨议', 'writers': '李延亮/崔伟/廖庆煌', 'actors': '丛林/王璐瑶/杨议/谢林彤/杨少华/张海燕/管晓龙/龙梅子/阿尔法/郝梓含', 'genre': '传记/历史', 'area': ' 中国大陆', 'duration': 90, 'publishedDate': '2019-04-05', 'rateCount': '0', 'doubanRate': ''}]
(Background on this error at: http://sqlalche.me/e/9h9h) (Background on this error at: http://sqlalche.me/e/7s2a)
2020-04-27 00:20:18,445 -  movie_spider.py[line:143] - WARNING: get 25 movie info, named None.
2020-04-27 00:20:18,446 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:18,446 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:18,447 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:18,447 -  scraper.py[line:236] - ERROR: Error processing {'boxInfo': ['3.75'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-27'],
 'movieID': [342568],
 'movieName': ['诡梦凶铃'],
 'releaseInfo': [''],
 'seatRate': ['56.9%'],
 'showInfo': [10],
 'showRate': ['<0.1%'],
 'showView': ['142'],
 'splitBoxInfo': ['3.75'],
 'splitSumBoxInfo': ['1106.3'],
 'sumBoxInfo': ['1106.3'],
 'yearRate': ['2019-04-27#25']}
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/pipelines.py", line 84, in process_item
    self.session.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1036, in commit
    self.transaction.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 501, in commit
    self._assert_active(prepared_ok=True)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 295, in _assert_active
    code="7s2a",
sqlalchemy.exc.InvalidRequestError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (pymysql.err.DataError) (1265, "Data truncated for column 'doubanRate' at row 1")
[SQL: INSERT INTO `movieInfo` (`dbMovieID`, `tppMovieID`, `movieName`, directors, writers, actors, genre, area, duration, `publishedDate`, `rateCount`, `doubanRate`) VALUES (%(dbMovieID)s, %(tppMovieID)s, %(movieName)s, %(directors)s, %(writers)s, %(actors)s, %(genre)s, %(area)s, %(duration)s, %(publishedDate)s, %(rateCount)s, %(doubanRate)s)]
[parameters: {'dbMovieID': '30486462', 'tppMovieID': 1239171, 'movieName': '长官传奇', 'directors': '杨议', 'writers': '李延亮/崔伟/廖庆煌', 'actors': '丛林/王璐瑶/杨议/谢林彤/杨少华/张海燕/管晓龙/龙梅子/阿尔法/郝梓含', 'genre': '传记/历史', 'area': ' 中国大陆', 'duration': 90, 'publishedDate': '2019-04-05', 'rateCount': '0', 'doubanRate': ''}]
(Background on this error at: http://sqlalche.me/e/9h9h) (Background on this error at: http://sqlalche.me/e/7s2a)
2020-04-27 00:20:18,524 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E8%BD%AC%E5%9E%8B%E5%9B%A2%E4%BC%99> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:18,525 -  movie_spider.py[line:143] - WARNING: get 26 movie info, named None.
2020-04-27 00:20:18,526 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:18,526 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:18,527 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:18,527 -  scraper.py[line:236] - ERROR: Error processing {'boxInfo': ['3.70'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-27'],
 'movieID': [78374],
 'movieName': ['斗牛'],
 'releaseInfo': [''],
 'seatRate': ['--'],
 'showInfo': [3],
 'showRate': ['<0.1%'],
 'showView': ['210'],
 'splitBoxInfo': ['3.70'],
 'splitSumBoxInfo': ['1130.6'],
 'sumBoxInfo': ['1130.6'],
 'yearRate': ['2019-04-27#26']}
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/pipelines.py", line 84, in process_item
    self.session.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1036, in commit
    self.transaction.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 501, in commit
    self._assert_active(prepared_ok=True)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 295, in _assert_active
    code="7s2a",
sqlalchemy.exc.InvalidRequestError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (pymysql.err.DataError) (1265, "Data truncated for column 'doubanRate' at row 1")
[SQL: INSERT INTO `movieInfo` (`dbMovieID`, `tppMovieID`, `movieName`, directors, writers, actors, genre, area, duration, `publishedDate`, `rateCount`, `doubanRate`) VALUES (%(dbMovieID)s, %(tppMovieID)s, %(movieName)s, %(directors)s, %(writers)s, %(actors)s, %(genre)s, %(area)s, %(duration)s, %(publishedDate)s, %(rateCount)s, %(doubanRate)s)]
[parameters: {'dbMovieID': '30486462', 'tppMovieID': 1239171, 'movieName': '长官传奇', 'directors': '杨议', 'writers': '李延亮/崔伟/廖庆煌', 'actors': '丛林/王璐瑶/杨议/谢林彤/杨少华/张海燕/管晓龙/龙梅子/阿尔法/郝梓含', 'genre': '传记/历史', 'area': ' 中国大陆', 'duration': 90, 'publishedDate': '2019-04-05', 'rateCount': '0', 'doubanRate': ''}]
(Background on this error at: http://sqlalche.me/e/9h9h) (Background on this error at: http://sqlalche.me/e/7s2a)
2020-04-27 00:20:18,710 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E8%AF%A1%E6%A2%A6%E5%87%B6%E9%93%83> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:18,711 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E5%9C%A8%E4%B9%8E%E4%BD%A0> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:18,713 -  movie_spider.py[line:143] - WARNING: get 27 movie info, named None.
2020-04-27 00:20:18,714 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:18,714 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:18,714 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:18,715 -  scraper.py[line:236] - ERROR: Error processing {'boxInfo': ['3.63'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-27'],
 'movieID': [1228750],
 'movieName': ['照相师'],
 'releaseInfo': [''],
 'seatRate': ['65.6%'],
 'showInfo': [18],
 'showRate': ['<0.1%'],
 'showView': ['60'],
 'splitBoxInfo': ['3.63'],
 'splitSumBoxInfo': ['2004.3'],
 'sumBoxInfo': ['2016.8'],
 'yearRate': ['2019-04-27#27']}
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/pipelines.py", line 84, in process_item
    self.session.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1036, in commit
    self.transaction.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 501, in commit
    self._assert_active(prepared_ok=True)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 295, in _assert_active
    code="7s2a",
sqlalchemy.exc.InvalidRequestError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (pymysql.err.DataError) (1265, "Data truncated for column 'doubanRate' at row 1")
[SQL: INSERT INTO `movieInfo` (`dbMovieID`, `tppMovieID`, `movieName`, directors, writers, actors, genre, area, duration, `publishedDate`, `rateCount`, `doubanRate`) VALUES (%(dbMovieID)s, %(tppMovieID)s, %(movieName)s, %(directors)s, %(writers)s, %(actors)s, %(genre)s, %(area)s, %(duration)s, %(publishedDate)s, %(rateCount)s, %(doubanRate)s)]
[parameters: {'dbMovieID': '30486462', 'tppMovieID': 1239171, 'movieName': '长官传奇', 'directors': '杨议', 'writers': '李延亮/崔伟/廖庆煌', 'actors': '丛林/王璐瑶/杨议/谢林彤/杨少华/张海燕/管晓龙/龙梅子/阿尔法/郝梓含', 'genre': '传记/历史', 'area': ' 中国大陆', 'duration': 90, 'publishedDate': '2019-04-05', 'rateCount': '0', 'doubanRate': ''}]
(Background on this error at: http://sqlalche.me/e/9h9h) (Background on this error at: http://sqlalche.me/e/7s2a)
2020-04-27 00:20:19,501 -  movie_spider.py[line:166] - INFO: movie_name = 海门深处 and movie_year = 2019-04-27 and tpp_id = 1215746
2020-04-27 00:20:19,501 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E6%B5%B7%E9%97%A8%E6%B7%B1%E5%A4%84
2020-04-27 00:20:19,502 -  movie_spider.py[line:172] - INFO: len of text is 0
2020-04-27 00:20:19,502 -  movie_spider.py[line:174] - ERROR: not response scraped
2020-04-27 00:20:19,523 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%96%97%E7%89%9B> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:19,524 -  movie_spider.py[line:143] - WARNING: get 28 movie info, named None.
2020-04-27 00:20:19,525 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:19,525 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:19,525 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:19,526 -  scraper.py[line:236] - ERROR: Error processing {'boxInfo': ['3.29'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-27'],
 'movieID': [248906],
 'movieName': ['流浪地球'],
 'releaseInfo': [''],
 'seatRate': ['7.6%'],
 'showInfo': [179],
 'showRate': ['<0.1%'],
 'showView': ['6'],
 'splitBoxInfo': ['3.05'],
 'splitSumBoxInfo': ['435300.0'],
 'sumBoxInfo': ['467900.0'],
 'yearRate': ['2019-04-27#28']}
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/pipelines.py", line 84, in process_item
    self.session.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1036, in commit
    self.transaction.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 501, in commit
    self._assert_active(prepared_ok=True)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 295, in _assert_active
    code="7s2a",
sqlalchemy.exc.InvalidRequestError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (pymysql.err.DataError) (1265, "Data truncated for column 'doubanRate' at row 1")
[SQL: INSERT INTO `movieInfo` (`dbMovieID`, `tppMovieID`, `movieName`, directors, writers, actors, genre, area, duration, `publishedDate`, `rateCount`, `doubanRate`) VALUES (%(dbMovieID)s, %(tppMovieID)s, %(movieName)s, %(directors)s, %(writers)s, %(actors)s, %(genre)s, %(area)s, %(duration)s, %(publishedDate)s, %(rateCount)s, %(doubanRate)s)]
[parameters: {'dbMovieID': '30486462', 'tppMovieID': 1239171, 'movieName': '长官传奇', 'directors': '杨议', 'writers': '李延亮/崔伟/廖庆煌', 'actors': '丛林/王璐瑶/杨议/谢林彤/杨少华/张海燕/管晓龙/龙梅子/阿尔法/郝梓含', 'genre': '传记/历史', 'area': ' 中国大陆', 'duration': 90, 'publishedDate': '2019-04-05', 'rateCount': '0', 'doubanRate': ''}]
(Background on this error at: http://sqlalche.me/e/9h9h) (Background on this error at: http://sqlalche.me/e/7s2a)
2020-04-27 00:20:20,067 -  movie_spider.py[line:166] - INFO: movie_name = 转型团伙 and movie_year = 2019-04-27 and tpp_id = 1162868
2020-04-27 00:20:20,067 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E8%BD%AC%E5%9E%8B%E5%9B%A2%E4%BC%99
2020-04-27 00:20:20,068 -  movie_spider.py[line:172] - INFO: len of text is 0
2020-04-27 00:20:20,068 -  movie_spider.py[line:174] - ERROR: not response scraped
2020-04-27 00:20:20,111 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E7%85%A7%E7%9B%B8%E5%B8%88> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:20,112 -  movie_spider.py[line:143] - WARNING: get 29 movie info, named None.
2020-04-27 00:20:20,113 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:20,113 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:20,113 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:20,114 -  scraper.py[line:236] - ERROR: Error processing {'boxInfo': ['3.19'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-27'],
 'movieID': [1237437],
 'movieName': ['云雾笼罩的山峰'],
 'releaseInfo': ['上映2天'],
 'seatRate': ['6.9%'],
 'showInfo': [452],
 'showRate': ['0.1%'],
 'showView': ['3'],
 'splitBoxInfo': ['3.07'],
 'splitSumBoxInfo': ['12.4'],
 'sumBoxInfo': ['12.8'],
 'yearRate': ['2019-04-27#29']}
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/pipelines.py", line 84, in process_item
    self.session.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1036, in commit
    self.transaction.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 501, in commit
    self._assert_active(prepared_ok=True)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 295, in _assert_active
    code="7s2a",
sqlalchemy.exc.InvalidRequestError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (pymysql.err.DataError) (1265, "Data truncated for column 'doubanRate' at row 1")
[SQL: INSERT INTO `movieInfo` (`dbMovieID`, `tppMovieID`, `movieName`, directors, writers, actors, genre, area, duration, `publishedDate`, `rateCount`, `doubanRate`) VALUES (%(dbMovieID)s, %(tppMovieID)s, %(movieName)s, %(directors)s, %(writers)s, %(actors)s, %(genre)s, %(area)s, %(duration)s, %(publishedDate)s, %(rateCount)s, %(doubanRate)s)]
[parameters: {'dbMovieID': '30486462', 'tppMovieID': 1239171, 'movieName': '长官传奇', 'directors': '杨议', 'writers': '李延亮/崔伟/廖庆煌', 'actors': '丛林/王璐瑶/杨议/谢林彤/杨少华/张海燕/管晓龙/龙梅子/阿尔法/郝梓含', 'genre': '传记/历史', 'area': ' 中国大陆', 'duration': 90, 'publishedDate': '2019-04-05', 'rateCount': '0', 'doubanRate': ''}]
(Background on this error at: http://sqlalche.me/e/9h9h) (Background on this error at: http://sqlalche.me/e/7s2a)
2020-04-27 00:20:20,590 -  movie_spider.py[line:166] - INFO: movie_name = 诡梦凶铃 and movie_year = 2019-04-27 and tpp_id = 342568
2020-04-27 00:20:20,591 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E8%AF%A1%E6%A2%A6%E5%87%B6%E9%93%83
2020-04-27 00:20:20,592 -  movie_spider.py[line:172] - INFO: len of text is 0
2020-04-27 00:20:20,592 -  movie_spider.py[line:174] - ERROR: not response scraped
2020-04-27 00:20:20,596 -  movie_spider.py[line:166] - INFO: movie_name = 在乎你 and movie_year = 2019-04-27 and tpp_id = 1213175
2020-04-27 00:20:20,596 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E5%9C%A8%E4%B9%8E%E4%BD%A0
2020-04-27 00:20:20,596 -  movie_spider.py[line:172] - INFO: len of text is 0
2020-04-27 00:20:20,597 -  movie_spider.py[line:174] - ERROR: not response scraped
2020-04-27 00:20:20,619 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E6%B5%81%E6%B5%AA%E5%9C%B0%E7%90%83> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:20,620 -  movie_spider.py[line:143] - WARNING: get 30 movie info, named None.
2020-04-27 00:20:20,622 -  pipelines.py[line:50] - INFO: item is new
2020-04-27 00:20:20,622 -  pipelines.py[line:74] - CRITICAL: type of item is <class 'movie.items.BoxOfficeItem'>
2020-04-27 00:20:20,622 -  pipelines.py[line:83] - WARNING: insert database finished
2020-04-27 00:20:20,622 -  scraper.py[line:236] - ERROR: Error processing {'boxInfo': ['3.19'],
 'boxRate': ['<0.1%'],
 'crawlDate': ['2019-04-27'],
 'movieID': [1262087],
 'movieName': ['醒来之爱的呼唤'],
 'releaseInfo': ['上映51天'],
 'seatRate': ['45.5%'],
 'showInfo': [23],
 'showRate': ['<0.1%'],
 'showView': ['49'],
 'splitBoxInfo': ['3.19'],
 'splitSumBoxInfo': ['307.9'],
 'sumBoxInfo': ['310.0'],
 'yearRate': ['2019-04-27#30']}
Traceback (most recent call last):
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/dawinia/CodeBook/ScrapyProject/movie/movie/pipelines.py", line 84, in process_item
    self.session.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 1036, in commit
    self.transaction.commit()
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 501, in commit
    self._assert_active(prepared_ok=True)
  File "/home/dawinia/anaconda3/envs/Scrapy/lib/python3.7/site-packages/sqlalchemy/orm/session.py", line 295, in _assert_active
    code="7s2a",
sqlalchemy.exc.InvalidRequestError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (pymysql.err.DataError) (1265, "Data truncated for column 'doubanRate' at row 1")
[SQL: INSERT INTO `movieInfo` (`dbMovieID`, `tppMovieID`, `movieName`, directors, writers, actors, genre, area, duration, `publishedDate`, `rateCount`, `doubanRate`) VALUES (%(dbMovieID)s, %(tppMovieID)s, %(movieName)s, %(directors)s, %(writers)s, %(actors)s, %(genre)s, %(area)s, %(duration)s, %(publishedDate)s, %(rateCount)s, %(doubanRate)s)]
[parameters: {'dbMovieID': '30486462', 'tppMovieID': 1239171, 'movieName': '长官传奇', 'directors': '杨议', 'writers': '李延亮/崔伟/廖庆煌', 'actors': '丛林/王璐瑶/杨议/谢林彤/杨少华/张海燕/管晓龙/龙梅子/阿尔法/郝梓含', 'genre': '传记/历史', 'area': ' 中国大陆', 'duration': 90, 'publishedDate': '2019-04-05', 'rateCount': '0', 'doubanRate': ''}]
(Background on this error at: http://sqlalche.me/e/9h9h) (Background on this error at: http://sqlalche.me/e/7s2a)
2020-04-27 00:20:20,644 -  movie_spider.py[line:166] - INFO: movie_name = 斗牛 and movie_year = 2019-04-27 and tpp_id = 78374
2020-04-27 00:20:20,644 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E6%96%97%E7%89%9B
2020-04-27 00:20:20,644 -  movie_spider.py[line:172] - INFO: len of text is 0
2020-04-27 00:20:20,644 -  movie_spider.py[line:174] - ERROR: not response scraped
2020-04-27 00:20:20,653 -  movie_spider.py[line:155] - ERROR: boxOffice start parse
2020-04-27 00:20:20,654 -  movie_spider.py[line:166] - INFO: movie_name = 照相师 and movie_year = 2019-04-27 and tpp_id = 1228750
2020-04-27 00:20:20,654 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E7%85%A7%E7%9B%B8%E5%B8%88
2020-04-27 00:20:20,654 -  movie_spider.py[line:172] - INFO: len of text is 0
2020-04-27 00:20:20,655 -  movie_spider.py[line:174] - ERROR: not response scraped
2020-04-27 00:20:20,676 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E4%BA%91%E9%9B%BE%E7%AC%BC%E7%BD%A9%E7%9A%84%E5%B1%B1%E5%B3%B0> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:20,708 -  engine.py[line:239] - DEBUG: Crawled (200) <GET https://movie.douban.com/j/subject_suggest?q=%E9%86%92%E6%9D%A5%E4%B9%8B%E7%88%B1%E7%9A%84%E5%91%BC%E5%94%A4> (referer: http://piaofang.maoyan.com/dashboard-ajax/movie?showDate=20190427)
2020-04-27 00:20:20,721 -  movie_spider.py[line:166] - INFO: movie_name = 流浪地球 and movie_year = 2019-04-27 and tpp_id = 248906
2020-04-27 00:20:20,721 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E6%B5%81%E6%B5%AA%E5%9C%B0%E7%90%83
2020-04-27 00:20:20,721 -  movie_spider.py[line:172] - INFO: len of text is 0
2020-04-27 00:20:20,721 -  movie_spider.py[line:174] - ERROR: not response scraped
2020-04-27 00:20:20,777 -  movie_spider.py[line:166] - INFO: movie_name = 云雾笼罩的山峰 and movie_year = 2019-04-27 and tpp_id = 1237437
2020-04-27 00:20:20,778 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E4%BA%91%E9%9B%BE%E7%AC%BC%E7%BD%A9%E7%9A%84%E5%B1%B1%E5%B3%B0
2020-04-27 00:20:20,778 -  movie_spider.py[line:172] - INFO: len of text is 0
2020-04-27 00:20:20,778 -  movie_spider.py[line:174] - ERROR: not response scraped
2020-04-27 00:20:20,810 -  movie_spider.py[line:166] - INFO: movie_name = 醒来之爱的呼唤 and movie_year = 2019-04-27 and tpp_id = 1262087
2020-04-27 00:20:20,810 -  movie_spider.py[line:171] - INFO: the url is https://movie.douban.com/j/subject_suggest?q=%E9%86%92%E6%9D%A5%E4%B9%8B%E7%88%B1%E7%9A%84%E5%91%BC%E5%94%A4
2020-04-27 00:20:20,810 -  movie_spider.py[line:172] - INFO: len of text is 0
2020-04-27 00:20:20,810 -  movie_spider.py[line:174] - ERROR: not response scraped
2020-04-27 00:20:20,812 -  engine.py[line:296] - INFO: Closing spider (finished)
2020-04-27 00:20:20,813 -  statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 50214,
 'downloader/request_count': 40,
 'downloader/request_method_count/GET': 40,
 'downloader/response_bytes': 244007,
 'downloader/response_count': 40,
 'downloader/response_status_count/200': 40,
 'elapsed_time_seconds': 28.544875,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2020, 4, 26, 16, 20, 20, 812949),
 'item_dropped_count': 1,
 'item_dropped_reasons_count/DropItem': 1,
 'item_scraped_count': 27,
 'log_count/CRITICAL': 54,
 'log_count/ERROR': 56,
 'log_count/WARNING': 67,
 'memusage/max': 74043392,
 'memusage/startup': 74043392,
 'request_depth_max': 2,
 'response_received_count': 40,
 'scheduler/dequeued': 40,
 'scheduler/dequeued/memory': 40,
 'scheduler/enqueued': 40,
 'scheduler/enqueued/memory': 40,
 'spider_exceptions/JSONDecodeError': 2,
 'start_time': datetime.datetime(2020, 4, 26, 16, 19, 52, 268074)}
2020-04-27 00:20:20,813 -  engine.py[line:327] - INFO: Spider closed (finished)
